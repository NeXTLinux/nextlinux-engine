<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>Anchore Engine – Anchore Engine</title><link>/docs/</link><description>Recent Hugo news from gohugo.io</description><generator>Hugo -- gohugo.io</generator><image><url>img/hugo.png</url><title>GoHugo.io</title><link>/docs/</link></image><atom:link href="/docs/index.xml" rel="self" type="application/rss+xml"/><item><title>Docs: Analysis Archive Storage Configuration</title><link>/docs/install/storage/analysis_archive/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/docs/install/storage/analysis_archive/</guid><description>
&lt;p>For information on what the analysis archive is and how it works, see &lt;a href="/docs/general/concepts/analysis_archive/">Concepts: Analysis Archive&lt;/a>&lt;/p>
&lt;p>The Analysis Archive is an &lt;a href="../object_store">object store&lt;/a> with specific semantics and thus is configured as an object store using the same
configuration options, just with a different config key: &lt;code>analysis_archive&lt;/code>&lt;/p>
&lt;p>Example configuration snippet for using the db for working set object store and S3 for the analysis archive:&lt;/p>
&lt;pre>&lt;code>...
services:
...
catalog:
...
object_store:
compression:
enabled: false
min_size_kbytes: 100
storage_driver:
name: db
config: {}
analysis_archive:
compression:
enabled: False
min_size_kbytes: 100
storage_driver:
name: 's3'
config:
access_key: 'MY_ACCESS_KEY'
secret_key: 'MY_SECRET_KEY'
#iamauto: True
url: 'https://S3-end-point.example.com'
region: False
bucket: 'anchorearchive'
create_bucket: True
&lt;/code>&lt;/pre>&lt;h2 id="default-configuration">Default Configuration&lt;/h2>
&lt;p>By default, if no &lt;code>analysis_archive&lt;/code> config is found or the property is not present in the config.yaml, the analysis archive
will use the &lt;code>object_store&lt;/code> or &lt;code>archive&lt;/code> (for backwards compatibility) config sections and those defaults (e.g. db if found).&lt;/p>
&lt;p>Anchore stores all of the analysis archive objects in an internal logical bucket: &lt;em>analysis_archive&lt;/em> that is distinct in
the configured backends (e.g a key prefix in the s3 bucket or swift container)&lt;/p>
&lt;h2 id="changing-configuration">Changing Configuration&lt;/h2>
&lt;p>Unless there are image analyses actually in the archive, there is no data to move if you need to update the configuration
to use a different backend, but once an image analysis has been archived to update the configuration you must follow
the object storage data migration process found &lt;a href="../object_store/migration">here&lt;/a>. As noted in that guide, if you need
to migrate to/from an &lt;code>analysis_archive&lt;/code> config you&amp;rsquo;ll need to use the &amp;ndash;from-analysis-archive/&amp;ndash;to-analysis-archive
options as needed to tell the migration process which configuration to use in the source and destination config files
used for the migration.&lt;/p>
&lt;h2 id="common-configurations">Common Configurations&lt;/h2>
&lt;ol>
&lt;li>
&lt;p>Single shared object store backend: omit the analysis_archive config, or set it to &lt;em>null&lt;/em> or &lt;em>{}&lt;/em>&lt;/p>
&lt;/li>
&lt;li>
&lt;p>Different bucket/container: the object_store and analysis_archive configurations are both specified and identical
with the exception of the &lt;em>bucket&lt;/em> or &lt;em>container&lt;/em> values for the analysis_archive so that its data is split into a
different backend bucket to allow for lifecycle controls or cost optimization since its access is much less frequent (if ever).&lt;/p>
&lt;/li>
&lt;li>
&lt;p>Primary object store in DB, analysis_archive in external S3/Swift: this keeps latency low as no external service is
needed for the object store and active data but lets you use more scalable external object storage for archive data. This
approach is most beneficial if you can keep the working set of images small and quickly transition old analysis to the
archive to ensure the db is kept small and the analysis archive handles the data scaling over time.&lt;/p>
&lt;/li>
&lt;/ol></description></item><item><title>Docs: Anchore Policies Checks</title><link>/docs/usage/cli_usage/policies/policy_checks/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/docs/usage/cli_usage/policies/policy_checks/</guid><description>
&lt;h2 id="introduction">Introduction&lt;/h2>
&lt;p>Information about the latest available policy gates, triggers and parameters can be retrieved from a running anchore-engine, using the anchore-cli command below:&lt;/p>
&lt;p>&lt;code># anchore-cli policy describe (--gate &amp;lt;gatename&amp;gt; ( --trigger &amp;lt;triggername))&lt;/code>&lt;/p>
&lt;h2 id="gates">Gates&lt;/h2>
&lt;table>
&lt;thead>
&lt;tr>
&lt;th>Gate&lt;/th>
&lt;th>Description&lt;/th>
&lt;/tr>
&lt;/thead>
&lt;tbody>
&lt;tr>
&lt;td>always&lt;/td>
&lt;td>Triggers that fire unconditionally if present in policy, useful for things like testing and blacklisting&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>dockerfile&lt;/td>
&lt;td>Checks against the content of a dockerfile if provided, or a guessed dockerfile based on docker layer history if the dockerfile is not provided&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>files&lt;/td>
&lt;td>Checks against files in the analyzed image including file content, file names, and filesystem attributes&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>licenses&lt;/td>
&lt;td>License checks against found software licenses in the container image&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>malware&lt;/td>
&lt;td>Checks for malware scan findings in the image&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>metadata&lt;/td>
&lt;td>Checks against image metadata, such as size, OS, distro, architecture, etc.&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>npms&lt;/td>
&lt;td>NPM Checks&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>packages&lt;/td>
&lt;td>Distro package checks&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>passwd_file&lt;/td>
&lt;td>Content checks for /etc/passwd for things like usernames, group ids, shells, or full entries&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>retrieved_files&lt;/td>
&lt;td>Checks against content and/or presence of files retrieved at analysis time from an image&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>ruby_gems&lt;/td>
&lt;td>Ruby Gem Checks&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>secret_scans&lt;/td>
&lt;td>Checks for secrets and content found in the image using configured regexes found in the &amp;ldquo;secret_search&amp;rdquo; section of analyzer_config.yaml&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>vulnerabilities&lt;/td>
&lt;td>CVE/Vulnerability checks&lt;/td>
&lt;/tr>
&lt;/tbody>
&lt;/table>
&lt;p>For a more in-depth list of available gates/triggers, refer to &lt;a href="/docs/general/concepts/policy/policy_checks/">Anchore Policy Checks&lt;/a>&lt;/p></description></item><item><title>Docs: Migrating Data to New Drivers</title><link>/docs/install/storage/object_store/migration/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/docs/install/storage/object_store/migration/</guid><description>
&lt;h3 id="overview">Overview&lt;/h3>
&lt;p>To cleanly migrate data from one archive driver to another, Anchore Engine includes some tooling that automates the process in the &amp;lsquo;anchore-manager&amp;rsquo; tool packaged with the system.&lt;/p>
&lt;p>The migration process is an offline process; Anchore Engine is not designed to handle an online migration.&lt;/p>
&lt;p>For the migration process you will need:&lt;/p>
&lt;ol>
&lt;li>The original config.yaml used by the services already, if services are split out or using different config.yaml for different services, you need the config.yaml used by the catalog services&lt;/li>
&lt;li>An updated config.yaml (named dest-config.yaml in this example), with the archive driver section of the catalog service config set to the config you want to migrate &lt;em>to&lt;/em>&lt;/li>
&lt;li>The db connection string from config.yaml, this is needed by the anchore-manager script directly&lt;/li>
&lt;li>Credentials and resources (bucket etc) for the destination of the migration&lt;/li>
&lt;/ol>
&lt;p>At a high-level the process is:&lt;/p>
&lt;ol>
&lt;li>Shutdown all anchore engine services and components. The system should be fully offline, but the database must be online and available. For a docker-compose install, this is achieved by simply stopping the engine container, but not deleting it.&lt;/li>
&lt;li>Prepare a new config.yaml that includes the new driver configuration for the destination of the migration (dest-config.yaml) in the same location as the existing config.yaml&lt;/li>
&lt;li>Test the new dest-config.yaml to ensure correct configuration&lt;/li>
&lt;li>Run the migration&lt;/li>
&lt;li>Get coffee&amp;hellip; this could take a while if you have a lot of analysis data&lt;/li>
&lt;li>When complete, view the results&lt;/li>
&lt;li>Ensure the dest-config.yaml is in place for all the components as config.yaml&lt;/li>
&lt;li>Start anchore-engine&lt;/li>
&lt;/ol>
&lt;h3 id="migration-example-using-docker-compose-deployed-anchore-engine">Migration Example Using Docker Compose Deployed Anchore Engine&lt;/h3>
&lt;p>The following is an example migration for an anchore-engine deployed via docker-compose on a single host with a local postgresql container&amp;ndash;basically the example used in &amp;lsquo;Installing Anchore Engine&amp;rsquo; documents. At the end of this section, we&amp;rsquo;ll cover the caveats and things to watch for a multi-node install of anchore engine.&lt;/p>
&lt;p>This process requires that you run the command in a location that has access to both the source archive driver configuration and the new archive driver configuration.&lt;/p>
&lt;h4 id="step-1-shutdown-all-services">Step 1: Shutdown all services&lt;/h4>
&lt;p>All services should be stopped, but the postgresql db must still be available and running.&lt;/p>
&lt;p>&lt;code>docker-compose stop anchore-engine&lt;/code>&lt;/p>
&lt;h4 id="step-2-prepare-a-new-configyaml">Step 2: Prepare a new config.yaml&lt;/h4>
&lt;p>Both the original and new configurations are needed, so create a copy and update the archive driver section to the configuration you want to migrate to&lt;/p>
&lt;pre>&lt;code>cd config
cp config.yaml dest-config.yaml
&amp;lt;edit dest-config.yaml&amp;gt;
&lt;/code>&lt;/pre>&lt;h4 id="step-3-test-the-destination-config">Step 3: Test the destination config&lt;/h4>
&lt;p>Assuming that config is &lt;code>dest-config.yaml&lt;/code>:&lt;/p>
&lt;pre>&lt;code>[user@host aevolume]$ docker-compose run anchore-engine /bin/bash
[root@3209ad44d7bb ~]# anchore-manager objectstorage --db-connect ${db} check /config/dest-config.yaml
[MainThread] [anchore_manager.cli.utils/connect_database()] [INFO] DB params: {&amp;quot;db_pool_size&amp;quot;: 30, &amp;quot;db_connect&amp;quot;: &amp;quot;postgresql+pg8000://postgres:postgres.dev@postgres-dev:5432/postgres&amp;quot;, &amp;quot;db_connect_args&amp;quot;: {&amp;quot;ssl&amp;quot;: false, &amp;quot;connect_timeout&amp;quot;: 120, &amp;quot;timeout&amp;quot;: 30}, &amp;quot;db_pool_max_overflow&amp;quot;: 100}
[MainThread] [anchore_manager.cli.utils/connect_database()] [INFO] DB connection configured: True
[MainThread] [anchore_manager.cli.utils/connect_database()] [INFO] DB attempting to connect...
[MainThread] [anchore_manager.cli.utils/connect_database()] [INFO] DB connected: True
[MainThread] [anchore_manager.cli.objectstorage/check()] [INFO] Using config file /config/dest-config.yaml
[MainThread] [anchore_engine.subsys.object_store.operations/initialize()] [INFO] Archive initialization complete
[MainThread] [anchore_manager.cli.objectstorage/check()] [INFO] Checking existence of test document with user_id = test, bucket = anchorecliconfigtest and archive_id = cliconfigtest
[MainThread] [anchore_manager.cli.objectstorage/check()] [INFO] Creating test document with user_id = test, bucket = anchorecliconfigtest and archive_id = cliconfigtest
[MainThread] [anchore_manager.cli.objectstorage/check()] [INFO] Checking document fetch
[MainThread] [anchore_manager.cli.objectstorage/check()] [INFO] Removing test object
[MainThread] [anchore_manager.cli.objectstorage/check()] [INFO] Archive config check completed successfully
&lt;/code>&lt;/pre>&lt;h4 id="step-3a-test-the-current-configyaml">Step 3a: Test the current config.yaml&lt;/h4>
&lt;p>If you are running the migration for a different location than one of the anchore engine containers&lt;/p>
&lt;p>Same as above but using &lt;code>/config/config.yaml&lt;/code> as the input to check (skipped in this instance since we&amp;rsquo;re running the migration from the same container)&lt;/p>
&lt;h4 id="step-4-run-the-migration">Step 4: Run the Migration&lt;/h4>
&lt;p>By default, the migration process will remove data from the source once it has confirmed it has been copied to the destination and the metadata has been updated in the anchore db. To skip the deletion on the source, use the &amp;lsquo;&amp;ndash;nodelete&amp;rsquo; option. it is the safest option, but if you use it, you are responsible for removing the data later.&lt;/p>
&lt;pre>&lt;code>[root@3209ad44d7bb ~]# anchore-manager objectstorage --db-connect ${db} migrate /config/config.yaml /config/dest-config.yaml
[MainThread] [anchore_manager.cli.utils/connect_database()] [INFO] DB params: {&amp;quot;db_pool_size&amp;quot;: 30, &amp;quot;db_connect&amp;quot;: &amp;quot;postgresql+pg8000://postgres:postgres.dev@postgres-dev:5432/postgres&amp;quot;, &amp;quot;db_connect_args&amp;quot;: {&amp;quot;ssl&amp;quot;: false, &amp;quot;connect_timeout&amp;quot;: 120, &amp;quot;timeout&amp;quot;: 30}, &amp;quot;db_pool_max_overflow&amp;quot;: 100}
[MainThread] [anchore_manager.cli.utils/connect_database()] [INFO] DB connection configured: True
[MainThread] [anchore_manager.cli.utils/connect_database()] [INFO] DB attempting to connect...
[MainThread] [anchore_manager.cli.utils/connect_database()] [INFO] DB connected: True
[MainThread] [anchore_manager.cli.objectstorage/migrate()] [INFO] Loading configs
[MainThread] [anchore_manager.cli.objectstorage/migrate()] [INFO] Migration from config: {
&amp;quot;storage_driver&amp;quot;: {
&amp;quot;config&amp;quot;: {},
&amp;quot;name&amp;quot;: &amp;quot;db&amp;quot;
},
&amp;quot;compression&amp;quot;: {
&amp;quot;enabled&amp;quot;: false,
&amp;quot;min_size_kbytes&amp;quot;: 100
}
}
[MainThread] [anchore_manager.cli.objectstorage/migrate()] [INFO] Migration to config: {
&amp;quot;storage_driver&amp;quot;: {
&amp;quot;config&amp;quot;: {
&amp;quot;access_key&amp;quot;: &amp;quot;9EB92C7W61YPFQ6QLDOU&amp;quot;,
&amp;quot;create_bucket&amp;quot;: true,
&amp;quot;url&amp;quot;: &amp;quot;http://minio-ephemeral-test:9000/&amp;quot;,
&amp;quot;region&amp;quot;: false,
&amp;quot;bucket&amp;quot;: &amp;quot;anchore-engine-testing&amp;quot;,
&amp;quot;prefix&amp;quot;: &amp;quot;internaltest&amp;quot;,
&amp;quot;secret_key&amp;quot;: &amp;quot;TuHo2UbBx+amD3YiCeidy+R3q82MPTPiyd+dlW+s&amp;quot;
},
&amp;quot;name&amp;quot;: &amp;quot;s3&amp;quot;
},
&amp;quot;compression&amp;quot;: {
&amp;quot;enabled&amp;quot;: true,
&amp;quot;min_size_kbytes&amp;quot;: 100
}
}
Performing this operation requires *all* anchore-engine services to be stopped - proceed? (y/N)y
[MainThread] [anchore_engine.subsys.object_store.migration/initiate_migration()] [INFO] Initializing migration from {'storage_driver': {'config': {}, 'name': 'db'}, 'compression': {'enabled': False, 'min_size_kbytes': 100}} to {'storage_driver': {'config': {'access_key': '9EB92C7W61YPFQ6QLDOU', 'create_bucket': True, 'url': 'http://minio-ephemeral-test:9000/', 'region': False, 'bucket': 'anchore-engine-testing', 'prefix': 'internaltest', 'secret_key': 'TuHo2UbBx+amD3YiCeidy+R3q82MPTPiyd+dlW+s'}, 'name': 's3'}, 'compression': {'enabled': True, 'min_size_kbytes': 100}}
[MainThread] [anchore_engine.subsys.object_store.migration/migration_context()] [INFO] Initializing source object_store: {'storage_driver': {'config': {}, 'name': 'db'}, 'compression': {'enabled': False, 'min_size_kbytes': 100}}
[MainThread] [anchore_engine.subsys.object_store.migration/migration_context()] [INFO] Initializing dest object_store: {'storage_driver': {'config': {'access_key': '9EB92C7W61YPFQ6QLDOU', 'create_bucket': True, 'url': 'http://minio-ephemeral-test:9000/', 'region': False, 'bucket': 'anchore-engine-testing', 'prefix': 'internaltest', 'secret_key': 'TuHo2UbBx+amD3YiCeidy+R3q82MPTPiyd+dlW+s'}, 'name': 's3'}, 'compression': {'enabled': True, 'min_size_kbytes': 100}}
[MainThread] [anchore_engine.subsys.object_store.migration/initiate_migration()] [INFO] Migration Task Id: 1
[MainThread] [anchore_engine.subsys.object_store.migration/initiate_migration()] [INFO] Entering main migration loop
[MainThread] [anchore_engine.subsys.object_store.migration/initiate_migration()] [INFO] Migrating 7 documents
[MainThread] [anchore_engine.subsys.object_store.migration/initiate_migration()] [INFO] Deleting document on source after successful migration to destination. Src = db://admin/policy_bundles/2c53a13c-1765-11e8-82ef-23527761d060
[MainThread] [anchore_engine.subsys.object_store.migration/initiate_migration()] [INFO] Deleting document on source after successful migration to destination. Src = db://admin/manifest_data/sha256:0873c923e00e0fd2ba78041bfb64a105e1ecb7678916d1f7776311e45bf5634b
[MainThread] [anchore_engine.subsys.object_store.migration/initiate_migration()] [INFO] Deleting document on source after successful migration to destination. Src = db://admin/analysis_data/sha256:0873c923e00e0fd2ba78041bfb64a105e1ecb7678916d1f7776311e45bf5634b
[MainThread] [anchore_engine.subsys.object_store.migration/initiate_migration()] [INFO] Deleting document on source after successful migration to destination. Src = db://admin/image_content_data/sha256:0873c923e00e0fd2ba78041bfb64a105e1ecb7678916d1f7776311e45bf5634b
[MainThread] [anchore_engine.subsys.object_store.migration/initiate_migration()] [INFO] Deleting document on source after successful migration to destination. Src = db://admin/manifest_data/sha256:a0cd2c88c5cc65499e959ac33c8ebab45f24e6348b48d8c34fd2308fcb0cc138
[MainThread] [anchore_engine.subsys.object_store.migration/initiate_migration()] [INFO] Deleting document on source after successful migration to destination. Src = db://admin/analysis_data/sha256:a0cd2c88c5cc65499e959ac33c8ebab45f24e6348b48d8c34fd2308fcb0cc138
[MainThread] [anchore_engine.subsys.object_store.migration/initiate_migration()] [INFO] Deleting document on source after successful migration to destination. Src = db://admin/image_content_data/sha256:a0cd2c88c5cc65499e959ac33c8ebab45f24e6348b48d8c34fd2308fcb0cc138
[MainThread] [anchore_engine.subsys.object_store.migration/initiate_migration()] [INFO] Migration result summary: {&amp;quot;last_state&amp;quot;: &amp;quot;running&amp;quot;, &amp;quot;executor_id&amp;quot;: &amp;quot;3209ad44d7bb:37:139731996518208:&amp;quot;, &amp;quot;archive_documents_migrated&amp;quot;: 7, &amp;quot;last_updated&amp;quot;: &amp;quot;2018-08-15T18:03:52.951364&amp;quot;, &amp;quot;online_migration&amp;quot;: null, &amp;quot;created_at&amp;quot;: &amp;quot;2018-08-15T18:03:52.951354&amp;quot;, &amp;quot;migrate_from_driver&amp;quot;: &amp;quot;db&amp;quot;, &amp;quot;archive_documents_to_migrate&amp;quot;: 7, &amp;quot;state&amp;quot;: &amp;quot;complete&amp;quot;, &amp;quot;migrate_to_driver&amp;quot;: &amp;quot;s3&amp;quot;, &amp;quot;ended_at&amp;quot;: &amp;quot;2018-08-15T18:03:53.720554&amp;quot;, &amp;quot;started_at&amp;quot;: &amp;quot;2018-08-15T18:03:52.949956&amp;quot;, &amp;quot;type&amp;quot;: &amp;quot;archivemigrationtask&amp;quot;, &amp;quot;id&amp;quot;: 1}
[MainThread] [anchore_manager.cli.objectstorage/migrate()] [INFO] After this migration, your anchore-engine config.yaml MUST have the following configuration options added before starting up again:
compression:
enabled: true
min_size_kbytes: 100
storage_driver:
config:
access_key: 9EB92C7W61YPFQ6QLDOU
bucket: anchore-engine-testing
create_bucket: true
prefix: internaltest
region: false
secret_key: TuHo2UbBx+amD3YiCeidy+R3q82MPTPiyd+dlW+s
url: http://minio-ephemeral-test:9000/
name: s3
&lt;/code>&lt;/pre>&lt;p>&lt;strong>Note:&lt;/strong> If something goes wrong you can reverse the parameters of the migrate command to migrate back to the original configuration (e.g. &amp;hellip; migrate /config/dest-config.yaml /config/config.yaml)&lt;/p>
&lt;h4 id="step-5-get-coffee">Step 5: Get coffee!&lt;/h4>
&lt;p>The migration time will depend on the amount of data and the source and destination systems performance.&lt;/p>
&lt;h4 id="step-6-view-results-summary">Step 6: View results summary&lt;/h4>
&lt;pre>&lt;code>[root@3209ad44d7bb ~]# anchore-manager objectstorage --db-connect ${db} list-migrations
[MainThread] [anchore_manager.cli.utils/connect_database()] [INFO] DB params: {&amp;quot;db_pool_size&amp;quot;: 30, &amp;quot;db_connect&amp;quot;: &amp;quot;postgresql+pg8000://postgres:postgres.dev@postgres-dev:5432/postgres&amp;quot;, &amp;quot;db_connect_args&amp;quot;: {&amp;quot;ssl&amp;quot;: false, &amp;quot;connect_timeout&amp;quot;: 120, &amp;quot;timeout&amp;quot;: 30}, &amp;quot;db_pool_max_overflow&amp;quot;: 100}
[MainThread] [anchore_manager.cli.utils/connect_database()] [INFO] DB connection configured: True
[MainThread] [anchore_manager.cli.utils/connect_database()] [INFO] DB attempting to connect...
[MainThread] [anchore_manager.cli.utils/connect_database()] [INFO] DB connected: True
id state start time end time from to migrated count total to migrate last updated
1 complete 2018-08-15T18:03:52.949956 2018-08-15T18:03:53.720554 db s3 7 7 2018-08-15T18:03:53.724628
&lt;/code>&lt;/pre>&lt;p>This lists all migrations for the service and the number of objects migrated. If you&amp;rsquo;ve run multiple migrations you&amp;rsquo;ll see multiple rows in this response.&lt;/p>
&lt;h3 id="step-7-replace-old-configyaml-with-updated-dest-configyaml">Step 7: Replace old config.yaml with updated dest-config.yaml&lt;/h3>
&lt;pre>&lt;code>[root@3209ad44d7bb ~]# cp /config/config.yaml /config/config.old.yaml
[root@3209ad44d7bb ~]# cp /config/dest-config.yaml /config/config.yaml
&lt;/code>&lt;/pre>&lt;h4 id="step-8-restart-anchore-engine-services">Step 8: Restart anchore-engine services&lt;/h4>
&lt;p>&lt;code>[user@host aevolume]$ docker-compose start anchore-engine&lt;/code>&lt;/p>
&lt;p>The system should now be up and running using the new configuration! You can verify with the anchore-cli by fetching a policy bundle, which will have been migrated:&lt;/p>
&lt;pre>&lt;code>[root@d8d3f49d9328 /]# anchore-cli policy list
Policy ID Active Created Updated
2c53a13c-1765-11e8-82ef-23527761d060 True 2018-08-15T17:16:33Z 2018-08-15T18:11:01Z
[root@d8d3f49d9328 /]# anchore-cli policy get 2c53a13c-1765-11e8-82ef-23527761d060 --detail
{
&amp;quot;blacklisted_images&amp;quot;: [],
&amp;quot;comment&amp;quot;: &amp;quot;Default bundle&amp;quot;,
&amp;quot;id&amp;quot;: &amp;quot;2c53a13c-1765-11e8-82ef-23527761d060&amp;quot;,
... &amp;lt;lots of json&amp;gt;
&lt;/code>&lt;/pre>&lt;p>If that returns the content properly, then you&amp;rsquo;re all done!&lt;/p>
&lt;h3 id="things-to-watch-for-in-a-multi-node-anchore-engine-installation">Things to Watch for in a Multi-Node Anchore Engine Installation&lt;/h3>
&lt;ul>
&lt;li>Before migration:
Ensure all services are down before starting migration&lt;/li>
&lt;li>At migration:
Ensure the place you&amp;rsquo;re running the migration from has the same db access and network access to the archive locations&lt;/li>
&lt;li>After migration:
Ensure that all components get the update config.yaml. Strictly speaking, only containers that run the catalog service need the update configuration, but its best to ensure that any config.yaml in the system which has a services.catalog definition also has the proper and up-to-date configuration to avoid confusion or accidental reverting of the config.&lt;/li>
&lt;/ul>
&lt;h3 id="example-process-with-docker-compose">Example Process with docker-compose&lt;/h3>
&lt;pre>&lt;code># ls docker-compose.yaml
docker-compose.yaml
# docker-compose ps
Name Command State Ports
--------------------------------------------------------------------------------------------------------------------------
aevolumepy3_anchore-db_1 docker-entrypoint.sh postgres Up 5432/tcp
aevolumepy3_anchore-engine_1 /bin/sh -c anchore-engine Up 0.0.0.0:8228-&amp;gt;8228/tcp, 0.0.0.0:8338-&amp;gt;8338/tcp
aevolumepy3_anchore-minio_1 /usr/bin/docker-entrypoint ... Up 0.0.0.0:9000-&amp;gt;9000/tcp
aevolumepy3_anchore-prometheus_1 /bin/prometheus --config.f ... Up 0.0.0.0:9090-&amp;gt;9090/tcp
aevolumepy3_anchore-redis_1 docker-entrypoint.sh redis ... Up 6379/tcp
aevolumepy3_anchore-ui_1 /bin/sh -c node /home/node ... Up 0.0.0.0:3000-&amp;gt;3000/tcp
# docker-compose stop anchore-engine
Stopping aevolume_anchore-engine_1 ... done
# docker-compose run anchore-engine anchore-manager objectstorage --db-connect postgresql+pg8000://postgres:mysecretpassword@anchore-db:5432/postgres check /config/config.yaml.new
[MainThread] [anchore_manager.cli.utils/connect_database()] [INFO] DB params: {&amp;quot;db_connect&amp;quot;: &amp;quot;postgresql+pg8000://postgres:mysecretpassword@anchore-db:5432/postgres&amp;quot;, &amp;quot;db_connect_args&amp;quot;: {&amp;quot;timeout&amp;quot;: 30, &amp;quot;ssl&amp;quot;: false}, &amp;quot;db_pool_size&amp;quot;: 30, &amp;quot;db_pool_max_overflow&amp;quot;: 100}
[MainThread] [anchore_manager.cli.utils/connect_database()] [INFO] DB connection configured: True
[MainThread] [anchore_manager.cli.utils/connect_database()] [INFO] DB attempting to connect...
[MainThread] [anchore_manager.cli.utils/connect_database()] [INFO] DB connected: True
[MainThread] [anchore_manager.cli.objectstorage/check()] [INFO] Using config file /config/config.yaml.new
...
...
# docker-compose run anchore-engine anchore-manager objectstorage --db-connect postgresql+pg8000://postgres:mysecretpassword@anchore-db:5432/postgres migrate /config/config.yaml /config/config.yaml.new
[MainThread] [anchore_manager.cli.utils/connect_database()] [INFO] DB params: {&amp;quot;db_connect&amp;quot;: &amp;quot;postgresql+pg8000://postgres:mysecretpassword@anchore-db:5432/postgres&amp;quot;, &amp;quot;db_connect_args&amp;quot;: {&amp;quot;timeout&amp;quot;: 30, &amp;quot;ssl&amp;quot;: false}, &amp;quot;db_pool_size&amp;quot;: 30, &amp;quot;db_pool_max_overflow&amp;quot;: 100}
[MainThread] [anchore_manager.cli.utils/connect_database()] [INFO] DB connection configured: True
[MainThread] [anchore_manager.cli.utils/connect_database()] [INFO] DB attempting to connect...
[MainThread] [anchore_manager.cli.utils/connect_database()] [INFO] DB connected: True
[MainThread] [anchore_manager.cli.objectstorage/migrate()] [INFO] Loading configs
[MainThread] [anchore_engine.configuration.localconfig/validate_config()] [WARN] no webhooks defined in configuration file - notifications will be disabled
[MainThread] [anchore_engine.configuration.localconfig/validate_config()] [WARN] no webhooks defined in configuration file - notifications will be disabled
[MainThread] [anchore_manager.cli.objectstorage/migrate()] [INFO] Migration from config: {
&amp;quot;compression&amp;quot;: {
&amp;quot;enabled&amp;quot;: false,
&amp;quot;min_size_kbytes&amp;quot;: 100
},
&amp;quot;storage_driver&amp;quot;: {
&amp;quot;name&amp;quot;: &amp;quot;db&amp;quot;,
&amp;quot;config&amp;quot;: {}
}
}
[MainThread] [anchore_manager.cli.objectstorage/migrate()] [INFO] Migration to config: {
&amp;quot;compression&amp;quot;: {
&amp;quot;enabled&amp;quot;: true,
&amp;quot;min_size_kbytes&amp;quot;: 100
},
&amp;quot;storage_driver&amp;quot;: {
&amp;quot;name&amp;quot;: &amp;quot;s3&amp;quot;,
&amp;quot;config&amp;quot;: {
&amp;quot;access_key&amp;quot;: &amp;quot;Z54LPSMFKXSP2E2L4TGX&amp;quot;,
&amp;quot;secret_key&amp;quot;: &amp;quot;EMaLAWLVhUmV/f6hnEqjJo5+/WeZ7ukyHaBKlscB&amp;quot;,
&amp;quot;url&amp;quot;: &amp;quot;http://anchore-minio:9000&amp;quot;,
&amp;quot;region&amp;quot;: false,
&amp;quot;bucket&amp;quot;: &amp;quot;anchorearchive&amp;quot;,
&amp;quot;create_bucket&amp;quot;: true
}
}
}
Performing this operation requires *all* anchore-engine services to be stopped - proceed? (y/N) y
...
...
...
[MainThread] [anchore_engine.subsys.object_store.migration/initiate_migration()] [INFO] Migration result summary: {&amp;quot;last_updated&amp;quot;: &amp;quot;2018-08-14T22:19:39.985250&amp;quot;, &amp;quot;started_at&amp;quot;: &amp;quot;2018-08-14T22:19:39.984603&amp;quot;, &amp;quot;last_state&amp;quot;: &amp;quot;running&amp;quot;, &amp;quot;online_migration&amp;quot;: null, &amp;quot;archive_documents_migrated&amp;quot;: 500, &amp;quot;migrate_to_driver&amp;quot;: &amp;quot;s3&amp;quot;, &amp;quot;id&amp;quot;: 9, &amp;quot;executor_id&amp;quot;: &amp;quot;e9fc8f77714d:1:140375539468096:&amp;quot;, &amp;quot;ended_at&amp;quot;: &amp;quot;2018-08-14T22:20:03.957291&amp;quot;, &amp;quot;created_at&amp;quot;: &amp;quot;2018-08-14T22:19:39.985246&amp;quot;, &amp;quot;state&amp;quot;: &amp;quot;complete&amp;quot;, &amp;quot;archive_documents_to_migrate&amp;quot;: 500, &amp;quot;migrate_from_driver&amp;quot;: &amp;quot;db&amp;quot;, &amp;quot;type&amp;quot;: &amp;quot;archivemigrationtask&amp;quot;}
[MainThread] [anchore_manager.cli.objectstorage/migrate()] [INFO] After this migration, your anchore-engine config.yaml MUST have the following configuration options added before starting up again:
...
...
# cp config/config.yaml config/config.yaml.original
# cp config/config.yaml.new config/config.yaml
# docker-compose start anchore-engine
Starting anchore-engine ... done
&lt;/code>&lt;/pre>&lt;h2 id="migrating-analysis-archive-data">Migrating Analysis Archive Data&lt;/h2>
&lt;p>The object storage migration process migrates any data stored in the source config to the destination configuration, if
the analysis archive is configured to use the same storage backend as the primary object store then that data is migrated
along with all other data, but if the source or destination configurations define different storage backends for the
analysis archive than that which is used by the primary object store, then additional paramters are necesary in the
migration commands to indicate which configurations to migrate to/from.&lt;/p>
&lt;p>The most common migration patterns are:&lt;/p>
&lt;ol>
&lt;li>
&lt;p>Migrate from a single backend configuration to a split configuration to move analysis archive data to an external system (db -&amp;gt; db + s3)&lt;/p>
&lt;/li>
&lt;li>
&lt;p>Migrate from a dual-backend configuration to a single-backend configuration with a different config (e.g. db + s3 -&amp;gt; s3)&lt;/p>
&lt;/li>
&lt;/ol>
&lt;h3 id="migrating-a-single-backend-to-split-backend">Migrating a single backend to split backend&lt;/h3>
&lt;p>For example, moving from unified db backend (default config) to a db + s3 configuration with s3 for the analysis archive .&lt;/p>
&lt;p>source-config.yaml snippet:&lt;/p>
&lt;pre>&lt;code>...
services:
...
catalog:
...
object_store:
compression:
enabled: false
min_size_kbytes: 100
storage_driver:
name: db
config: {}
...
&lt;/code>&lt;/pre>&lt;p>dest-config.yaml snippet:&lt;/p>
&lt;pre>&lt;code>...
services:
...
catalog:
...
object_store:
compression:
enabled: false
min_size_kbytes: 100
storage_driver:
name: db
config: {}
analysis_archive:
enabled: true
compression:
enabled: false
min_size_kbytes: 100
storage_driver:
name: s3
config:
access_key: 9EB92C7W61YPFQ6QLDOU
secret_key: TuHo2UbBx+amD3YiCeidy+R3q82MPTPiyd+dlW+s
url: 'http://minio-ephemeral-test:9000'
region: null
bucket: analysisarchive
...
&lt;/code>&lt;/pre>&lt;p>Anchore stores its internal data in logical &amp;lsquo;buckets&amp;rsquo; that are overlayed onto the storage backed in a driver-specific
way, so to migrate specific internal buckets (effectively these are classes of data), use the &amp;ndash;bucket option in the
manager cli. This should generally not be necessary, but for specific kinds of migrations it may be needed.&lt;/p>
&lt;p>The following command will execute the migration. Note that the &amp;ndash;bucket option is for an internal Anchore logical-bucket, not
and actual bucket in S3:&lt;/p>
&lt;pre>&lt;code>anchore-manager objectstorage --db-connect migrate --to-analysis-archive --bucket analysis_archive source-config.yaml dest-config.yaml
&lt;/code>&lt;/pre>&lt;h3 id="migrating-from-dual-object-storage-backends-to-a-single-backend">Migrating from dual object storage backends to a single backend&lt;/h3>
&lt;p>For example, migrating from a db + s3 backend to a single s3 backend in a different bucket:&lt;/p>
&lt;p>Example source-config.yaml snippet:&lt;/p>
&lt;pre>&lt;code>...
services:
...
catalog:
...
object_store:
compression:
enabled: false
min_size_kbytes: 100
storage_driver:
name: db
config: {}
analysis_archive:
enabled: true
compression:
enabled: false
min_size_kbytes: 100
storage_driver:
name: s3
config:
access_key: 9EB92C7W61YPFQ6QLDOU
secret_key: TuHo2UbBx+amD3YiCeidy+R3q82MPTPiyd+dlW+s
url: 'http://minio-ephemeral-test:9000'
region: null
bucket: analysisarchive
...
&lt;/code>&lt;/pre>&lt;p>The dest config is a single backend. In this case, note the S3 bucket has changed so all data must be migrated.&lt;/p>
&lt;p>Example dest-config.yaml snippet:&lt;/p>
&lt;pre>&lt;code>...
services:
...
catalog:
...
object_store:
enabled: true
compression:
enabled: false
min_size_kbytes: 100
storage_driver:
name: s3
config:
access_key: 9EB92C7W61YPFQ6QLDOU
secret_key: TuHo2UbBx+amD3YiCeidy+R3q82MPTPiyd+dlW+s
url: 'http://minio-ephemeral-test:9000'
region: null
bucket: newanchorebucket
...
&lt;/code>&lt;/pre>&lt;p>First, migrate the object data in the db on the source:&lt;/p>
&lt;pre>&lt;code>anchore-manager objectstorage --db-connect migrate source-config.yaml dest-config.yaml
&lt;/code>&lt;/pre>&lt;p>Next, migrate the object data in the analysis archive from the old config (s3 bucket &amp;lsquo;analysisarchive&amp;rsquo; to the new config
(s3 bucket &amp;lsquo;newanchorebucket&amp;rsquo;):&lt;/p>
&lt;pre>&lt;code>anchore-manager objectstorage --db-connect migrate --from-analysis-archive source-config.yaml dest-config.yaml
&lt;/code>&lt;/pre></description></item><item><title>Docs: CircleCI</title><link>/docs/usage/integration/ci_cd/circleci/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/docs/usage/integration/ci_cd/circleci/</guid><description>
&lt;p>Integrating Anchore engine into your CircleCI pipeline can be accomplished seamlessly using the official Anchore Engine Orb.&lt;/p>
&lt;p>Orb source code &amp;amp; usage examples can be found on our &lt;a href="https://github.com/anchore/ci-tools/tree/master/circleci-orbs/anchore-engine">GitHub Repository&lt;/a>.&lt;/p>
&lt;p>To read more about CircleCI orbs, view the official documentation - &lt;a href="https://circleci.com/docs/2.0/orb-intro/">CircleCi Orbs&lt;/a>&lt;/p></description></item><item><title>Docs: Feed Configuration</title><link>/docs/install/configuration/feed_configuration/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/docs/install/configuration/feed_configuration/</guid><description>
&lt;h3 id="feed-synchronization-interval">Feed Synchronization Interval&lt;/h3>
&lt;p>The default configuration for the Anchore Engine will download vulnerability data from Anchore&amp;rsquo;s feed service every 21,600 seconds (6hours).&lt;/p>
&lt;p>For most users the only configuration option that is typically updated is the feed synchronization interval - the time interval (in seconds) at which the feed sync is run.&lt;/p>
&lt;pre>&lt;code class="language-policy_engine:" data-lang="policy_engine:"> .....
cycle_timers:
...
feed_sync: 14400
&lt;/code>&lt;/pre>&lt;h3 id="feed-settings">Feed Settings&lt;/h3>
&lt;p>Feed sync configuration is set in the config.yaml file used by policy engine service. The &lt;code>services.policy_engine.vulnerabilities.sync.data&lt;/code> section
of the configuration file in the policy engine&amp;rsquo;s container controls the behavior of feed syncs done by that particular container. Ensure this config is synchronized between containers if you are running more than one policy engine. This is usually handled for you by Helm Charts on Kubernetes, for example.&lt;/p>
&lt;p>The Anchore Engine will default to downloading feed data from Anchore&amp;rsquo;s feed service hosted at &lt;a href="https://ancho.re/v1/service/feeds">https://ancho.re/v1/service/feeds&lt;/a> and running in AWS in the
us-west-2 region.&lt;/p>
&lt;p>By default, Anchore Engine will only sync the non-grype feeds enabled in the config section shown below. Setting additional feed types to true or false will
enable or disable, respectively, synchronization of the specified feed.&lt;/p>
&lt;pre>&lt;code>services:
...
policy_engine:
...
vulnerabilities:
...
sync:
...
data:
grypedb:
enabled: true
url: ${ANCHORE_GRYPE_DB_URL}
packages:
enabled: true
url: ${ANCHORE_FEEDS_URL}
&lt;/code>&lt;/pre>&lt;p>&lt;em>&lt;strong>Note:&lt;/strong>&lt;/em> As shown above, Anchore Engine&amp;rsquo;s default is now Grype. The Grype feed is the default, and the only one that Anchore Engine syncs.&lt;/p>
&lt;h4 id="read-timeout">Read Timeout&lt;/h4>
&lt;p>Under rare circumstances you may see syncs failing with errors to fetch data due to timeouts. This is typically due to load on the feed service, network issues, or
some other temporary condition. However, if you want to increase the timeout to improve the likelihood of success, modify the &lt;em>read_timeout_seconds&lt;/em> of the feeds configuration:&lt;/p>
&lt;pre>&lt;code>feeds:
...
read_timeout_seconds: 180
&lt;/code>&lt;/pre>&lt;h3 id="controlling-which-feeds-and-groups-are-synced">Controlling Which Feeds and Groups are Synced&lt;/h3>
&lt;p>Note: The package and nvd data feeds are large, resulting in the initial sync taking some time.&lt;/p>
&lt;p>During initial feed sync, you can always query the progress and status of the feed sync using the anchore-cli.&lt;/p>
&lt;pre>&lt;code>anchore@93d6977e2061 anchore-engine]$ anchore-cli system feeds list
Feed Group LastSync RecordCount
github github:composer 2020-03-27T22:19:57.328440 78
github github:gem 2020-03-27T22:19:59.069349 333
github github:java 2020-03-27T22:20:03.393652 432
github github:npm 2020-03-27T22:20:09.422600 653
github github:nuget 2020-03-27T22:20:16.628054 50
github github:python 2020-03-27T22:20:17.754270 250
nvdv2 nvdv2:cves 2020-03-27T20:42:13.104384 141090
vulnerabilities alpine:3.10 2020-03-27T19:47:27.188488 1725
vulnerabilities alpine:3.11 2020-03-27T19:47:42.467000 1904
vulnerabilities alpine:3.3 2020-03-27T19:47:59.309026 457
vulnerabilities alpine:3.4 2020-03-27T19:48:03.531092 681
vulnerabilities alpine:3.5 2020-03-27T19:48:09.396503 875
vulnerabilities alpine:3.6 2020-03-27T19:48:17.029289 1051
vulnerabilities alpine:3.7 2020-03-27T19:48:27.230411 1395
vulnerabilities alpine:3.8 2020-03-27T19:48:39.811189 1486
vulnerabilities alpine:3.9 2020-03-27T19:48:53.472895 1558
vulnerabilities amzn:2 2020-03-27T19:49:08.039725 320
vulnerabilities centos:5 2020-03-27T19:49:19.232142 1347
vulnerabilities centos:6 2020-03-27T19:49:45.948061 1393
vulnerabilities centos:7 2020-03-27T19:50:16.913685 1004
vulnerabilities centos:8 2020-03-27T19:50:47.762328 199
vulnerabilities debian:10 2020-03-27T19:50:56.298579 22407
vulnerabilities debian:11 2020-03-27T19:55:05.935002 19443
vulnerabilities debian:7 2020-03-27T19:58:55.500030 20455
vulnerabilities debian:8 2020-03-27T20:01:58.427061 23481
vulnerabilities debian:9 2020-03-27T20:05:20.356754 22507
vulnerabilities debian:unstable 2020-03-27T20:09:37.909305 23701
vulnerabilities ol:5 2020-03-27T20:12:55.707457 1245
vulnerabilities ol:6 2020-03-27T20:13:25.500670 1504
vulnerabilities ol:7 2020-03-27T20:14:03.279049 1121
vulnerabilities ol:8 2020-03-27T20:14:37.144376 157
vulnerabilities rhel:5 2020-03-27T20:14:43.707760 7237
vulnerabilities rhel:6 2020-03-27T20:16:59.010218 6805
vulnerabilities rhel:7 2020-03-27T20:18:09.917886 5846
vulnerabilities rhel:8 2020-03-27T20:19:12.650326 1428
vulnerabilities ubuntu:12.04 2020-03-27T20:19:28.540257 14948
vulnerabilities ubuntu:12.10 2020-03-27T20:21:27.080478 5652
vulnerabilities ubuntu:13.04 2020-03-27T20:23:09.806360 4127
vulnerabilities ubuntu:14.04 2020-03-27T20:23:40.672987 21176
vulnerabilities ubuntu:14.10 2020-03-27T20:27:27.221192 4456
vulnerabilities ubuntu:15.04 2020-03-27T20:28:05.360075 5877
vulnerabilities ubuntu:15.10 2020-03-27T20:28:53.416816 6513
vulnerabilities ubuntu:16.04 2020-03-27T20:29:51.105326 18288
vulnerabilities ubuntu:16.10 2020-03-27T20:33:29.612544 8647
vulnerabilities ubuntu:17.04 2020-03-27T20:35:33.512059 9157
vulnerabilities ubuntu:17.10 2020-03-27T20:36:39.141950 7936
vulnerabilities ubuntu:18.04 2020-03-27T20:37:35.077867 12547
vulnerabilities ubuntu:18.10 2020-03-27T20:39:20.097963 8397
vulnerabilities ubuntu:19.04 2020-03-27T20:40:18.628869 8664
vulnerabilities ubuntu:19.10 2020-03-27T20:41:20.828796 7327
&lt;/code>&lt;/pre>&lt;p>&lt;em>&lt;strong>Note:&lt;/strong>&lt;/em> The Grype feed is the only feed that will be synced. It will
contain the records from all the other groups. It is not possible to include or exclude groups from the Grype feed.&lt;/p>
&lt;h3 id="using-the-config-file-to-includeexclude-feeds-at-system-bootstrap">Using the Config File to Include/Exclude Feeds at System Bootstrap&lt;/h3>
&lt;p>The most common way to set which feeds are synced is in the config.yaml for the policy engine. By default,
the &lt;em>grypedb&lt;/em> and &lt;em>packages&lt;/em> feeds are synced to provide good vulnerability matching support for a variety of linux distros
and application package types. Normally it will not be necessary to modify that set.&lt;/p>
&lt;p>To disable a feed or enable a disabled feed, modify the config.yaml&amp;rsquo;s &lt;em>feeds&lt;/em> section to:&lt;/p>
&lt;pre>&lt;code>feeds:
selective_sync:
enabled: true
feeds:
grypedb: true
packages: true
&lt;/code>&lt;/pre>&lt;p>Those boolean values can be used to enable/disable the feeds. Note that changes will require a restart of the policy engine to take effect and setting
a feed to &amp;lsquo;false&amp;rsquo; will not remove any data or show in the API/CLI, it will simply skip updates during sync operations.&lt;/p>
&lt;h3 id="using-the-cli-to-configure-feeds">Using the CLI to Configure Feeds&lt;/h3>
&lt;h4 id="disabling-an-entire-feed">Disabling an Entire Feed&lt;/h4>
&lt;p>An entire feed can be disabled. This means that all the feed&amp;rsquo;s groups will no longer be updated and no new groups will be synced either on subsequent sync operations.
This does not, however, remove any existing data nor will it remove the feed or feed group metadata records.&lt;/p>
&lt;p>Example:&lt;/p>
&lt;pre>&lt;code>[anchore@93d6977e2061 anchore-engine]$ anchore-cli system feeds config github --disable
Feed Group LastSync RecordCount
github(disabled) github:composer 2020-03-27T22:19:57.328440 78
github(disabled) github:gem 2020-03-27T22:19:59.069349 333
github(disabled) github:java 2020-03-27T22:20:03.393652 432
github(disabled) github:npm 2020-03-27T22:20:09.422600 653
github(disabled) github:nuget 2020-03-27T22:20:16.628054 50
github(disabled) github:python 2020-03-27T22:20:17.754270 250
&lt;/code>&lt;/pre>&lt;p>The feed can be enabled again using a similar command and on the next sync operation its data will be updated.
Example:&lt;/p>
&lt;pre>&lt;code>[anchore@93d6977e2061 anchore-engine]$ anchore-cli system feeds config github --enable
Feed Group LastSync RecordCount
github github:composer 2020-03-27T22:19:57.328440 78
github github:gem 2020-03-27T22:19:59.069349 333
github github:java 2020-03-27T22:20:03.393652 432
github github:npm 2020-03-27T22:20:09.422600 653
github github:nuget 2020-03-27T22:20:16.628054 50
github github:python 2020-03-27T22:20:17.754270 250
[anchore@93d6977e2061 anchore-engine]$ anchore-cli system feeds sync
WARNING: This operation should not normally need to be performed except when the anchore-engine operator is certain that it is required - the operation will take a long time (hours) to complete, and there may be an impact on anchore-engine performance during the re-sync/flush.
Really perform a manual feed data sync/flush? (y/N)y
Feed Group Status Records Updated Sync Duration
github github:composer success 0 0.59s
github github:gem success 0 0.47s
github github:java success 0 0.60s
github github:npm success 0 0.52s
github github:nuget success 0 0.45s
github github:python success 0 0.50s
nvdv2 nvdv2:cves success 0 0.68s
vulnerabilities alpine:3.10 success 0 0.56s
vulnerabilities alpine:3.11 success 0 0.50s
...
&lt;/code>&lt;/pre>&lt;h4 id="disabling-specific-feed-groups">Disabling Specific Feed Groups&lt;/h4>
&lt;p>For a more granular approach, you can disable a single group within a feed.&lt;/p>
&lt;h3 id="using-the-cli-to-delete-feed-data">Using the CLI to Delete Feed Data&lt;/h3>
&lt;h4 id="deleting-and-entire-feed">Deleting and Entire Feed&lt;/h4>
&lt;p>Deleting feed data&lt;/p>
&lt;pre>&lt;code>[anchore@93d6977e2061 anchore-engine]$ anchore-cli system feeds delete github
[anchore@93d6977e2061 anchore-engine]$ anchore-cli system feeds list
Feed Group LastSync RecordCount
nvdv2 nvdv2:cves 2020-03-28T00:03:34.079006 141090
vulnerabilities alpine:3.10 2020-03-28T00:03:32.065414 1725
vulnerabilities alpine:3.11 2020-03-28T00:03:32.685733 1904
vulnerabilities alpine:3.3 2020-03-28T00:02:03.906147 457
vulnerabilities alpine:3.4 2020-03-28T00:02:03.160375 681
vulnerabilities alpine:3.5 2020-03-28T00:02:04.693883 875
vulnerabilities alpine:3.6 2020-03-28T00:02:06.155002 1051
vulnerabilities alpine:3.7 2020-03-28T00:02:06.717063 1395
vulnerabilities alpine:3.8 2020-03-28T00:02:07.329353 1486
vulnerabilities alpine:3.9 2020-03-28T00:02:05.434095 1558
vulnerabilities amzn:2 2020-03-28T00:02:08.671245 320
vulnerabilities centos:5 2020-03-28T00:02:09.375775 1347
vulnerabilities centos:6 2020-03-28T00:02:10.061647 1393
vulnerabilities centos:7 2020-03-28T00:02:10.656073 1004
vulnerabilities centos:8 2020-03-28T00:02:11.268366 199
vulnerabilities debian:10 2020-03-28T00:03:33.244902 22414
vulnerabilities debian:11 2020-03-28T00:02:11.914975 19450
vulnerabilities debian:7 2020-03-28T00:02:12.732607 20455
vulnerabilities debian:8 2020-03-28T00:02:13.759757 23488
vulnerabilities debian:9 2020-03-28T00:02:07.960085 22514
vulnerabilities debian:unstable 2020-03-28T00:02:14.585239 23708
vulnerabilities ol:5 2020-03-28T00:02:15.882825 1245
vulnerabilities ol:6 2020-03-28T00:02:15.368850 1504
vulnerabilities ol:7 2020-03-28T00:02:17.334177 1121
vulnerabilities ol:8 2020-03-28T00:02:18.057855 157
vulnerabilities rhel:5 2020-03-28T00:02:18.748398 7237
vulnerabilities rhel:6 2020-03-28T00:02:16.548115 6805
vulnerabilities rhel:7 2020-03-28T00:03:20.039569 5846
vulnerabilities rhel:8 2020-03-28T00:03:21.424688 1428
vulnerabilities ubuntu:12.04 2020-03-28T00:03:30.795672 14948
vulnerabilities ubuntu:12.10 2020-03-28T00:03:20.686089 5652
vulnerabilities ubuntu:13.04 2020-03-28T00:03:22.630122 4127
vulnerabilities ubuntu:14.04 2020-03-28T00:03:23.376621 21176
vulnerabilities ubuntu:14.10 2020-03-28T00:03:24.059663 4456
vulnerabilities ubuntu:15.04 2020-03-28T00:03:22.070692 5877
vulnerabilities ubuntu:15.10 2020-03-28T00:03:24.656382 6513
vulnerabilities ubuntu:16.04 2020-03-28T00:03:26.013850 18288
vulnerabilities ubuntu:16.10 2020-03-28T00:03:25.370678 8647
vulnerabilities ubuntu:17.04 2020-03-28T00:03:27.278963 9157
vulnerabilities ubuntu:17.10 2020-03-28T00:03:26.605719 7936
vulnerabilities ubuntu:18.04 2020-03-28T00:03:27.845497 12547
vulnerabilities ubuntu:18.10 2020-03-28T00:03:28.482261 8397
vulnerabilities ubuntu:19.04 2020-03-28T00:03:31.400152 8664
vulnerabilities ubuntu:19.10 2020-03-28T00:03:29.122119 7327
[anchore@93d6977e2061 anchore-engine]$ anchore-cli system feeds sync
WARNING: This operation should not normally need to be performed except when the anchore-engine operator is certain that it is required - the operation will take a long time (hours) to complete, and there may be an impact on anchore-engine performance during the re-sync/flush.
Really perform a manual feed data sync/flush? (y/N)y
Feed Group Status Records Updated Sync Duration
nvdv2 nvdv2:cves success 0 0.81s
vulnerabilities alpine:3.10 success 0 0.54s
vulnerabilities alpine:3.11 success 0 0.60s
vulnerabilities alpine:3.3 success 0 0.59s
vulnerabilities alpine:3.4 success 0 0.75s
vulnerabilities alpine:3.5 success 0 0.95s
vulnerabilities alpine:3.6 success 0 0.57s
vulnerabilities alpine:3.7 success 0 0.59s
vulnerabilities alpine:3.8 success 0 0.51s
vulnerabilities alpine:3.9 success 0 1.34s
vulnerabilities amzn:2 success 0 0.52s
vulnerabilities centos:5 success 0 0.64s
vulnerabilities centos:6 success 0 0.80s
vulnerabilities centos:7 success 0 0.82s
vulnerabilities centos:8 success 0 0.58s
vulnerabilities debian:10 success 0 0.47s
vulnerabilities debian:11 success 0 0.58s
vulnerabilities debian:7 success 0 0.64s
vulnerabilities debian:8 success 0 0.71s
vulnerabilities debian:9 success 0 0.76s
vulnerabilities debian:unstable success 0 0.78s
vulnerabilities ol:5 success 0 0.65s
vulnerabilities ol:6 success 0 0.68s
vulnerabilities ol:7 success 0 0.69s
vulnerabilities ol:8 success 0 0.68s
vulnerabilities rhel:5 success 0 0.76s
vulnerabilities rhel:6 success 0 0.49s
vulnerabilities rhel:7 success 0 0.61s
vulnerabilities rhel:8 success 0 0.89s
vulnerabilities ubuntu:12.04 success 0 0.76s
vulnerabilities ubuntu:12.10 success 0 0.60s
vulnerabilities ubuntu:13.04 success 0 0.65s
vulnerabilities ubuntu:14.04 success 0 0.59s
vulnerabilities ubuntu:14.10 success 0 1.01s
vulnerabilities ubuntu:15.04 success 0 0.70s
vulnerabilities ubuntu:15.10 success 0 0.60s
vulnerabilities ubuntu:16.04 success 0 0.82s
vulnerabilities ubuntu:16.10 success 0 0.57s
vulnerabilities ubuntu:17.04 success 0 0.61s
vulnerabilities ubuntu:17.10 success 0 0.51s
vulnerabilities ubuntu:18.04 success 0 0.60s
vulnerabilities ubuntu:18.10 success 0 0.60s
vulnerabilities ubuntu:19.04 success 0 0.61s
vulnerabilities ubuntu:19.10 success 0 0.60s
&lt;/code>&lt;/pre>&lt;h4 id="deleting-specific-feed-groups">Deleting Specific Feed Groups&lt;/h4>
&lt;pre>&lt;code>[anchore@93d6977e2061 ~]$ anchore-cli system feeds config --disable vulnerabilities --group centos:5
Group LastSync RecordCount
centos:5(disabled) 2020-03-28T00:22:57.113534 1347
[anchore@93d6977e2061 ~]$ anchore-cli system feeds delete vulnerabilities --group centos:5
Group LastSync RecordCount
centos:5(disabled) pending 0
&lt;/code>&lt;/pre>&lt;h4 id="restoring-deleted-data">Restoring Deleted Data&lt;/h4>
&lt;p>If you want to get data back, simply enable the feed and/or group and run a feed sync manually or wait for the next scheduled sync.&lt;/p>
&lt;p>For an entire feed, here is an example of removal and re-adding it:&lt;/p>
&lt;pre>&lt;code>[anchore@93d6977e2061 ~]$ anchore-cli system feeds config github --disable
Feed Group LastSync RecordCount
github(disabled) github:composer 2020-03-28T01:08:58.652868 78
github(disabled) github:gem 2020-03-28T01:08:59.179493 333
github(disabled) github:java 2020-03-28T01:08:59.699348 432
github(disabled) github:npm 2020-03-28T00:34:48.167115 653
github(disabled) github:nuget 2020-03-28T01:12:01.116613 50
github(disabled) github:python 2020-03-28T01:08:58.083361 250
[anchore@93d6977e2061 ~]$ anchore-cli system feeds delete github
[anchore@93d6977e2061 ~]$ anchore-cli system feeds config github --enable
[anchore@93d6977e2061 ~]$ anchore-cli system feeds sync
WARNING: This operation should not normally need to be performed except when the anchore-engine operator is certain that it is required - the operation will take a long time (hours) to complete, and there may be an impact on anchore-engine performance during the re-sync/flush.
Really perform a manual feed data sync/flush? (y/N)y
Feed Group Status Records Updated Sync Duration
github github:composer success 78 1.64s
github github:gem success 333 4.48s
github github:java success 432 6.07s
github github:npm success 653 7.39s
github github:nuget success 50 1.10s
github github:python success 250 3.34s
nvdv2 nvdv2:cves success 0 60.90s
vulnerabilities alpine:3.10 success 0 0.52s
vulnerabilities alpine:3.11 success 0 0.47s
vulnerabilities alpine:3.3 success 0 0.56s
vulnerabilities alpine:3.4 success 0 0.46s
vulnerabilities alpine:3.5 success 0 0.52s
vulnerabilities alpine:3.6 success 0 0.54s
vulnerabilities alpine:3.7 success 0 60.76s
vulnerabilities alpine:3.8 success 0 0.54s
vulnerabilities alpine:3.9 success 0 0.54s
vulnerabilities amzn:2 success 0 0.49s
vulnerabilities centos:5 success 0 0.47s
vulnerabilities centos:6 success 0 0.49s
vulnerabilities centos:7 success 0 0.48s
vulnerabilities centos:8 success 0 0.53s
vulnerabilities debian:10 success 0 0.62s
vulnerabilities debian:11 success 0 0.50s
...
&lt;/code>&lt;/pre>&lt;p>For a single feed group, here is an example of removal and re-adding it:&lt;/p>
&lt;pre>&lt;code>[anchore@93d6977e2061 ~]$ anchore-cli system feeds config --enable vulnerabilities --group centos:5
Group LastSync RecordCount
centos:5 pending 0
[anchore@93d6977e2061 ~]$ anchore-cli system feeds sync
WARNING: This operation should not normally need to be performed except when the anchore-engine operator is certain that it is required - the operation will take a long time (hours) to complete, and there may be an impact on anchore-engine performance during the re-sync/flush.
Really perform a manual feed data sync/flush? (y/N)y
Feed Group Status Records Updated Sync Duration
...
vulnerabilities centos:5 success 1347 27.41s
...
&lt;/code>&lt;/pre>&lt;p>With these controls you can better customize the data set that anchore stores in the database. However, note that this should not normally be necessary
and modifying feed groups &amp;amp; data has implications on the sets of distros and types of artifacts Anchore can match vulnerabilities against.&lt;/p></description></item><item><title>Docs: Feed Configuration</title><link>/docs/usage/cli_usage/feeds/feed_configuration/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/docs/usage/cli_usage/feeds/feed_configuration/</guid><description>
&lt;h3 id="feed-synchronization-interval">Feed Synchronization Interval&lt;/h3>
&lt;p>The default configuration for the Anchore Engine will download vulnerability data from Anchore&amp;rsquo;s feed service every 21,600 seconds (6hours).&lt;/p>
&lt;p>For most users the only configuration option that is typically updated is the feed synchronization interval - the time interval (in seconds) at which the feed sync is run.&lt;/p>
&lt;pre>&lt;code class="language-policy_engine:" data-lang="policy_engine:"> .....
cycle_timers:
...
feed_sync: 14400
&lt;/code>&lt;/pre>&lt;h3 id="feed-settings">Feed Settings&lt;/h3>
&lt;p>Feed sync configuration is set in the config.yaml file used by policy engine service. The &lt;code>feeds&lt;/code> section of the configuration file in the policy engine&amp;rsquo;s container controls the behavior of feed syncs done by that particular container. Ensure these are synchronized between
containers if you are running more than one policy engine. This is usually handled for you by Helm Charts on Kubernetes, for example.&lt;/p>
&lt;p>The Anchore Engine will default to downloading feed data from Anchore&amp;rsquo;s feed service hosted at &lt;a href="https://ancho.re/v1/service/feeds">https://ancho.re/v1/service/feeds&lt;/a> and running in AWS in the us-west-2 region.&lt;/p>
&lt;p>By default the Anchore Engine will perform a selective sync enabling only the vulnerabilities feed. Setting the (selective_sync) enabled flag to false, or updating the other feed types to True will enable synchronization of the specified feed.&lt;/p>
&lt;pre>&lt;code>feeds:
selective_sync:
# If enabled only sync specific feeds instead of all.
enabled: True
feeds:
vulnerabilities: True
packages: False
nvdv2: True
github: True
url: 'https://ancho.re/v1/service/feeds'
connection_timeout_seconds: 3
read_timeout_seconds: 60
&lt;/code>&lt;/pre>&lt;h4 id="read-timeout">Read Timeout&lt;/h4>
&lt;p>Under rare circumstances you may see syncs failing with errors to fetch data due to timeouts. This is typically due to load on the feed service, network issues, or
some other temporary condition. However, if you want to increase the timeout to increate the likelihood of success, modify the &lt;em>read_timeout_seconds&lt;/em> of the feeds configuration:&lt;/p>
&lt;pre>&lt;code>feeds:
...
read_timeout_seconds: 180
&lt;/code>&lt;/pre>&lt;h3 id="controlling-which-feeds-and-groups-are-synced">Controlling Which Feeds and Groups are Synced&lt;/h3>
&lt;p>Note: The package and nvd data feeds are large, resulting in the initial sync taking some time time.&lt;/p>
&lt;p>During initial feed sync, you can always query the progress and status of the feed sync using the anchore-cli.&lt;/p>
&lt;pre>&lt;code>anchore@93d6977e2061 anchore-engine]$ anchore-cli system feeds list
Feed Group LastSync RecordCount
github github:composer 2020-03-27T22:19:57.328440 78
github github:gem 2020-03-27T22:19:59.069349 333
github github:java 2020-03-27T22:20:03.393652 432
github github:npm 2020-03-27T22:20:09.422600 653
github github:nuget 2020-03-27T22:20:16.628054 50
github github:python 2020-03-27T22:20:17.754270 250
nvdv2 nvdv2:cves 2020-03-27T20:42:13.104384 141090
vulnerabilities alpine:3.10 2020-03-27T19:47:27.188488 1725
vulnerabilities alpine:3.11 2020-03-27T19:47:42.467000 1904
vulnerabilities alpine:3.3 2020-03-27T19:47:59.309026 457
vulnerabilities alpine:3.4 2020-03-27T19:48:03.531092 681
vulnerabilities alpine:3.5 2020-03-27T19:48:09.396503 875
vulnerabilities alpine:3.6 2020-03-27T19:48:17.029289 1051
vulnerabilities alpine:3.7 2020-03-27T19:48:27.230411 1395
vulnerabilities alpine:3.8 2020-03-27T19:48:39.811189 1486
vulnerabilities alpine:3.9 2020-03-27T19:48:53.472895 1558
vulnerabilities amzn:2 2020-03-27T19:49:08.039725 320
vulnerabilities centos:5 2020-03-27T19:49:19.232142 1347
vulnerabilities centos:6 2020-03-27T19:49:45.948061 1393
vulnerabilities centos:7 2020-03-27T19:50:16.913685 1004
vulnerabilities centos:8 2020-03-27T19:50:47.762328 199
vulnerabilities debian:10 2020-03-27T19:50:56.298579 22407
vulnerabilities debian:11 2020-03-27T19:55:05.935002 19443
vulnerabilities debian:7 2020-03-27T19:58:55.500030 20455
vulnerabilities debian:8 2020-03-27T20:01:58.427061 23481
vulnerabilities debian:9 2020-03-27T20:05:20.356754 22507
vulnerabilities debian:unstable 2020-03-27T20:09:37.909305 23701
vulnerabilities ol:5 2020-03-27T20:12:55.707457 1245
vulnerabilities ol:6 2020-03-27T20:13:25.500670 1504
vulnerabilities ol:7 2020-03-27T20:14:03.279049 1121
vulnerabilities ol:8 2020-03-27T20:14:37.144376 157
vulnerabilities rhel:5 2020-03-27T20:14:43.707760 7237
vulnerabilities rhel:6 2020-03-27T20:16:59.010218 6805
vulnerabilities rhel:7 2020-03-27T20:18:09.917886 5846
vulnerabilities rhel:8 2020-03-27T20:19:12.650326 1428
vulnerabilities ubuntu:12.04 2020-03-27T20:19:28.540257 14948
vulnerabilities ubuntu:12.10 2020-03-27T20:21:27.080478 5652
vulnerabilities ubuntu:13.04 2020-03-27T20:23:09.806360 4127
vulnerabilities ubuntu:14.04 2020-03-27T20:23:40.672987 21176
vulnerabilities ubuntu:14.10 2020-03-27T20:27:27.221192 4456
vulnerabilities ubuntu:15.04 2020-03-27T20:28:05.360075 5877
vulnerabilities ubuntu:15.10 2020-03-27T20:28:53.416816 6513
vulnerabilities ubuntu:16.04 2020-03-27T20:29:51.105326 18288
vulnerabilities ubuntu:16.10 2020-03-27T20:33:29.612544 8647
vulnerabilities ubuntu:17.04 2020-03-27T20:35:33.512059 9157
vulnerabilities ubuntu:17.10 2020-03-27T20:36:39.141950 7936
vulnerabilities ubuntu:18.04 2020-03-27T20:37:35.077867 12547
vulnerabilities ubuntu:18.10 2020-03-27T20:39:20.097963 8397
vulnerabilities ubuntu:19.04 2020-03-27T20:40:18.628869 8664
vulnerabilities ubuntu:19.10 2020-03-27T20:41:20.828796 7327
&lt;/code>&lt;/pre>&lt;h3 id="using-the-config-file-to-includeexclude-feeds-at-system-bootstrap">Using the Config File to Include/Exclude Feeds at System Bootstrap&lt;/h3>
&lt;p>The most common way to set which feeds are synced is in the config.yaml for the policy engine. By default,
the &lt;em>vulnerabilities&lt;/em>, &lt;em>nvdv2&lt;/em>, and &lt;em>github&lt;/em> feeds are synced to provide good vulnerability matching support for a variety of linux distros
and application package types. Normally it will not be necessary to modify that set.&lt;/p>
&lt;p>To disable a feed or enable a disabled feed, modify the config.yaml&amp;rsquo;s &lt;em>feeds&lt;/em> section to:&lt;/p>
&lt;pre>&lt;code>feeds:
selective_sync:
enabled: true
feeds:
vulnerabilities: true
nvdv2: true
github: true
packages: false
&lt;/code>&lt;/pre>&lt;p>Those boolean values can be used to enable/disable the feeds. Note that changes will require a restart of the policy engine to take effect and settng
a feed to &amp;lsquo;false&amp;rsquo; will not remove any data or show in the API/CLI, it will simply skip updates during sync operations.&lt;/p>
&lt;h3 id="using-the-cli-to-configure-feeds">Using the CLI to Configure Feeds&lt;/h3>
&lt;h4 id="disabling-an-entire-feed">Disabling an Entire Feed&lt;/h4>
&lt;p>An entire feed can be disabled. This means that all the feed&amp;rsquo;s groups will no longer be updated and no new groups will be synced either on subsequent sync operations.
This does not, however, remove any existing data nor will it remove the feed or feed group metadata records.&lt;/p>
&lt;p>Example:&lt;/p>
&lt;pre>&lt;code>[anchore@93d6977e2061 anchore-engine]$ anchore-cli system feeds config github --disable
Feed Group LastSync RecordCount
github(disabled) github:composer 2020-03-27T22:19:57.328440 78
github(disabled) github:gem 2020-03-27T22:19:59.069349 333
github(disabled) github:java 2020-03-27T22:20:03.393652 432
github(disabled) github:npm 2020-03-27T22:20:09.422600 653
github(disabled) github:nuget 2020-03-27T22:20:16.628054 50
github(disabled) github:python 2020-03-27T22:20:17.754270 250
&lt;/code>&lt;/pre>&lt;p>The feed can be enabled again using a similar command and on the next sync operation its data will be updated.
Example:&lt;/p>
&lt;pre>&lt;code>[anchore@93d6977e2061 anchore-engine]$ anchore-cli system feeds config github --enable
Feed Group LastSync RecordCount
github github:composer 2020-03-27T22:19:57.328440 78
github github:gem 2020-03-27T22:19:59.069349 333
github github:java 2020-03-27T22:20:03.393652 432
github github:npm 2020-03-27T22:20:09.422600 653
github github:nuget 2020-03-27T22:20:16.628054 50
github github:python 2020-03-27T22:20:17.754270 250
[anchore@93d6977e2061 anchore-engine]$ anchore-cli system feeds sync
WARNING: This operation should not normally need to be performed except when the anchore-engine operator is certain that it is required - the operation will take a long time (hours) to complete, and there may be an impact on anchore-engine performance during the re-sync/flush.
Really perform a manual feed data sync/flush? (y/N)y
Feed Group Status Records Updated Sync Duration
github github:composer success 0 0.59s
github github:gem success 0 0.47s
github github:java success 0 0.60s
github github:npm success 0 0.52s
github github:nuget success 0 0.45s
github github:python success 0 0.50s
nvdv2 nvdv2:cves success 0 0.68s
vulnerabilities alpine:3.10 success 0 0.56s
vulnerabilities alpine:3.11 success 0 0.50s
...
&lt;/code>&lt;/pre>&lt;h4 id="disabling-specific-feed-groups">Disabling Specific Feed Groups&lt;/h4>
&lt;p>For a more granular approach, you can disable a single group within a feed.&lt;/p>
&lt;h3 id="using-the-cli-to-delete-feed-data">Using the CLI to Delete Feed Data&lt;/h3>
&lt;h4 id="deleting-and-entire-feed">Deleting and Entire Feed&lt;/h4>
&lt;p>Deleting feed data&lt;/p>
&lt;pre>&lt;code>[anchore@93d6977e2061 anchore-engine]$ anchore-cli system feeds delete github
[anchore@93d6977e2061 anchore-engine]$ anchore-cli system feeds list
Feed Group LastSync RecordCount
nvdv2 nvdv2:cves 2020-03-28T00:03:34.079006 141090
vulnerabilities alpine:3.10 2020-03-28T00:03:32.065414 1725
vulnerabilities alpine:3.11 2020-03-28T00:03:32.685733 1904
vulnerabilities alpine:3.3 2020-03-28T00:02:03.906147 457
vulnerabilities alpine:3.4 2020-03-28T00:02:03.160375 681
vulnerabilities alpine:3.5 2020-03-28T00:02:04.693883 875
vulnerabilities alpine:3.6 2020-03-28T00:02:06.155002 1051
vulnerabilities alpine:3.7 2020-03-28T00:02:06.717063 1395
vulnerabilities alpine:3.8 2020-03-28T00:02:07.329353 1486
vulnerabilities alpine:3.9 2020-03-28T00:02:05.434095 1558
vulnerabilities amzn:2 2020-03-28T00:02:08.671245 320
vulnerabilities centos:5 2020-03-28T00:02:09.375775 1347
vulnerabilities centos:6 2020-03-28T00:02:10.061647 1393
vulnerabilities centos:7 2020-03-28T00:02:10.656073 1004
vulnerabilities centos:8 2020-03-28T00:02:11.268366 199
vulnerabilities debian:10 2020-03-28T00:03:33.244902 22414
vulnerabilities debian:11 2020-03-28T00:02:11.914975 19450
vulnerabilities debian:7 2020-03-28T00:02:12.732607 20455
vulnerabilities debian:8 2020-03-28T00:02:13.759757 23488
vulnerabilities debian:9 2020-03-28T00:02:07.960085 22514
vulnerabilities debian:unstable 2020-03-28T00:02:14.585239 23708
vulnerabilities ol:5 2020-03-28T00:02:15.882825 1245
vulnerabilities ol:6 2020-03-28T00:02:15.368850 1504
vulnerabilities ol:7 2020-03-28T00:02:17.334177 1121
vulnerabilities ol:8 2020-03-28T00:02:18.057855 157
vulnerabilities rhel:5 2020-03-28T00:02:18.748398 7237
vulnerabilities rhel:6 2020-03-28T00:02:16.548115 6805
vulnerabilities rhel:7 2020-03-28T00:03:20.039569 5846
vulnerabilities rhel:8 2020-03-28T00:03:21.424688 1428
vulnerabilities ubuntu:12.04 2020-03-28T00:03:30.795672 14948
vulnerabilities ubuntu:12.10 2020-03-28T00:03:20.686089 5652
vulnerabilities ubuntu:13.04 2020-03-28T00:03:22.630122 4127
vulnerabilities ubuntu:14.04 2020-03-28T00:03:23.376621 21176
vulnerabilities ubuntu:14.10 2020-03-28T00:03:24.059663 4456
vulnerabilities ubuntu:15.04 2020-03-28T00:03:22.070692 5877
vulnerabilities ubuntu:15.10 2020-03-28T00:03:24.656382 6513
vulnerabilities ubuntu:16.04 2020-03-28T00:03:26.013850 18288
vulnerabilities ubuntu:16.10 2020-03-28T00:03:25.370678 8647
vulnerabilities ubuntu:17.04 2020-03-28T00:03:27.278963 9157
vulnerabilities ubuntu:17.10 2020-03-28T00:03:26.605719 7936
vulnerabilities ubuntu:18.04 2020-03-28T00:03:27.845497 12547
vulnerabilities ubuntu:18.10 2020-03-28T00:03:28.482261 8397
vulnerabilities ubuntu:19.04 2020-03-28T00:03:31.400152 8664
vulnerabilities ubuntu:19.10 2020-03-28T00:03:29.122119 7327
[anchore@93d6977e2061 anchore-engine]$ anchore-cli system feeds sync
WARNING: This operation should not normally need to be performed except when the anchore-engine operator is certain that it is required - the operation will take a long time (hours) to complete, and there may be an impact on anchore-engine performance during the re-sync/flush.
Really perform a manual feed data sync/flush? (y/N)y
Feed Group Status Records Updated Sync Duration
nvdv2 nvdv2:cves success 0 0.81s
vulnerabilities alpine:3.10 success 0 0.54s
vulnerabilities alpine:3.11 success 0 0.60s
vulnerabilities alpine:3.3 success 0 0.59s
vulnerabilities alpine:3.4 success 0 0.75s
vulnerabilities alpine:3.5 success 0 0.95s
vulnerabilities alpine:3.6 success 0 0.57s
vulnerabilities alpine:3.7 success 0 0.59s
vulnerabilities alpine:3.8 success 0 0.51s
vulnerabilities alpine:3.9 success 0 1.34s
vulnerabilities amzn:2 success 0 0.52s
vulnerabilities centos:5 success 0 0.64s
vulnerabilities centos:6 success 0 0.80s
vulnerabilities centos:7 success 0 0.82s
vulnerabilities centos:8 success 0 0.58s
vulnerabilities debian:10 success 0 0.47s
vulnerabilities debian:11 success 0 0.58s
vulnerabilities debian:7 success 0 0.64s
vulnerabilities debian:8 success 0 0.71s
vulnerabilities debian:9 success 0 0.76s
vulnerabilities debian:unstable success 0 0.78s
vulnerabilities ol:5 success 0 0.65s
vulnerabilities ol:6 success 0 0.68s
vulnerabilities ol:7 success 0 0.69s
vulnerabilities ol:8 success 0 0.68s
vulnerabilities rhel:5 success 0 0.76s
vulnerabilities rhel:6 success 0 0.49s
vulnerabilities rhel:7 success 0 0.61s
vulnerabilities rhel:8 success 0 0.89s
vulnerabilities ubuntu:12.04 success 0 0.76s
vulnerabilities ubuntu:12.10 success 0 0.60s
vulnerabilities ubuntu:13.04 success 0 0.65s
vulnerabilities ubuntu:14.04 success 0 0.59s
vulnerabilities ubuntu:14.10 success 0 1.01s
vulnerabilities ubuntu:15.04 success 0 0.70s
vulnerabilities ubuntu:15.10 success 0 0.60s
vulnerabilities ubuntu:16.04 success 0 0.82s
vulnerabilities ubuntu:16.10 success 0 0.57s
vulnerabilities ubuntu:17.04 success 0 0.61s
vulnerabilities ubuntu:17.10 success 0 0.51s
vulnerabilities ubuntu:18.04 success 0 0.60s
vulnerabilities ubuntu:18.10 success 0 0.60s
vulnerabilities ubuntu:19.04 success 0 0.61s
vulnerabilities ubuntu:19.10 success 0 0.60s
&lt;/code>&lt;/pre>&lt;h4 id="deleting-specific-feed-groups">Deleting Specific Feed Groups&lt;/h4>
&lt;pre>&lt;code>[anchore@93d6977e2061 ~]$ anchore-cli system feeds config --disable vulnerabilities --group centos:5
Group LastSync RecordCount
centos:5(disabled) 2020-03-28T00:22:57.113534 1347
[anchore@93d6977e2061 ~]$ anchore-cli system feeds delete vulnerabilities --group centos:5
Group LastSync RecordCount
centos:5(disabled) pending 0
&lt;/code>&lt;/pre>&lt;h4 id="restoring-deleted-data">Restoring Deleted Data&lt;/h4>
&lt;p>If you want to get data back, simply enable the feed and/or group and run a feed sync manually or wait for the next scheduled sync.&lt;/p>
&lt;p>For an entire feed, here is an example of removal and re-adding it:&lt;/p>
&lt;pre>&lt;code>[anchore@93d6977e2061 ~]$ anchore-cli system feeds config github --disable
Feed Group LastSync RecordCount
github(disabled) github:composer 2020-03-28T01:08:58.652868 78
github(disabled) github:gem 2020-03-28T01:08:59.179493 333
github(disabled) github:java 2020-03-28T01:08:59.699348 432
github(disabled) github:npm 2020-03-28T00:34:48.167115 653
github(disabled) github:nuget 2020-03-28T01:12:01.116613 50
github(disabled) github:python 2020-03-28T01:08:58.083361 250
[anchore@93d6977e2061 ~]$ anchore-cli system feeds delete github
[anchore@93d6977e2061 ~]$ anchore-cli system feeds config github --enable
[anchore@93d6977e2061 ~]$ anchore-cli system feeds sync
WARNING: This operation should not normally need to be performed except when the anchore-engine operator is certain that it is required - the operation will take a long time (hours) to complete, and there may be an impact on anchore-engine performance during the re-sync/flush.
Really perform a manual feed data sync/flush? (y/N)y
Feed Group Status Records Updated Sync Duration
github github:composer success 78 1.64s
github github:gem success 333 4.48s
github github:java success 432 6.07s
github github:npm success 653 7.39s
github github:nuget success 50 1.10s
github github:python success 250 3.34s
nvdv2 nvdv2:cves success 0 60.90s
vulnerabilities alpine:3.10 success 0 0.52s
vulnerabilities alpine:3.11 success 0 0.47s
vulnerabilities alpine:3.3 success 0 0.56s
vulnerabilities alpine:3.4 success 0 0.46s
vulnerabilities alpine:3.5 success 0 0.52s
vulnerabilities alpine:3.6 success 0 0.54s
vulnerabilities alpine:3.7 success 0 60.76s
vulnerabilities alpine:3.8 success 0 0.54s
vulnerabilities alpine:3.9 success 0 0.54s
vulnerabilities amzn:2 success 0 0.49s
vulnerabilities centos:5 success 0 0.47s
vulnerabilities centos:6 success 0 0.49s
vulnerabilities centos:7 success 0 0.48s
vulnerabilities centos:8 success 0 0.53s
vulnerabilities debian:10 success 0 0.62s
vulnerabilities debian:11 success 0 0.50s
...
&lt;/code>&lt;/pre>&lt;p>For a single feed group, here is an example of removal and re-adding it:&lt;/p>
&lt;pre>&lt;code>[anchore@93d6977e2061 ~]$ anchore-cli system feeds config --enable vulnerabilities --group centos:5
Group LastSync RecordCount
centos:5 pending 0
[anchore@93d6977e2061 ~]$ anchore-cli system feeds sync
WARNING: This operation should not normally need to be performed except when the anchore-engine operator is certain that it is required - the operation will take a long time (hours) to complete, and there may be an impact on anchore-engine performance during the re-sync/flush.
Really perform a manual feed data sync/flush? (y/N)y
Feed Group Status Records Updated Sync Duration
...
vulnerabilities centos:5 success 1347 27.41s
...
&lt;/code>&lt;/pre>&lt;p>With these controls you can better customize the data set that anchore stores in the db. However, note that this should not normally be necessary
and modifying feed groups &amp;amp; data has implication on the sets of distros and types of artifacts Anchore can match vulnerabilities against.&lt;/p></description></item><item><title>Docs: Configuring Registries</title><link>/docs/usage/cli_usage/registries/configuring_registries/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/docs/usage/cli_usage/registries/configuring_registries/</guid><description>
&lt;p>The Anchore Engine will attempt to download images from any registry without requiring further configuration.
However if your registry requires authentication then the registry and corresponding credentials will need to be defined.&lt;/p>
&lt;h3 id="listing-registries">Listing Registries&lt;/h3>
&lt;p>Running the following command lists the defined registries.&lt;/p>
&lt;pre>&lt;code>$ anchore-cli registry list
Registry User
docker.io anchore
quay.io anchore
registry.example.com johndoe
192.168.1.200:5000 janedoe
&lt;/code>&lt;/pre>&lt;p>Here we can see that 4 registries have been defined. If no registry was defined then the Anchore Engine would attempt to pull images without authentication but a registry is defined then all pulls for images from that registry will use the specified username and password.&lt;/p>
&lt;h3 id="adding-a-registry">Adding a Registry&lt;/h3>
&lt;p>Registries can be added using the following syntax.&lt;/p>
&lt;p>&lt;code>anchore-cli registry add REGISTRY USERNAME PASSWORD&lt;/code>&lt;/p>
&lt;p>The REGISTRY parameter should include the fully qualified hostname and port number of the registry. For example: registry.anchore.com:5000&lt;/p>
&lt;p>Anchore Engine will only pull images from a TLS/SSL enabled registry. If the registry is protected with a self signed certificate or a certificated signed by an unknown certificate authority then the &lt;code>--insecure&lt;/code> parameter can be passed which instructs the Anchore Engine not to validate the certificate.&lt;/p>
&lt;p>Most Docker V2 compatible registries require username and password for authentication. Amazon ECR, Google GCR and Microsoft Azure include support
for their own native credentialing. See Working with &lt;a href="/docs/usage/cli_usage/registries/ecr_configuration/">AWS ECR Registry Credentials&lt;/a>,
&lt;a href="/docs/usage/cli_usage/registries/gcr_configuration/">Working with Google GCR Registry Credentials&lt;/a> and
&lt;a href="/docs/usage/cli_usage/registries/acr_configuration/">Working with Azure Registry Credentials&lt;/a> for more details.&lt;/p>
&lt;h3 id="getting-registry-details">Getting Registry Details&lt;/h3>
&lt;p>The &lt;em>registry get&lt;/em> command allows the user to retrieve details about a specific registry.&lt;/p>
&lt;p>For example:&lt;/p>
&lt;pre>&lt;code>$ anchore-cli registry get registry.example.com
Registry: registry.example.com
User: johndoe
Verify TLS: False
Created: 2017-09-02T18:25:34
Updated: 2017-09-02T18:25:34
&lt;/code>&lt;/pre>&lt;p>In this example we can see that the registry.example.com registry was added to the Anchore Engine on the 2nd September at 18:25 UTC. This registry. The password for the registry cannot be retrieved through the API or CLI.&lt;/p>
&lt;h3 id="updating-registry-details">Updating Registry Details&lt;/h3>
&lt;p>Once a registry had been defined the parameters can be updated using the &lt;em>update&lt;/em> command. This allows a registry&amp;rsquo;s username, password and insecure (validate TLS) parameters to be updated.&lt;/p>
&lt;p>&lt;code>anchore-cli registry update REGISTRY USERNAME PASSWORD [--insecure]&lt;/code>&lt;/p>
&lt;h3 id="deleting-registries">Deleting Registries&lt;/h3>
&lt;p>A Registry can be deleted from Anchore&amp;rsquo;s configuration using the &lt;code>del&lt;/code> command.&lt;/p>
&lt;p>For example to delete the configuration for registry.example.com the following command should be issued:&lt;/p>
&lt;p>&lt;code>anchore-cli registry delete registry.example.com&lt;/code>&lt;/p>
&lt;p>&lt;strong>Note:&lt;/strong> Deleting a registry record does not delete the records of images/tags associated with that registry.&lt;/p>
&lt;h3 id="advanced">Advanced&lt;/h3>
&lt;p>Anchore engine attempts to perform a credential validation upon registry addition, but there are cases where a credential can be valid but the validation routine can fail (in particular, credential validation methods are changing for public registries over time). If you are unable to add a registry but believe that the credential you are providing is valid, or you wish to add a credential to anchore before it is in place in the registry, you can bypass the registry credential validation process using the &lt;code>--skip-validation&lt;/code> option to the &lt;code>registry add&lt;/code> command.&lt;/p></description></item><item><title>Docs: Configuring the Anchore CLI</title><link>/docs/install/anchore_cli/cli_config/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/docs/install/anchore_cli/cli_config/</guid><description>
&lt;p>By default the Anchore CLI will try to connect to the Anchore Engine at http://localhost/v1 with no authentication.&lt;/p>
&lt;p>The username, password and URL for the server can be passed to the Anchore CLI using one of three methods:&lt;/p>
&lt;h3 id="command-line-parameters">Command Line Parameters&lt;/h3>
&lt;p>The following command line parameters are used to configure the Anchore CLI to connect to and authenticate with the Anchore Engine.&lt;/p>
&lt;pre>&lt;code>--u TEXT Username eg. admin
--p TEXT Password eg. foobar
--url TEXT Service URL eg. http://localhost:8228/v1
--insecure Skip certificate validation checks (optional)
&lt;/code>&lt;/pre>&lt;p>These connection parameters should be passed before any other commands.
eg.&lt;/p>
&lt;p>&lt;code>$ anchore-cli --u admin --p foobar --url http://anchore.example.com:8228/v1&lt;/code>&lt;/p>
&lt;h3 id="environment-variables">Environment Variables&lt;/h3>
&lt;p>Rather than passing command line parameters for every call to the Anchore CLI they can be stored as environment variables.&lt;/p>
&lt;pre>&lt;code>ANCHORE_CLI_URL=http://myserver.example.com:8228/v1
ANCHORE_CLI_USER=admin
ANCHORE_CLI_PASS=foobar
ANCHORE_CLI_SSL_VERIFY=n
&lt;/code>&lt;/pre>&lt;h3 id="credentials-file-recommended">Credentials File (recommended)&lt;/h3>
&lt;p>The server URL and authentications credentials can be stored in a configuration file stored in the user&amp;rsquo;s home directory.&lt;/p>
&lt;p>The file should be stored in the following location: $HOME/.anchore/credentials.yaml&lt;/p>
&lt;pre>&lt;code>default:
ANCHORE_CLI_USER: 'admin'
ANCHORE_CLI_PASS: 'foobar'
ANCHORE_CLI_URL: 'http://localhost:8228/v1'
&lt;/code>&lt;/pre>&lt;h3 id="order-or-precedence">Order or Precedence&lt;/h3>
&lt;p>The Anchore CLi will first look for configuration via command line parameters. If no command line parameters are passed then the environment is checked, finally the CLI will check for a credentials file.&lt;/p>
&lt;p>&lt;strong>Note:&lt;/strong> All examples in the documentation will presume that the credentials have been configured using either environment variables or the credentials file.&lt;/p></description></item><item><title>Docs: Considerations for Mac</title><link>/docs/install/considerations/mac_special_considerations/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/docs/install/considerations/mac_special_considerations/</guid><description>
&lt;h2 id="special-considerations-for-installing-on-docker-for-mac">Special Considerations for installing on Docker for Mac&lt;/h2>
&lt;p>If you are running Anchore Engine using Docker for Mac the default memory allocation will be insufficient to run the Anchore Engine. It is recommended that a minimum of 4GB of RAM is allocated to Docker for Mac&amp;rsquo;s virtual machine.&lt;/p>
&lt;ul>
&lt;li>Click on the Moby (Whale) icon on the right side of the menu bar.&lt;/li>
&lt;li>Select Preferences&lt;/li>
&lt;li>Select Advanced&lt;/li>
&lt;li>Increase Memory to at least 4GB&lt;/li>
&lt;li>Press Apply &amp;amp; Restart&lt;/li>
&lt;/ul></description></item><item><title>Docs: Considerations for RHEL</title><link>/docs/install/considerations/rhel_special_considerations/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/docs/install/considerations/rhel_special_considerations/</guid><description>
&lt;h3 id="special-considerations-for-installing-on-red-hat-enterprise-linux-andor-any-docker-host-using-overlayfs-as-storage-drivers">Special Considerations for installing on Red Hat Enterprise Linux (and/or any docker host using OverlayFS* as storage drivers)&lt;/h3>
&lt;p>If the Anchore Engine container is run on a Red Hat Enterprise Linux (RHEL) system or derivative such as CentOS or Oracle Linux (or any docker host configured to use the OverlayFS* storage drivers) then special consideration should be taken with configuring storage.&lt;/p>
&lt;p>Red Hat Enterprise Linux 7.5 defaults to using OverlayFS for storage of container images including ephemeral container storage. Previous versions of Red Hat Enterprise Linux used the devicemapper storage driver.&lt;/p>
&lt;p>There is a bug in the OverlayFS driver in kernels older than 4.13 that may result in errors during image analysis. During the image extraction process the following error may be logged:&lt;/p>
&lt;p>&lt;code>Directory renamed before its status could be extracted&lt;/code>&lt;/p>
&lt;p>This error is seen during the extraction of the image layer tar files.&lt;/p>
&lt;p>To work around this error, until the appropriate fix can be backported into Red Hat Enterprise Linux&amp;rsquo;s 3.1 kernel it is recommended that an external volume is used to during image extraction.&lt;/p>
&lt;p>By default the Anchore Engine uses the /tmp directory within the container to download and extract images. Configure a volume to be mounted into the container at a specified path and configure this path in config.yaml&lt;/p>
&lt;p>&lt;code>tmp_dir: '/scratch'&lt;/code>&lt;/p>
&lt;p>In this example a volume has been mounted as /scratch within the container and config.yaml updated to use /scratch as the temporary directory for image analysis. This volume should be sized to at least 3 times the uncompressed image size to be analyzed.&lt;/p></description></item><item><title>Docs: Database</title><link>/docs/install/requirements/database/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/docs/install/requirements/database/</guid><description>
&lt;p>The Anchore Engine requires PostgreSQL version 9.6 or higher database to provide persistent storage for image, policy and analysis data.&lt;/p>
&lt;p>This database can be run in a container, as configured in the example Docker Compose file or can be provided as an external service to the Anchore Engine.
PostgreSQL compatible databases such as Amazon RDS for PostgreSQL can be used for highly scalable cloud deployments.&lt;/p></description></item><item><title>Docs: Database Storage</title><link>/docs/install/storage/database/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/docs/install/storage/database/</guid><description>
&lt;p>Anchore stores all metadata in a structured format in a PostgreSQL database to support API operations and searches.&lt;/p>
&lt;p>Examples of data persisted in the database:&lt;/p>
&lt;ul>
&lt;li>Image metadata (distro, version, layer counts, &amp;hellip;)&lt;/li>
&lt;li>Image digests to tag mapping (docker.io/nginx:latest is hash sha256:abcd at time &lt;em>t&lt;/em>)&lt;/li>
&lt;li>Image analysis content indexed for policy evaluation (files, packages, ..)&lt;/li>
&lt;li>Feed data
&lt;ul>
&lt;li>vulnerability info&lt;/li>
&lt;li>package info from upstream (gem/npm)&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>Accounts, users&amp;hellip;&lt;/li>
&lt;li>&amp;hellip;&lt;/li>
&lt;/ul>
&lt;p>If the &lt;a href="../object_store">object store&lt;/a> is not explicitly set to an external provider, then that data is also persisted in
the database but can be &lt;a href="../object_store/migration">migrated&lt;/a>&lt;/p></description></item><item><title>Docs: Database Driver</title><link>/docs/install/storage/object_store/database_driver/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/docs/install/storage/object_store/database_driver/</guid><description>
&lt;p>The default object store driver is the PostgreSQL database driver which stores all object store documents within the PostgreSQL database.&lt;/p>
&lt;p>Compression is not supported for this driver since the underlying database will handle compression.&lt;/p>
&lt;p>There are no configuration options required for the Database driver.&lt;/p>
&lt;p>The embedded configuration for anchore engine includes the default configuration for the db driver.&lt;/p>
&lt;div class="highlight">&lt;pre style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4">&lt;code class="language-YAML" data-lang="YAML">&lt;span style="color:#204a87;font-weight:bold">object_store&lt;/span>&lt;span style="color:#000;font-weight:bold">:&lt;/span>&lt;span style="color:#f8f8f8;text-decoration:underline">
&lt;/span>&lt;span style="color:#f8f8f8;text-decoration:underline"> &lt;/span>&lt;span style="color:#204a87;font-weight:bold">compression&lt;/span>&lt;span style="color:#000;font-weight:bold">:&lt;/span>&lt;span style="color:#f8f8f8;text-decoration:underline">
&lt;/span>&lt;span style="color:#f8f8f8;text-decoration:underline"> &lt;/span>&lt;span style="color:#204a87;font-weight:bold">enabled&lt;/span>&lt;span style="color:#000;font-weight:bold">:&lt;/span>&lt;span style="color:#f8f8f8;text-decoration:underline"> &lt;/span>&lt;span style="color:#204a87;font-weight:bold">False&lt;/span>&lt;span style="color:#f8f8f8;text-decoration:underline">
&lt;/span>&lt;span style="color:#f8f8f8;text-decoration:underline"> &lt;/span>&lt;span style="color:#204a87;font-weight:bold">min_size_kbytes&lt;/span>&lt;span style="color:#000;font-weight:bold">:&lt;/span>&lt;span style="color:#f8f8f8;text-decoration:underline"> &lt;/span>&lt;span style="color:#0000cf;font-weight:bold">100&lt;/span>&lt;span style="color:#f8f8f8;text-decoration:underline">
&lt;/span>&lt;span style="color:#f8f8f8;text-decoration:underline"> &lt;/span>&lt;span style="color:#204a87;font-weight:bold">storage_driver&lt;/span>&lt;span style="color:#000;font-weight:bold">:&lt;/span>&lt;span style="color:#f8f8f8;text-decoration:underline">
&lt;/span>&lt;span style="color:#f8f8f8;text-decoration:underline"> &lt;/span>&lt;span style="color:#204a87;font-weight:bold">name&lt;/span>&lt;span style="color:#000;font-weight:bold">:&lt;/span>&lt;span style="color:#f8f8f8;text-decoration:underline"> &lt;/span>&lt;span style="color:#000">db&lt;/span>&lt;span style="color:#f8f8f8;text-decoration:underline">
&lt;/span>&lt;span style="color:#f8f8f8;text-decoration:underline"> &lt;/span>&lt;span style="color:#204a87;font-weight:bold">config&lt;/span>&lt;span style="color:#000;font-weight:bold">:&lt;/span>&lt;span style="color:#f8f8f8;text-decoration:underline"> &lt;/span>{}&lt;span style="color:#f8f8f8;text-decoration:underline">
&lt;/span>&lt;/code>&lt;/pre>&lt;/div></description></item><item><title>Docs: Evaluating Images Against Policies</title><link>/docs/usage/cli_usage/policies/evaluating_images_against_policies/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/docs/usage/cli_usage/policies/evaluating_images_against_policies/</guid><description>
&lt;p>The &lt;code>evaluate&lt;/code> command can be used to evaluate a given image for policy compliance.&lt;/p>
&lt;p>The image to be evaluated can be in the following format:&lt;/p>
&lt;ul>
&lt;li>Image Digest&lt;/li>
&lt;li>Image ID&lt;/li>
&lt;li>registry/repo:tag&lt;/li>
&lt;/ul>
&lt;pre>&lt;code>$ anchore-cli evaluate check debian:latest
Image Digest: sha256:427752aa7da803378f765f5a8efba421df5925cbde8ab011717f3642f406fb15
Full Tag: docker.io/debian:latest
Status: fail
Last Eval: 2017-09-02T15:19:55
Policy ID: 715a6056-87ab-49fb-abef-f4b4198c67bf
&lt;/code>&lt;/pre>&lt;p>By default only the summary of the evaluation is shown. Passing the &lt;code>--detail&lt;/code> parameter will show the policy checks that raised warnings or errors.&lt;/p>
&lt;pre>&lt;code>$ anchore-cli evaluate check debian:latest
Image Digest: sha256:427752aa7da803378f765f5a8efba421df5925cbde8ab011717f3642f406fb15
Full Tag: docker.io/debian:latest
Status: fail
Last Eval: 2017-09-02T15:19:55
Policy ID: 715a6056-87ab-49fb-abef-f4b4198c67bf
Gate Trigger Detail Status
DOCKERFILECHECK NOHEALTHCHECK Dockerfile does not contain any HEALTHCHECK instructions
ANCHORESEC VULNHIGH HIGH Vulnerability found in package - mount (CVE-2016-2779 - https://security-tracker.debian.org/tracker/CVE-2016-2779) stop
ANCHORESEC VULNHIGH HIGH Vulnerability found in package - libuuid1 (CVE-2016-2779 - https://security-tracker.debian.org/tracker/CVE-2016-2779) stop
&lt;/code>&lt;/pre>&lt;p>In this example we specified library/repo:tag which could be ambiguous. At the time of writing the image Digest for &lt;code>library/debian:latest&lt;/code> was &lt;code>sha:256:427752aa.....&lt;/code> however previously different images may have been tagged as &lt;code>library/debian:latest&lt;/code>. The &lt;code>--show-history&lt;/code> parameter can be passed to show historic evaluations based on previous images or previous policy bundles.&lt;/p>
&lt;p>Anchore supports whitelisting and blacklisting images by their name, ID or digest. A blacklist or whitelist takes precedence over any policy checks. For example if an image is explicitly listed as &lt;em>blacklisted&lt;/em> then even if all the individual policy checks pass the image will still fail evaluation.&lt;/p>
&lt;pre>&lt;code>$ anchore-cli evaluate check library/alpine:latest --detail
Image Digest: sha256:8c03bb07a531c53ad7d0f6e7041b64d81f99c6e493cb39abba56d956b40eacbc
Full Tag: docker.io/library/alpine:latest
Image ID: 3fd9065eaf02feaf94d68376da52541925650b81698c53c6824d92ff63f98353
Status: fail
Last Eval: 2018-04-29T13:50:32
Policy ID: 2c53a13c-1765-11e8-82ef-23527761d060
Final Action: stop
Final Action Reason: blacklisted
Gate Trigger Detail Status
dockerfile instruction Dockerfile directive 'HEALTHCHECK' not found, matching condition 'not_exists' check warn
&lt;/code>&lt;/pre>&lt;p>In this example even though the image only had one policy check that raised a warning the image fails policy evaluation since it is present on a blacklist.&lt;/p>
&lt;h3 id="evaluating-status-based-on-digest-or-id">Evaluating status based on Digest or ID&lt;/h3>
&lt;p>Performing an evaluation on an image specified by name is not recommended since an image name is ambiguous. For example the tag &lt;code>docker.io/library/centos:latest&lt;/code> refers to whatever image has the tag &lt;code>library/centos:latest&lt;/code> at the time of evaluation. At any point in time another image may be tagged as &lt;code>library/centos:latest&lt;/code>.&lt;/p>
&lt;p>It is recommended that images are referenced by their Digest. For example at the time of writing the digest of the &amp;lsquo;current&amp;rsquo; library/centos:latest image is &lt;code>sha256:191c883e479a7da2362b2d54c0840b2e8981e5ab62e11ab925abf8808d3d5d44&lt;/code>&lt;/p>
&lt;p>If the image to be evaluated is specified by Image ID or Image Digest then the &lt;code>--tag&lt;/code> parameter must be added. Policies are mapped to images based on registry/repo:tag so since an Image ID may may to multiple different names we must specify the name user in the evaluation.&lt;/p>
&lt;p>For example - referencing by Image Digest:&lt;/p>
&lt;p>&lt;code>$ anchore-cli evaluate check docker.io/library/centos@sha256:191c883e479a7da2362b2d54c0840b2e8981e5ab62e11ab925abf8808d3d5d44 --tag=latest&lt;/code>&lt;/p>
&lt;p>For example - referencing by image ID:&lt;/p>
&lt;p>&lt;code>$ anchore-cli evaluate check e934aafc22064b7322c0250f1e32e5ce93b2d19b356f4537f5864bd102e8531f --tag=docker.io/library/centos:latest&lt;/code>&lt;/p></description></item><item><title>Docs: Filesystem Driver</title><link>/docs/install/storage/object_store/filesystem_driver/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/docs/install/storage/object_store/filesystem_driver/</guid><description>
&lt;p>Using the file system driver object store documents can be stored on a filesystem volume passed to the Anchore Engine container.&lt;/p>
&lt;p>&lt;strong>The filesystem driver does not handle distribution or replication.&lt;/strong> To replicate the file system across nodes for performance and redundancy a clustered / shared filesystem such as Gluster, CephFS or Amazon EFS should be used.&lt;/p>
&lt;p>&lt;strong>WARNING:&lt;/strong> This driver is not recommended for scale-out production deployments&lt;/p>
&lt;p>For environments who do not want to utilize the default PostgresSQL storage and need scale or redundancy an object store such as S3 or Swift will provide a better solution than the filesystem driver.&lt;/p>
&lt;h3 id="compression">Compression&lt;/h3>
&lt;p>The localfs (filesystem) driver supports compression of object_store documents. The object_store documents are JSON formatted and will see significant reduction in size through compression there is an overhead incurred by running compression and decompression on every access of these documents. The Anchore Engine can be configured to only compress documents above a certain size to reduce unnecessary overhead. In the example below any document over 100kb in size will be compressed.&lt;/p>
&lt;div class="highlight">&lt;pre style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4">&lt;code class="language-YAML" data-lang="YAML">&lt;span style="color:#204a87;font-weight:bold">object_store&lt;/span>&lt;span style="color:#000;font-weight:bold">:&lt;/span>&lt;span style="color:#f8f8f8;text-decoration:underline">
&lt;/span>&lt;span style="color:#f8f8f8;text-decoration:underline"> &lt;/span>&lt;span style="color:#204a87;font-weight:bold">compression&lt;/span>&lt;span style="color:#000;font-weight:bold">:&lt;/span>&lt;span style="color:#f8f8f8;text-decoration:underline">
&lt;/span>&lt;span style="color:#f8f8f8;text-decoration:underline"> &lt;/span>&lt;span style="color:#204a87;font-weight:bold">enabled&lt;/span>&lt;span style="color:#000;font-weight:bold">:&lt;/span>&lt;span style="color:#f8f8f8;text-decoration:underline"> &lt;/span>&lt;span style="color:#204a87;font-weight:bold">True&lt;/span>&lt;span style="color:#f8f8f8;text-decoration:underline">
&lt;/span>&lt;span style="color:#f8f8f8;text-decoration:underline"> &lt;/span>&lt;span style="color:#204a87;font-weight:bold">min_size_kbytes&lt;/span>&lt;span style="color:#000;font-weight:bold">:&lt;/span>&lt;span style="color:#f8f8f8;text-decoration:underline"> &lt;/span>&lt;span style="color:#0000cf;font-weight:bold">100&lt;/span>&lt;span style="color:#f8f8f8;text-decoration:underline">
&lt;/span>&lt;span style="color:#f8f8f8;text-decoration:underline"> &lt;/span>&lt;span style="color:#204a87;font-weight:bold">storage_driver&lt;/span>&lt;span style="color:#000;font-weight:bold">:&lt;/span>&lt;span style="color:#f8f8f8;text-decoration:underline">
&lt;/span>&lt;span style="color:#f8f8f8;text-decoration:underline"> &lt;/span>&lt;span style="color:#204a87;font-weight:bold">name&lt;/span>&lt;span style="color:#000;font-weight:bold">:&lt;/span>&lt;span style="color:#f8f8f8;text-decoration:underline"> &lt;/span>&lt;span style="color:#000">localfs&lt;/span>&lt;span style="color:#f8f8f8;text-decoration:underline">
&lt;/span>&lt;span style="color:#f8f8f8;text-decoration:underline"> &lt;/span>&lt;span style="color:#204a87;font-weight:bold">config&lt;/span>&lt;span style="color:#000;font-weight:bold">:&lt;/span>&lt;span style="color:#f8f8f8;text-decoration:underline">
&lt;/span>&lt;span style="color:#f8f8f8;text-decoration:underline"> &lt;/span>&lt;span style="color:#204a87;font-weight:bold">archive_data_dir&lt;/span>&lt;span style="color:#000;font-weight:bold">:&lt;/span>&lt;span style="color:#f8f8f8;text-decoration:underline"> &lt;/span>&lt;span style="color:#4e9a06">&amp;#39;/object_store&amp;#39;&lt;/span>&lt;span style="color:#f8f8f8;text-decoration:underline">
&lt;/span>&lt;/code>&lt;/pre>&lt;/div></description></item><item><title>Docs: Configuration</title><link>/docs/install/configuration/config/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/docs/install/configuration/config/</guid><description>
&lt;h2 id="initial-configuration">Initial Configuration&lt;/h2>
&lt;p>A single configuration file config.yaml is required to run the Anchore Engine - by default, this file is embedded in the anchore-engine container image, located in /config/config.yaml. The default configuration file is provided as a way to get started, which is functional out of the box, without modification, when combined with either the Helm method or docker-compose method of installing anchore-engine. The default configuration is set up to use environment variable substitutions so that configuration values can be controlled by setting the corresponding environment variables at deployment time (see &lt;a href="/docs/install/configuration/using_env_vars/">Using Environment Variables in Anchore&lt;/a>. To review the embedded configuration file settings, see the &lt;a href="https://github.com/anchore/anchore-engine/blob/master/conf/default_config.yaml">default config.yaml on github&lt;/a> which is populated with several environment variables (all starting with ANCHORE_), the &lt;a href="https://github.com/anchore/anchore-engine/blob/master/docker-compose.yaml">example docker-compose.yaml on github&lt;/a> which includes several site-specific environment variable default settings, and the &lt;a href="https://github.com/anchore/anchore-engine/tree/master/Dockerfile">anchore engine Dockerfile on github&lt;/a> which sets baseline environment variable settings.&lt;/p>
&lt;p>Each environment variable (starting with ANCHORE_) in the default config.yaml is set (either the baseline as set in the Dockerfile, or an override in docker-compose or Helm) to ensure that the system comes up with a fully populated configuration.&lt;/p>
&lt;p>Some examples of useful initial settings follow.&lt;/p>
&lt;ul>
&lt;li>Default admin credentials: A default admin email and password is required for the initial bootstrap of engine to succeed, which are both set through the default config file using the &lt;strong>ANCHORE_ADMIN_PASSWORD&lt;/strong> and &lt;strong>ANCHORE_ADMIN_EMAIL&lt;/strong> environment variables respectively. The Dockerfile defines a default email &lt;strong>admin@myanchore&lt;/strong>, but does not define a default password. If using the default config file, the user must set a value for &lt;strong>ANCHORE_ADMIN_PASSWORD&lt;/strong> in order to succeed the initial bootstrap of the system. To set the default password or to override the default email, simply add overrides for &lt;strong>ANCHORE_ADMIN_PASSWORD&lt;/strong> and &lt;strong>ANCHORE_ADMIN_EMAIL&lt;/strong> environment variables, set to your preferred values prior to deploying anchore engine. After the initial bootstrap, this can be removed if desired. The docker-compose file referenced in the quickstart installation guide has set &lt;strong>ANCHORE_ADMIN_PASSWORD&lt;/strong> to &lt;code>foobar&lt;/code>&lt;/li>
&lt;/ul>
&lt;div class="highlight">&lt;pre style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4">&lt;code class="language-YAML" data-lang="YAML">&lt;span style="color:#204a87;font-weight:bold">default_admin_password&lt;/span>&lt;span style="color:#000;font-weight:bold">:&lt;/span>&lt;span style="color:#f8f8f8;text-decoration:underline"> &lt;/span>&lt;span style="color:#4e9a06">&amp;#39;${ANCHORE_ADMIN_PASSWORD}&amp;#39;&lt;/span>&lt;span style="color:#f8f8f8;text-decoration:underline">
&lt;/span>&lt;span style="color:#f8f8f8;text-decoration:underline">&lt;/span>&lt;span style="color:#204a87;font-weight:bold">default_admin_email&lt;/span>&lt;span style="color:#000;font-weight:bold">:&lt;/span>&lt;span style="color:#f8f8f8;text-decoration:underline"> &lt;/span>&lt;span style="color:#4e9a06">&amp;#39;${ANCHORE_ADMIN_EMAIL}&amp;#39;&lt;/span>&lt;span style="color:#f8f8f8;text-decoration:underline">
&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;ul>
&lt;li>Log level: anchore engine is configured to run at the INFO log level by default. The full set of options are FATAL, ERROR, WARN, INFO, and DEBUG (in ascending order of log output verbosity). To set the log level of anchore engine services, add an override for &lt;strong>ANCHORE_LOG_LEVEL&lt;/strong> prior to deploying anchore engine.&lt;/li>
&lt;/ul>
&lt;div class="highlight">&lt;pre style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4">&lt;code class="language-YAML" data-lang="YAML">&lt;span style="color:#204a87;font-weight:bold">log_level&lt;/span>&lt;span style="color:#000;font-weight:bold">:&lt;/span>&lt;span style="color:#f8f8f8;text-decoration:underline"> &lt;/span>&lt;span style="color:#4e9a06">&amp;#39;${ANCHORE_LOG_LEVEL}&amp;#39;&lt;/span>&lt;span style="color:#f8f8f8;text-decoration:underline">
&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;ul>
&lt;li>Postgres Database: the anchore engine requires access to a PostgreSQL database to operate. The database can be run as a container with a persistent volume or outside of your container environment (which is set up automatically if the example docker-compose.yaml is used). If you wish to use an external Postgres Database, the elements of the connection string in the config.yaml can be specified as environment variable overrides. The default configuration is set up to connect to a postgres DB that is deployed alongside the engine services when using docker-compose or Helm, to the internal host &lt;strong>anchore-db&lt;/strong> on port &lt;strong>5432&lt;/strong> using username &lt;strong>postgres&lt;/strong> with password &lt;strong>mysecretpassword&lt;/strong> and db &lt;strong>postgres&lt;/strong>. If an external database service is being used then you will need to provide the use, password, host, port and DB name environment variables, as shown below.&lt;/li>
&lt;/ul>
&lt;div class="highlight">&lt;pre style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4">&lt;code class="language-YAML" data-lang="YAML">&lt;span style="color:#204a87;font-weight:bold">db_connect&lt;/span>&lt;span style="color:#000;font-weight:bold">:&lt;/span>&lt;span style="color:#f8f8f8;text-decoration:underline"> &lt;/span>&lt;span style="color:#4e9a06">&amp;#39;postgresql://${ANCHORE_DB_USER}:${ANCHORE_DB_PASSWORD}@${ANCHORE_DB_HOST}:${ANCHORE_DB_PORT}/${ANCHORE_DB_NAME}&amp;#39;&lt;/span>&lt;span style="color:#f8f8f8;text-decoration:underline">
&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;h2 id="manual-configuration-file-override">Manual Configuration File Override&lt;/h2>
&lt;p>While anchore engine is set up to run out of the box without modifications, and many useful values can be overriden using environment variables as described above, one can always opt to have full control over the configuration by providing a config.yaml file explicitly, typically by generating the file and making it available from an external mount/configmap/etc. at deployment time. A good method to start if you wish to provide your own config.yaml is to extract the default config.yaml from the anchore engine container image, modify it, and then override the embedded /config/config.yaml at deployment time. For example:&lt;/p>
&lt;ul>
&lt;li>Extract the default config file from the anchore-engine container image:&lt;/li>
&lt;/ul>
&lt;pre>&lt;code># docker pull docker.io/anchore/anchore-engine:latest
# docker create --name ae docker.io/anchore/anchore-engine:latest
# docker cp ae:/config/config.yaml ./my_config.yaml
# docker rm ae
&lt;/code>&lt;/pre>&lt;ul>
&lt;li>
&lt;p>Modify the configuration file to your liking.&lt;/p>
&lt;/li>
&lt;li>
&lt;p>Set up your deployment to override the embedded &lt;strong>/config/config.yaml&lt;/strong> at run time (below example shows how to achieve this with docker-compose). Edit the docker-compose.yaml to include a volume mount that mounts your &lt;strong>my_config.yaml&lt;/strong> over the embedded &lt;strong>/config/config.yaml&lt;/strong>, resulting in a volume section for each anchore engine service definition.&lt;/p>
&lt;/li>
&lt;/ul>
&lt;div class="highlight">&lt;pre style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4">&lt;code class="language-YAML" data-lang="YAML">&lt;span style="color:#000">...&lt;/span>&lt;span style="color:#f8f8f8;text-decoration:underline">
&lt;/span>&lt;span style="color:#f8f8f8;text-decoration:underline"> &lt;/span>&lt;span style="color:#204a87;font-weight:bold">engine-api&lt;/span>&lt;span style="color:#000;font-weight:bold">:&lt;/span>&lt;span style="color:#f8f8f8;text-decoration:underline">
&lt;/span>&lt;span style="color:#f8f8f8;text-decoration:underline">&lt;/span>&lt;span style="color:#000">...&lt;/span>&lt;span style="color:#f8f8f8;text-decoration:underline">
&lt;/span>&lt;span style="color:#f8f8f8;text-decoration:underline"> &lt;/span>&lt;span style="color:#204a87;font-weight:bold">volumes&lt;/span>&lt;span style="color:#000;font-weight:bold">:&lt;/span>&lt;span style="color:#f8f8f8;text-decoration:underline">
&lt;/span>&lt;span style="color:#f8f8f8;text-decoration:underline"> &lt;/span>- &lt;span style="color:#000">/path/to/my_config.yaml:/config/config.yaml:z&lt;/span>&lt;span style="color:#f8f8f8;text-decoration:underline">
&lt;/span>&lt;span style="color:#f8f8f8;text-decoration:underline">&lt;/span>&lt;span style="color:#000">...&lt;/span>&lt;span style="color:#f8f8f8;text-decoration:underline">
&lt;/span>&lt;span style="color:#f8f8f8;text-decoration:underline"> &lt;/span>&lt;span style="color:#204a87;font-weight:bold">engine-catalog&lt;/span>&lt;span style="color:#000;font-weight:bold">:&lt;/span>&lt;span style="color:#f8f8f8;text-decoration:underline">
&lt;/span>&lt;span style="color:#f8f8f8;text-decoration:underline">&lt;/span>&lt;span style="color:#000">...&lt;/span>&lt;span style="color:#f8f8f8;text-decoration:underline">
&lt;/span>&lt;span style="color:#f8f8f8;text-decoration:underline"> &lt;/span>&lt;span style="color:#204a87;font-weight:bold">volumes&lt;/span>&lt;span style="color:#000;font-weight:bold">:&lt;/span>&lt;span style="color:#f8f8f8;text-decoration:underline">
&lt;/span>&lt;span style="color:#f8f8f8;text-decoration:underline"> &lt;/span>- &lt;span style="color:#000">/path/to/my_config.yaml:/config/config.yaml:z&lt;/span>&lt;span style="color:#f8f8f8;text-decoration:underline">
&lt;/span>&lt;span style="color:#f8f8f8;text-decoration:underline">&lt;/span>&lt;span style="color:#000">...&lt;/span>&lt;span style="color:#f8f8f8;text-decoration:underline">
&lt;/span>&lt;span style="color:#f8f8f8;text-decoration:underline"> &lt;/span>&lt;span style="color:#204a87;font-weight:bold">engine-simpleq&lt;/span>&lt;span style="color:#000;font-weight:bold">:&lt;/span>&lt;span style="color:#f8f8f8;text-decoration:underline">
&lt;/span>&lt;span style="color:#f8f8f8;text-decoration:underline">&lt;/span>&lt;span style="color:#000">...&lt;/span>&lt;span style="color:#f8f8f8;text-decoration:underline">
&lt;/span>&lt;span style="color:#f8f8f8;text-decoration:underline"> &lt;/span>&lt;span style="color:#204a87;font-weight:bold">volumes&lt;/span>&lt;span style="color:#000;font-weight:bold">:&lt;/span>&lt;span style="color:#f8f8f8;text-decoration:underline">
&lt;/span>&lt;span style="color:#f8f8f8;text-decoration:underline"> &lt;/span>- &lt;span style="color:#000">/path/to/my_config.yaml:/config/config.yaml:z&lt;/span>&lt;span style="color:#f8f8f8;text-decoration:underline">
&lt;/span>&lt;span style="color:#f8f8f8;text-decoration:underline">&lt;/span>&lt;span style="color:#000">...&lt;/span>&lt;span style="color:#f8f8f8;text-decoration:underline">
&lt;/span>&lt;span style="color:#f8f8f8;text-decoration:underline"> &lt;/span>&lt;span style="color:#204a87;font-weight:bold">engine-policy-engine&lt;/span>&lt;span style="color:#000;font-weight:bold">:&lt;/span>&lt;span style="color:#f8f8f8;text-decoration:underline">
&lt;/span>&lt;span style="color:#f8f8f8;text-decoration:underline">&lt;/span>&lt;span style="color:#000">...&lt;/span>&lt;span style="color:#f8f8f8;text-decoration:underline">
&lt;/span>&lt;span style="color:#f8f8f8;text-decoration:underline"> &lt;/span>&lt;span style="color:#204a87;font-weight:bold">volumes&lt;/span>&lt;span style="color:#000;font-weight:bold">:&lt;/span>&lt;span style="color:#f8f8f8;text-decoration:underline">
&lt;/span>&lt;span style="color:#f8f8f8;text-decoration:underline"> &lt;/span>- &lt;span style="color:#000">/path/to/my_config.yaml:/config/config.yaml:z&lt;/span>&lt;span style="color:#f8f8f8;text-decoration:underline">
&lt;/span>&lt;span style="color:#f8f8f8;text-decoration:underline">&lt;/span>&lt;span style="color:#000">...&lt;/span>&lt;span style="color:#f8f8f8;text-decoration:underline">
&lt;/span>&lt;span style="color:#f8f8f8;text-decoration:underline"> &lt;/span>&lt;span style="color:#204a87;font-weight:bold">engine-analyzer&lt;/span>&lt;span style="color:#000;font-weight:bold">:&lt;/span>&lt;span style="color:#f8f8f8;text-decoration:underline">
&lt;/span>&lt;span style="color:#f8f8f8;text-decoration:underline">&lt;/span>&lt;span style="color:#000">...&lt;/span>&lt;span style="color:#f8f8f8;text-decoration:underline">
&lt;/span>&lt;span style="color:#f8f8f8;text-decoration:underline"> &lt;/span>&lt;span style="color:#204a87;font-weight:bold">volumes&lt;/span>&lt;span style="color:#000;font-weight:bold">:&lt;/span>&lt;span style="color:#f8f8f8;text-decoration:underline">
&lt;/span>&lt;span style="color:#f8f8f8;text-decoration:underline"> &lt;/span>- &lt;span style="color:#000">/path/to/my_config.yaml:/config/config.yaml:z&lt;/span>&lt;span style="color:#f8f8f8;text-decoration:underline">
&lt;/span>&lt;span style="color:#f8f8f8;text-decoration:underline">&lt;/span>&lt;span style="color:#000">...&lt;/span>&lt;span style="color:#f8f8f8;text-decoration:underline">
&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>Now, each service will come up with your external &lt;strong>my_config.yaml&lt;/strong> mounted over the embedded &lt;strong>/config/config.yaml&lt;/strong>.&lt;/p></description></item><item><title>Docs: Inspecting Image Content</title><link>/docs/usage/cli_usage/images/inspecting_image_content/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/docs/usage/cli_usage/images/inspecting_image_content/</guid><description>
&lt;p>During the analysis of container images the Anchore Engine performs deep inspection, collecting data on all artifacts in the image including files, operating system packages and software artifacts such as Ruby GEMs and Node.JS NPM modules.&lt;/p>
&lt;p>The &lt;code>image content&lt;/code> command can be used to return detailed information about the content of the container image.&lt;/p>
&lt;p>&lt;code>anchore-cli image content INPUT_IMAGE CONTENT_TYPE&lt;/code>&lt;/p>
&lt;p>The INPUT_IMAGE can be specified in one of the following formats:&lt;/p>
&lt;ul>
&lt;li>Image Digest&lt;/li>
&lt;li>Image ID&lt;/li>
&lt;li>registry/repo:tag&lt;/li>
&lt;/ul>
&lt;p>the CONTENT_TYPE can be one of the following types:&lt;/p>
&lt;ul>
&lt;li>os: Operating System Packages&lt;/li>
&lt;li>files: All files in the image&lt;/li>
&lt;li>npm: Node.JS NPM Modules&lt;/li>
&lt;li>gem: Ruby GEMs&lt;/li>
&lt;li>java: Java Archives&lt;/li>
&lt;li>python: Python Artifacts&lt;/li>
&lt;li>nuget: .NET NuGet Artifacts&lt;/li>
&lt;li>malware: malware findings from scanners (default is ClamAV)&lt;/li>
&lt;li>binary: specific binaries that are statically checked for metadata (e.g. python and go runtime)&lt;/li>
&lt;/ul>
&lt;p>For example: &lt;code>anchore-cli image content debian:latest files&lt;/code>&lt;/p>
&lt;p>The CLI will output a subset of fields from the content view, for example for &lt;code>files&lt;/code> on the file name and size are displayed. To retrieve the full output the &lt;code>--json&lt;/code> parameter should be passed.&lt;/p>
&lt;p>For example: &lt;code>anchore-cli --json image content debian:latest files&lt;/code>&lt;/p>
&lt;h3 id="next-steps">Next Steps&lt;/h3>
&lt;ul>
&lt;li>&lt;a href="/docs/usage/cli_usage/images/viewing_security_vulnerabilities/">View security vulnerabilities in the image&lt;/a>&lt;/li>
&lt;li>&lt;a href="/docs/usage/cli_usage/policies/evaluating_images_against_policies/">Evaluate the image&lt;/a> against policies you create&lt;/li>
&lt;li>Subscribe to receive &lt;a href="/docs/usage/cli_usage/subscriptions/">notifications&lt;/a> when the image is updated, when the policy status changes, or when new vulnerabilities are detected.&lt;/li>
&lt;/ul></description></item><item><title>Docs: Layer Caching</title><link>/docs/install/storage/layer_caching/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/docs/install/storage/layer_caching/</guid><description>
&lt;p>Once an image is submitted to the Anchore Engine for analysis the Engine will attempt to retrieve metadata about the image from the Docker registry and if successful will download the image and queue the image for analysis.&lt;/p>
&lt;p>The Anchore Engine can run one or more analyzer services to scale out processing of images. The next available analyzer worker will process the image.&lt;/p>
&lt;p>Docker Images are made up of one or more layers, which are described in the manifest. The manifest lists the layers which are typically stored as gzipped compressed TAR files.&lt;/p>
&lt;p>As part of image analysis the Anchore Engine will:&lt;/p>
&lt;ul>
&lt;li>Download all layers that comprise an image&lt;/li>
&lt;li>Extract the layers to a temporary file system location&lt;/li>
&lt;li>Perform analysis on the contents of the image including:
&lt;ul>
&lt;li>Digest of every file (SHA1, SHA256 and MD5)&lt;/li>
&lt;li>File attributes (size, owner, permissions, etc)&lt;/li>
&lt;li>Operating System package manifest&lt;/li>
&lt;li>Software library package manifest (NPM, GEM, Java, Python, NuGet)&lt;/li>
&lt;li>Scan for secret materials (api keys, private keys, etc&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;p>Following the analysis the extracted layers and downloaded layer tar files are deleted.&lt;/p>
&lt;p>In many cases the images will share a number of common layers, especially if images are built form a consistent set of base images. To speed up the Anchore Engine can be configure to cache image layers to eliminate the need to download the same layer for many different images. The layer cache is displayed in the default Anchore Engine configuration. To enable the cache the following changes should be made:&lt;/p>
&lt;ol>
&lt;li>Define temporary directory for cache data&lt;/li>
&lt;/ol>
&lt;p>It is recommended that the cache data is stored in an external volume to ensure that the cache does not use up the ephemeral storage space allocated to the container host.&lt;/p>
&lt;p>By default the Anchore Engine uses the /tmp directory within the container to download and extract images. Configure a volume to be mounted into the container at a specified path and configure this path in config.yaml&lt;/p>
&lt;p>&lt;code>tmp_dir: '/scratch'&lt;/code>&lt;/p>
&lt;p>In this example a volume has been mounted as /scratch within the container and config.yaml updated to use /scratch as the temporary directory for image analysis.&lt;/p>
&lt;p>With the cache disabled the temporary directory should be sized to at least 3 times the uncompressed image size to be analyzed.
To enable layer caching the layer_cache_enable parameter and layer_cache_max_gigabytes parameter should be added to the analyzer section of the Anchore Engine configuration file config.yaml.&lt;/p>
&lt;div class="highlight">&lt;pre style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4">&lt;code class="language-YAML" data-lang="YAML">&lt;span style="color:#204a87;font-weight:bold">analyzer&lt;/span>&lt;span style="color:#000;font-weight:bold">:&lt;/span>&lt;span style="color:#f8f8f8;text-decoration:underline">
&lt;/span>&lt;span style="color:#f8f8f8;text-decoration:underline"> &lt;/span>&lt;span style="color:#204a87;font-weight:bold">enabled&lt;/span>&lt;span style="color:#000;font-weight:bold">:&lt;/span>&lt;span style="color:#f8f8f8;text-decoration:underline"> &lt;/span>&lt;span style="color:#204a87;font-weight:bold">True&lt;/span>&lt;span style="color:#f8f8f8;text-decoration:underline">
&lt;/span>&lt;span style="color:#f8f8f8;text-decoration:underline"> &lt;/span>&lt;span style="color:#204a87;font-weight:bold">require_auth&lt;/span>&lt;span style="color:#000;font-weight:bold">:&lt;/span>&lt;span style="color:#f8f8f8;text-decoration:underline"> &lt;/span>&lt;span style="color:#204a87;font-weight:bold">True&lt;/span>&lt;span style="color:#f8f8f8;text-decoration:underline">
&lt;/span>&lt;span style="color:#f8f8f8;text-decoration:underline"> &lt;/span>&lt;span style="color:#204a87;font-weight:bold">cycle_timer_seconds&lt;/span>&lt;span style="color:#000;font-weight:bold">:&lt;/span>&lt;span style="color:#f8f8f8;text-decoration:underline"> &lt;/span>&lt;span style="color:#0000cf;font-weight:bold">1&lt;/span>&lt;span style="color:#f8f8f8;text-decoration:underline">
&lt;/span>&lt;span style="color:#f8f8f8;text-decoration:underline"> &lt;/span>&lt;span style="color:#204a87;font-weight:bold">max_threads&lt;/span>&lt;span style="color:#000;font-weight:bold">:&lt;/span>&lt;span style="color:#f8f8f8;text-decoration:underline"> &lt;/span>&lt;span style="color:#0000cf;font-weight:bold">1&lt;/span>&lt;span style="color:#f8f8f8;text-decoration:underline">
&lt;/span>&lt;span style="color:#f8f8f8;text-decoration:underline"> &lt;/span>&lt;span style="color:#204a87;font-weight:bold">analyzer_driver&lt;/span>&lt;span style="color:#000;font-weight:bold">:&lt;/span>&lt;span style="color:#f8f8f8;text-decoration:underline"> &lt;/span>&lt;span style="color:#4e9a06">&amp;#39;nodocker&amp;#39;&lt;/span>&lt;span style="color:#f8f8f8;text-decoration:underline">
&lt;/span>&lt;span style="color:#f8f8f8;text-decoration:underline"> &lt;/span>&lt;span style="color:#204a87;font-weight:bold">endpoint_hostname&lt;/span>&lt;span style="color:#000;font-weight:bold">:&lt;/span>&lt;span style="color:#f8f8f8;text-decoration:underline"> &lt;/span>&lt;span style="color:#4e9a06">&amp;#39;${ANCHORE_HOST_ID}&amp;#39;&lt;/span>&lt;span style="color:#f8f8f8;text-decoration:underline">
&lt;/span>&lt;span style="color:#f8f8f8;text-decoration:underline"> &lt;/span>&lt;span style="color:#204a87;font-weight:bold">listen&lt;/span>&lt;span style="color:#000;font-weight:bold">:&lt;/span>&lt;span style="color:#f8f8f8;text-decoration:underline"> &lt;/span>&lt;span style="color:#4e9a06">&amp;#39;0.0.0.0&amp;#39;&lt;/span>&lt;span style="color:#f8f8f8;text-decoration:underline">
&lt;/span>&lt;span style="color:#f8f8f8;text-decoration:underline"> &lt;/span>&lt;span style="color:#204a87;font-weight:bold">port&lt;/span>&lt;span style="color:#000;font-weight:bold">:&lt;/span>&lt;span style="color:#f8f8f8;text-decoration:underline"> &lt;/span>&lt;span style="color:#0000cf;font-weight:bold">8084&lt;/span>&lt;span style="color:#f8f8f8;text-decoration:underline">
&lt;/span>&lt;span style="color:#f8f8f8;text-decoration:underline"> &lt;/span>&lt;span style="color:#204a87;font-weight:bold">layer_cache_enable&lt;/span>&lt;span style="color:#000;font-weight:bold">:&lt;/span>&lt;span style="color:#f8f8f8;text-decoration:underline"> &lt;/span>&lt;span style="color:#204a87;font-weight:bold">True&lt;/span>&lt;span style="color:#f8f8f8;text-decoration:underline">
&lt;/span>&lt;span style="color:#f8f8f8;text-decoration:underline"> &lt;/span>&lt;span style="color:#204a87;font-weight:bold">layer_cache_max_gigabytes&lt;/span>&lt;span style="color:#000;font-weight:bold">:&lt;/span>&lt;span style="color:#f8f8f8;text-decoration:underline"> &lt;/span>&lt;span style="color:#0000cf;font-weight:bold">4&lt;/span>&lt;span style="color:#f8f8f8;text-decoration:underline">
&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>In this example the cache is set to 4 gigabytes. The temporary volume should be sized to at least 3 times the uncompressed image size + 4 gigabytes.&lt;/p>
&lt;ul>
&lt;li>The minimum size for the cache is 1 gigabyte.&lt;/li>
&lt;li>The cache users a least recently used (LRU) policy.&lt;/li>
&lt;li>The cache files will be stored in the anchore_layercache directory of the /tmp_dir volume.&lt;/li>
&lt;/ul></description></item><item><title>Docs: Memory</title><link>/docs/install/requirements/memory/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/docs/install/requirements/memory/</guid><description>
&lt;p>The Anchore Engine container will typically operate at a steady state using less than 2 GB of memory. However under load and during large feed synchronization operations, memory usage may burst above 4GB. Anchore recommends a minimum of 8GB for each service, for production deployments.&lt;/p></description></item><item><title>Docs: Network</title><link>/docs/install/requirements/network/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/docs/install/requirements/network/</guid><description>
&lt;p>The Anchore Engine requires two categories of network access:&lt;/p>
&lt;ul>
&lt;li>Registry Access
Network connectivity, including DNS resolution, to the registries from which the Anchore Engine needs to download images.&lt;/li>
&lt;li>Feed Service
The Anchore Engine synchronizes feed data such as operating system vulnerabilities (CVEs) from the Anchore Cloud Service. The initial synchronization may take 5 to 10 minutes, based on network speed, after which time the Anchore Engine will download updated feed data at a user configurable interval, by default every 4 hours. Only a single end point is required for this synchronization, host: ancho.re TCP port: 443&lt;/li>
&lt;/ul>
&lt;p>Notes:&lt;/p>
&lt;ul>
&lt;li>No data is uploaded to Anchore, the synchronization is one-way.&lt;/li>
&lt;li>An optional on-premises feed service and policy editor is available to commercial users.&lt;/li>
&lt;li>A Network Proxy can be used to provide external connectivity to the Anchore Engine. See: &lt;a href="/docs/install/configuration/network_proxies/">Network Proxy documentation&lt;/a>.&lt;/li>
&lt;/ul></description></item><item><title>Docs: Policy Gate: dockerfile</title><link>/docs/usage/cli_usage/policies/policy_gate_dockerfile/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/docs/usage/cli_usage/policies/policy_gate_dockerfile/</guid><description>
&lt;p>This article reviews the &amp;ldquo;dockerfile&amp;rdquo; gate and its triggers. The dockerfile gate allows users to perform checks on the content of the dockerfile or docker history for an image and make policy actions based on the construction of an image, not just its content. This is particularly useful for enforcing best practices or metadata inclusion (e.g. labels) on images.&lt;/p>
&lt;p>Anchore is either given a dockerfile or infers one from the docker image layer history. There are implications to what data is available and what it means depending on these differing sources, so first, we&amp;rsquo;ll cover the input data for the gate and how it impacts the triggers and parameters used.&lt;/p>
&lt;h3 id="the-dockerfile">The &amp;ldquo;dockerfile&amp;rdquo;&lt;/h3>
&lt;p>The data that this gate operates on can come from two different sources:&lt;/p>
&lt;ol>
&lt;li>The actual dockerfile used to build an image, as provided by the user at the time of running &lt;code>anchore-cli image add &amp;lt;img ref&amp;gt; --dockerfile &amp;lt;filename&amp;gt;&lt;/code> or the corresponding API call to: POST /images?dockerfile=&lt;!-- raw HTML omitted -->&lt;/li>
&lt;li>The history from layers as encoded in the image itself (see &lt;code>docker history &amp;lt;img&amp;gt;&lt;/code> for this output)&lt;/li>
&lt;/ol>
&lt;p>All images have data from history available, but data from the actual dockerfile is only available when a user provides it. This also means that any images analyzed by the tag watcher functionality will not have an actual dockerfile.&lt;/p>
&lt;h4 id="the-from-line">The FROM line&lt;/h4>
&lt;p>In the actual dockerfile, the FROM instruction is preserved and available as used to build the image, however in the history data, the FROM line will always be the very first FROM instruction used to build the image and all of its dependent based image. Thus, for most images, the value in the history will be omitted and Anchore will automatically infer a FROM scratch line, which is logically inserted for this gate if the dockerfile/history does not contain an explicit FROM entry.&lt;/p>
&lt;p>For example, using the docker.io/jenkins/jenkins image:&lt;/p>
&lt;pre>&lt;code>IMAGE CREATED CREATED BY SIZE COMMENT
sha256:3b9c9666a66e53473c05a3c69eb2cb888a8268f76935eecc7530653cddc28981 11 hours ago /bin/sh -c #(nop) COPY file:3a15c25533fd87983edc33758f62af7b543ccc3ce9dd570e473eb0702f5f298e in /usr/local/bin/install-plugins.sh 8.79kB
&amp;lt;missing&amp;gt; 11 hours ago /bin/sh -c #(nop) COPY file:f97999fac8a63cf8b635a54ea84a2bc95ae3da4d81ab55267c92b28b502d8812 in /usr/local/bin/plugins.sh 3.96kB
&amp;lt;missing&amp;gt; 11 hours ago /bin/sh -c #(nop) ENTRYPOINT [&amp;quot;/sbin/tini&amp;quot; &amp;quot;--&amp;quot; &amp;quot;/usr/local/bin/jenkins.sh&amp;quot;] 0B
&amp;lt;missing&amp;gt; 11 hours ago /bin/sh -c #(nop) COPY file:dc942ca949bb159f81bbc954773b3491e433d2d3e3ef90bac80ecf48a313c9c9 in /bin/tini 529B
&amp;lt;missing&amp;gt; 11 hours ago /bin/sh -c #(nop) COPY file:a8f986413b77bf4d88562b9d3a0dce98ab6e75403192aa4d4153fb41f450843d in /usr/local/bin/jenkins.sh 1.45kB
&amp;lt;missing&amp;gt; 11 hours ago /bin/sh -c #(nop) COPY file:55594d9d2aed007553a6743a43039b1a48b30527f8fb991ad93e1fd5b1298f60 in /usr/local/bin/jenkins-support 6.12kB
&amp;lt;missing&amp;gt; 11 hours ago /bin/sh -c #(nop) USER jenkins 0B
&amp;lt;missing&amp;gt; 11 hours ago /bin/sh -c #(nop) ENV COPY_REFERENCE_FILE_LOG=/var/jenkins_home/copy_reference_file.log 0B
&amp;lt;missing&amp;gt; 11 hours ago /bin/sh -c #(nop) EXPOSE 50000 0B
&amp;lt;missing&amp;gt; 11 hours ago /bin/sh -c #(nop) EXPOSE 8080 0B
&amp;lt;missing&amp;gt; 11 hours ago |9 JENKINS_SHA=e026221efcec9528498019b6c1581cca70fe9c3f6b10303777d85c6699bca0e4 JENKINS_URL=https://repo.jenkins-ci.org/public/org/jenkins-ci/main/jenkins-war/2.161/jenkins-war-2.161.war TINI_VERSION=v0.16.1 agent_port=50000 gid=1000 group=jenkins http_port=8080 uid=1000 user=jenkins /bin/sh -c chown -R ${user} &amp;quot;$JENKINS_HOME&amp;quot; /usr/share/jenkins/ref 328B
&amp;lt;missing&amp;gt; 11 hours ago /bin/sh -c #(nop) ENV JENKINS_INCREMENTALS_REPO_MIRROR=https://repo.jenkins-ci.org/incrementals 0B
&amp;lt;missing&amp;gt; 11 hours ago /bin/sh -c #(nop) ENV JENKINS_UC_EXPERIMENTAL=https://updates.jenkins.io/experimental 0B
&amp;lt;missing&amp;gt; 11 hours ago /bin/sh -c #(nop) ENV JENKINS_UC=https://updates.jenkins.io 0B
&amp;lt;missing&amp;gt; 11 hours ago |9 JENKINS_SHA=e026221efcec9528498019b6c1581cca70fe9c3f6b10303777d85c6699bca0e4 JENKINS_URL=https://repo.jenkins-ci.org/public/org/jenkins-ci/main/jenkins-war/2.161/jenkins-war-2.161.war TINI_VERSION=v0.16.1 agent_port=50000 gid=1000 group=jenkins http_port=8080 uid=1000 user=jenkins /bin/sh -c curl -fsSL ${JENKINS_URL} -o /usr/share/jenkins/jenkins.war &amp;amp;&amp;amp; echo &amp;quot;${JENKINS_SHA} /usr/share/jenkins/jenkins.war&amp;quot; | sha256sum -c - 76MB
&amp;lt;missing&amp;gt; 11 hours ago /bin/sh -c #(nop) ARG JENKINS_URL=https://repo.jenkins-ci.org/public/org/jenkins-ci/main/jenkins-war/2.161/jenkins-war-2.161.war 0B
&amp;lt;missing&amp;gt; 11 hours ago /bin/sh -c #(nop) ARG JENKINS_SHA=5bb075b81a3929ceada4e960049e37df5f15a1e3cfc9dc24d749858e70b48919 0B
&amp;lt;missing&amp;gt; 11 hours ago /bin/sh -c #(nop) ENV JENKINS_VERSION=2.161 0B
&amp;lt;missing&amp;gt; 11 hours ago /bin/sh -c #(nop) ARG JENKINS_VERSION 0B
&amp;lt;missing&amp;gt; 11 hours ago /bin/sh -c #(nop) COPY file:c84b91c835048a52bb864c1f4662607c56befe3c4b1520b0ea94633103a4554f in /usr/share/jenkins/ref/init.groovy.d/tcp-slave-agent-port.groovy 328B
&amp;lt;missing&amp;gt; 11 hours ago |7 TINI_VERSION=v0.16.1 agent_port=50000 gid=1000 group=jenkins http_port=8080 uid=1000 user=jenkins /bin/sh -c curl -fsSL https://github.com/krallin/tini/releases/download/${TINI_VERSION}/tini-static-$(dpkg --print-architecture) -o /sbin/tini &amp;amp;&amp;amp; curl -fsSL https://github.com/krallin/tini/releases/download/${TINI_VERSION}/tini-static-$(dpkg --print-architecture).asc -o /sbin/tini.asc &amp;amp;&amp;amp; gpg --no-tty --import ${JENKINS_HOME}/tini_pub.gpg &amp;amp;&amp;amp; gpg --verify /sbin/tini.asc &amp;amp;&amp;amp; rm -rf /sbin/tini.asc /root/.gnupg &amp;amp;&amp;amp; chmod +x /sbin/tini 866kB
&amp;lt;missing&amp;gt; 11 hours ago /bin/sh -c #(nop) COPY file:653491cb486e752a4c2b4b407a46ec75646a54eabb597634b25c7c2b82a31424 in /var/jenkins_home/tini_pub.gpg 7.15kB
&amp;lt;missing&amp;gt; 11 hours ago /bin/sh -c #(nop) ARG TINI_VERSION=v0.16.1 0B
&amp;lt;missing&amp;gt; 11 hours ago |6 agent_port=50000 gid=1000 group=jenkins http_port=8080 uid=1000 user=jenkins /bin/sh -c mkdir -p /usr/share/jenkins/ref/init.groovy.d 0B
&amp;lt;missing&amp;gt; 11 hours ago /bin/sh -c #(nop) VOLUME [/var/jenkins_home] 0B
&amp;lt;missing&amp;gt; 11 hours ago |6 agent_port=50000 gid=1000 group=jenkins http_port=8080 uid=1000 user=jenkins /bin/sh -c mkdir -p $JENKINS_HOME &amp;amp;&amp;amp; chown ${uid}:${gid} $JENKINS_HOME &amp;amp;&amp;amp; groupadd -g ${gid} ${group} &amp;amp;&amp;amp; useradd -d &amp;quot;$JENKINS_HOME&amp;quot; -u ${uid} -g ${gid} -m -s /bin/bash ${user} 328kB
&amp;lt;missing&amp;gt; 11 hours ago /bin/sh -c #(nop) ENV JENKINS_SLAVE_AGENT_PORT=50000 0B
&amp;lt;missing&amp;gt; 11 hours ago /bin/sh -c #(nop) ENV JENKINS_HOME=/var/jenkins_home 0B
&amp;lt;missing&amp;gt; 11 hours ago /bin/sh -c #(nop) ARG JENKINS_HOME=/var/jenkins_home 0B
&amp;lt;missing&amp;gt; 11 hours ago /bin/sh -c #(nop) ARG agent_port=50000 0B
&amp;lt;missing&amp;gt; 11 hours ago /bin/sh -c #(nop) ARG http_port=8080 0B
&amp;lt;missing&amp;gt; 11 hours ago /bin/sh -c #(nop) ARG gid=1000 0B
&amp;lt;missing&amp;gt; 11 hours ago /bin/sh -c #(nop) ARG uid=1000 0B
&amp;lt;missing&amp;gt; 11 hours ago /bin/sh -c #(nop) ARG group=jenkins 0B
&amp;lt;missing&amp;gt; 11 hours ago /bin/sh -c #(nop) ARG user=jenkins 0B
&amp;lt;missing&amp;gt; 11 hours ago /bin/sh -c apt-get update &amp;amp;&amp;amp; apt-get install -y git curl &amp;amp;&amp;amp; rm -rf /var/lib/apt/lists/* 0B
&amp;lt;missing&amp;gt; 3 weeks ago /bin/sh -c set -ex; if [ ! -d /usr/share/man/man1 ]; then mkdir -p /usr/share/man/man1; fi; apt-get update; apt-get install -y --no-install-recommends openjdk-8-jdk=&amp;quot;$JAVA_DEBIAN_VERSION&amp;quot; ; rm -rf /var/lib/apt/lists/*; [ &amp;quot;$(readlink -f &amp;quot;$JAVA_HOME&amp;quot;)&amp;quot; = &amp;quot;$(docker-java-home)&amp;quot; ]; update-alternatives --get-selections | awk -v home=&amp;quot;$(readlink -f &amp;quot;$JAVA_HOME&amp;quot;)&amp;quot; 'index($3, home) == 1 { $2 = &amp;quot;manual&amp;quot;; print | &amp;quot;update-alternatives --set-selections&amp;quot; }'; update-alternatives --query java | grep -q 'Status: manual' 348MB
&amp;lt;missing&amp;gt; 3 weeks ago /bin/sh -c #(nop) ENV JAVA_DEBIAN_VERSION=8u181-b13-2~deb9u1 0B
&amp;lt;missing&amp;gt; 3 weeks ago /bin/sh -c #(nop) ENV JAVA_VERSION=8u181 0B
&amp;lt;missing&amp;gt; 3 weeks ago /bin/sh -c #(nop) ENV JAVA_HOME=/docker-java-home 0B
&amp;lt;missing&amp;gt; 3 weeks ago /bin/sh -c ln -svT &amp;quot;/usr/lib/jvm/java-8-openjdk-$(dpkg --print-architecture)&amp;quot; /docker-java-home 33B
&amp;lt;missing&amp;gt; 3 weeks ago /bin/sh -c { echo '#!/bin/sh'; echo 'set -e'; echo; echo 'dirname &amp;quot;$(dirname &amp;quot;$(readlink -f &amp;quot;$(which javac || which java)&amp;quot;)&amp;quot;)&amp;quot;'; } &amp;gt; /usr/local/bin/docker-java-home &amp;amp;&amp;amp; chmod +x /usr/local/bin/docker-java-home 87B
&amp;lt;missing&amp;gt; 3 weeks ago /bin/sh -c #(nop) ENV LANG=C.UTF-8 0B
&amp;lt;missing&amp;gt; 3 weeks ago /bin/sh -c apt-get update &amp;amp;&amp;amp; apt-get install -y --no-install-recommends bzip2 unzip xz-utils &amp;amp;&amp;amp; rm -rf /var/lib/apt/lists/* 2.21MB
&amp;lt;missing&amp;gt; 3 weeks ago /bin/sh -c apt-get update &amp;amp;&amp;amp; apt-get install -y --no-install-recommends bzr git mercurial openssh-client subversion procps &amp;amp;&amp;amp; rm -rf /var/lib/apt/lists/* 142MB
&amp;lt;missing&amp;gt; 3 weeks ago /bin/sh -c set -ex; if ! command -v gpg &amp;gt; /dev/null; then apt-get update; apt-get install -y --no-install-recommends gnupg dirmngr ; rm -rf /var/lib/apt/lists/*; fi 7.81MB
&amp;lt;missing&amp;gt; 3 weeks ago /bin/sh -c apt-get update &amp;amp;&amp;amp; apt-get install -y --no-install-recommends ca-certificates curl netbase wget &amp;amp;&amp;amp; rm -rf /var/lib/apt/lists/* 23.2MB
&amp;lt;missing&amp;gt; 3 weeks ago /bin/sh -c #(nop) CMD [&amp;quot;bash&amp;quot;] 0B
&amp;lt;missing&amp;gt; 3 weeks ago /bin/sh -c #(nop) ADD file:da71baf0d22cb2ede91c5e3ff959607e47459a9d7bda220a62a3da362b0e59ea in / 101MB
&lt;/code>&lt;/pre>&lt;p>Where the actual dockerfile for that image is:&lt;/p>
&lt;pre>&lt;code>FROM openjdk:8-jdk-stretch
RUN apt-get update &amp;amp;&amp;amp; apt-get install -y git curl &amp;amp;&amp;amp; rm -rf /var/lib/apt/lists/*
ARG user=jenkins
ARG group=jenkins
ARG uid=1000
ARG gid=1000
ARG http_port=8080
ARG agent_port=50000
ARG JENKINS_HOME=/var/jenkins_home
ENV JENKINS_HOME $JENKINS_HOME
ENV JENKINS_SLAVE_AGENT_PORT ${agent_port}
# Jenkins is run with user `jenkins`, uid = 1000
# If you bind mount a volume from the host or a data container,
# ensure you use the same uid
RUN mkdir -p $JENKINS_HOME \
&amp;amp;&amp;amp; chown ${uid}:${gid} $JENKINS_HOME \
&amp;amp;&amp;amp; groupadd -g ${gid} ${group} \
&amp;amp;&amp;amp; useradd -d &amp;quot;$JENKINS_HOME&amp;quot; -u ${uid} -g ${gid} -m -s /bin/bash ${user}
# Jenkins home directory is a volume, so configuration and build history
# can be persisted and survive image upgrades
VOLUME $JENKINS_HOME
# `/usr/share/jenkins/ref/` contains all reference configuration we want
# to set on a fresh new installation. Use it to bundle additional plugins
# or config file with your custom jenkins Docker image.
RUN mkdir -p /usr/share/jenkins/ref/init.groovy.d
# Use tini as subreaper in Docker container to adopt zombie processes
ARG TINI_VERSION=v0.16.1
COPY tini_pub.gpg ${JENKINS_HOME}/tini_pub.gpg
RUN curl -fsSL https://github.com/krallin/tini/releases/download/${TINI_VERSION}/tini-static-$(dpkg --print-architecture) -o /sbin/tini \
&amp;amp;&amp;amp; curl -fsSL https://github.com/krallin/tini/releases/download/${TINI_VERSION}/tini-static-$(dpkg --print-architecture).asc -o /sbin/tini.asc \
&amp;amp;&amp;amp; gpg --no-tty --import ${JENKINS_HOME}/tini_pub.gpg \
&amp;amp;&amp;amp; gpg --verify /sbin/tini.asc \
&amp;amp;&amp;amp; rm -rf /sbin/tini.asc /root/.gnupg \
&amp;amp;&amp;amp; chmod +x /sbin/tini
COPY init.groovy /usr/share/jenkins/ref/init.groovy.d/tcp-slave-agent-port.groovy
# jenkins version being bundled in this docker image
ARG JENKINS_VERSION
ENV JENKINS_VERSION ${JENKINS_VERSION:-2.121.1}
# jenkins.war checksum, download will be validated using it
ARG JENKINS_SHA=5bb075b81a3929ceada4e960049e37df5f15a1e3cfc9dc24d749858e70b48919
# Can be used to customize where jenkins.war get downloaded from
ARG JENKINS_URL=https://repo.jenkins-ci.org/public/org/jenkins-ci/main/jenkins-war/${JENKINS_VERSION}/jenkins-war-${JENKINS_VERSION}.war
# could use ADD but this one does not check Last-Modified header neither does it allow to control checksum
# see https://github.com/docker/docker/issues/8331
RUN curl -fsSL ${JENKINS_URL} -o /usr/share/jenkins/jenkins.war \
&amp;amp;&amp;amp; echo &amp;quot;${JENKINS_SHA} /usr/share/jenkins/jenkins.war&amp;quot; | sha256sum -c -
ENV JENKINS_UC https://updates.jenkins.io
ENV JENKINS_UC_EXPERIMENTAL=https://updates.jenkins.io/experimental
ENV JENKINS_INCREMENTALS_REPO_MIRROR=https://repo.jenkins-ci.org/incrementals
RUN chown -R ${user} &amp;quot;$JENKINS_HOME&amp;quot; /usr/share/jenkins/ref
# for main web interface:
EXPOSE ${http_port}
# will be used by attached slave agents:
EXPOSE ${agent_port}
ENV COPY_REFERENCE_FILE_LOG $JENKINS_HOME/copy_reference_file.log
USER ${user}
COPY jenkins-support /usr/local/bin/jenkins-support
COPY jenkins.sh /usr/local/bin/jenkins.sh
COPY tini-shim.sh /bin/tini
ENTRYPOINT [&amp;quot;/sbin/tini&amp;quot;, &amp;quot;--&amp;quot;, &amp;quot;/usr/local/bin/jenkins.sh&amp;quot;]
# from a derived Dockerfile, can use `RUN plugins.sh active.txt` to setup /usr/share/jenkins/ref/plugins from a support bundle
COPY plugins.sh /usr/local/bin/plugins.sh
COPY install-plugins.sh /usr/local/bin/install-plugins.sh
&lt;/code>&lt;/pre>&lt;p>Anchore will detect the history/dockerfile as this, if not explicitly provided (note order is reversed from docker history output, so it reads in same order as actual dockerfile):&lt;/p>
&lt;pre>&lt;code>[
{
&amp;quot;Size&amp;quot; : 45323792,
&amp;quot;Tags&amp;quot; : [],
&amp;quot;Comment&amp;quot; : &amp;quot;&amp;quot;,
&amp;quot;Id&amp;quot; : &amp;quot;sha256:cd8eada9c7bb496eb685fc6d2198c33db7cb05daf0fde42e4cf5bf0127cbdf38&amp;quot;,
&amp;quot;Created&amp;quot; : &amp;quot;2018-12-28T23:29:37.981962131Z&amp;quot;,
&amp;quot;CreatedBy&amp;quot; : &amp;quot;/bin/sh -c #(nop) ADD file:da71baf0d22cb2ede91c5e3ff959607e47459a9d7bda220a62a3da362b0e59ea in / &amp;quot;
},
{
&amp;quot;Size&amp;quot; : 0,
&amp;quot;Tags&amp;quot; : [],
&amp;quot;Comment&amp;quot; : &amp;quot;&amp;quot;,
&amp;quot;Id&amp;quot; : &amp;quot;&amp;lt;missing&amp;gt;&amp;quot;,
&amp;quot;Created&amp;quot; : &amp;quot;2018-12-28T23:29:38.226681736Z&amp;quot;,
&amp;quot;CreatedBy&amp;quot; : &amp;quot;/bin/sh -c #(nop) CMD [\&amp;quot;bash\&amp;quot;]&amp;quot;
},
{
&amp;quot;Size&amp;quot; : 10780911,
&amp;quot;Comment&amp;quot; : &amp;quot;&amp;quot;,
&amp;quot;Tags&amp;quot; : [],
&amp;quot;CreatedBy&amp;quot; : &amp;quot;/bin/sh -c apt-get update &amp;amp;&amp;amp; apt-get install -y --no-install-recommends \t\tca-certificates \t\tcurl \t\tnetbase \t\twget \t&amp;amp;&amp;amp; rm -rf /var/lib/apt/lists/*&amp;quot;,
&amp;quot;Created&amp;quot; : &amp;quot;2018-12-29T00:04:28.920875483Z&amp;quot;,
&amp;quot;Id&amp;quot; : &amp;quot;sha256:c2677faec825930a8844845f55454ee0495ceb5bea9fc904d5b3125de863dc1d&amp;quot;
},
{
&amp;quot;Comment&amp;quot; : &amp;quot;&amp;quot;,
&amp;quot;Tags&amp;quot; : [],
&amp;quot;Size&amp;quot; : 4340024,
&amp;quot;CreatedBy&amp;quot; : &amp;quot;/bin/sh -c set -ex; \tif ! command -v gpg &amp;gt; /dev/null; then \t\tapt-get update; \t\tapt-get install -y --no-install-recommends \t\t\tgnupg \t\t\tdirmngr \t\t; \t\trm -rf /var/lib/apt/lists/*; \tfi&amp;quot;,
&amp;quot;Created&amp;quot; : &amp;quot;2018-12-29T00:04:34.642152001Z&amp;quot;,
&amp;quot;Id&amp;quot; : &amp;quot;sha256:fcce419a96b1219a265bf7a933d66b585a6f8d73448533f3833c73ad49fb5e88&amp;quot;
},
{
&amp;quot;Size&amp;quot; : 50062697,
&amp;quot;Tags&amp;quot; : [],
&amp;quot;Comment&amp;quot; : &amp;quot;&amp;quot;,
&amp;quot;Id&amp;quot; : &amp;quot;sha256:045b51e26e750443c84216071a1367a7aae0b76245800629dc04934628b4b1ea&amp;quot;,
&amp;quot;CreatedBy&amp;quot; : &amp;quot;/bin/sh -c apt-get update &amp;amp;&amp;amp; apt-get install -y --no-install-recommends \t\tbzr \t\tgit \t\tmercurial \t\topenssh-client \t\tsubversion \t\t\t\tprocps \t&amp;amp;&amp;amp; rm -rf /var/lib/apt/lists/*&amp;quot;,
&amp;quot;Created&amp;quot; : &amp;quot;2018-12-29T00:04:59.676112605Z&amp;quot;
},
... &amp;lt;truncated for brevity&amp;gt; ...
{
&amp;quot;Tags&amp;quot; : [],
&amp;quot;Comment&amp;quot; : &amp;quot;&amp;quot;,
&amp;quot;Size&amp;quot; : 0,
&amp;quot;Id&amp;quot; : &amp;quot;&amp;lt;missing&amp;gt;&amp;quot;,
&amp;quot;CreatedBy&amp;quot; : &amp;quot;/bin/sh -c #(nop) ENTRYPOINT [\&amp;quot;/sbin/tini\&amp;quot; \&amp;quot;--\&amp;quot; \&amp;quot;/usr/local/bin/jenkins.sh\&amp;quot;]&amp;quot;,
&amp;quot;Created&amp;quot; : &amp;quot;2019-01-21T08:56:30.737221895Z&amp;quot;
},
{
&amp;quot;Size&amp;quot; : 1549,
&amp;quot;Tags&amp;quot; : [],
&amp;quot;Comment&amp;quot; : &amp;quot;&amp;quot;,
&amp;quot;Id&amp;quot; : &amp;quot;sha256:283cd3aba8691a3b9d22d923de66243b105758e74de7d9469fe55a6a58aeee30&amp;quot;,
&amp;quot;Created&amp;quot; : &amp;quot;2019-01-21T08:56:32.015667468Z&amp;quot;,
&amp;quot;CreatedBy&amp;quot; : &amp;quot;/bin/sh -c #(nop) COPY file:f97999fac8a63cf8b635a54ea84a2bc95ae3da4d81ab55267c92b28b502d8812 in /usr/local/bin/plugins.sh &amp;quot;
},
{
&amp;quot;Comment&amp;quot; : &amp;quot;&amp;quot;,
&amp;quot;Tags&amp;quot; : [],
&amp;quot;Size&amp;quot; : 3079,
&amp;quot;Created&amp;quot; : &amp;quot;2019-01-21T08:56:33.158854485Z&amp;quot;,
&amp;quot;CreatedBy&amp;quot; : &amp;quot;/bin/sh -c #(nop) COPY file:3a15c25533fd87983edc33758f62af7b543ccc3ce9dd570e473eb0702f5f298e in /usr/local/bin/install-plugins.sh &amp;quot;,
&amp;quot;Id&amp;quot; : &amp;quot;sha256:b0ce8ab5a5a7da5d762f25af970f4423b98437a8318cb9852c3f21354cbf914f&amp;quot;
}
]
&lt;/code>&lt;/pre>&lt;p>NOTE: Anchore processes the leading /bin/sh commands, so you do not have to include those in any trigger param config if using the docker history output.&lt;/p>
&lt;h4 id="the-actual_dockerfile_only-parameter">The actual_dockerfile_only Parameter&lt;/h4>
&lt;p>The actual vs history impacts the semantics of the dockerfile gate&amp;rsquo;s triggers. To allow explicit control of the differences, most triggers in this gate includes a parameter: actual_dockerfile_only that if set to true or false will ensure the trigger check is only done on the source of data specified. If actual_dockerfile_only = true, then the trigger will evaluate only if an actual dockerfile is available for the image and will skip evaluation if not. If actual_dockerfile_only is false or omitted, then the trigger will run on the actual dockerfile if available, or the history data if the dockerfile was not provided.&lt;/p>
&lt;h4 id="differences-in-data-between-docker-history-and-actual-dockerfile">Differences in data between Docker History and actual Dockerfile&lt;/h4>
&lt;p>With Actual Dockerfile:&lt;/p>
&lt;ol>
&lt;li>FROM line is preserved, so the parent tag of the image is easily available&lt;/li>
&lt;li>Instruction checks are all against instructions created during the build for that exact image, not any parent images
&lt;ol>
&lt;li>When the actual_dockerfile_only parameter is set to true, all instructions from the parent image are ignored in policy processing. This may have some unexpected consequences depending on how your images are structured and layered (e.g. golden base images that establish common patterns of volumes, labels, healthchecks)&lt;/li>
&lt;/ol>
&lt;/li>
&lt;li>COPY/ADD instructions will maintain the actual values used&lt;/li>
&lt;li>Multistage-builds in that specific dockerfile will be visible with multiple FROM lines in the output&lt;/li>
&lt;/ol>
&lt;p>With Docker History data, when no dockerfile is provided:&lt;/p>
&lt;ol>
&lt;li>FROM line is not accurate, and will nearly always default to &amp;lsquo;FROM scratch&amp;rsquo;&lt;/li>
&lt;li>Instructions are processed from all layers in the image&lt;/li>
&lt;li>COPY and ADD instructions are transformed into SHAs rather than the actual file path/name used at build-time&lt;/li>
&lt;li>Multi-stage builds are not tracked with multiple FROM lines, only the copy operations between the phases&lt;/li>
&lt;/ol>
&lt;h3 id="trigger-instruction">Trigger: instruction&lt;/h3>
&lt;p>This trigger evaluates instructions found in the &amp;ldquo;dockerfile&amp;rdquo;&lt;/p>
&lt;p>Supported Directives/Instructions (as of Anchore Engine v.0.3.2):&lt;/p>
&lt;h4 id="parameters">Parameters&lt;/h4>
&lt;p>actual_dockerfile_only (optional): See above&lt;/p>
&lt;p>instruction: The dockerfile instruction to check against. One of:&lt;/p>
&lt;ul>
&lt;li>ADD&lt;/li>
&lt;li>ARG&lt;/li>
&lt;li>COPY&lt;/li>
&lt;li>CMD&lt;/li>
&lt;li>ENTRYPOINT&lt;/li>
&lt;li>ENV&lt;/li>
&lt;li>EXPOSE&lt;/li>
&lt;li>FROM&lt;/li>
&lt;li>HEALTHCHECK&lt;/li>
&lt;li>LABEL&lt;/li>
&lt;li>MAINTAINER&lt;/li>
&lt;li>ONBUILD&lt;/li>
&lt;li>USER&lt;/li>
&lt;li>RUN&lt;/li>
&lt;li>SHELL&lt;/li>
&lt;li>STOPSIGNAL&lt;/li>
&lt;li>VOLUME&lt;/li>
&lt;li>WORKDIR&lt;/li>
&lt;/ul>
&lt;p>&lt;em>check:&lt;/em> The comparison/evaluation to perform. One of: =, != , exists, not_exists, like, not_like, in, not_in.&lt;/p>
&lt;p>&lt;em>value&lt;/em> (optional): A string value to compare against, if applicable&lt;/p>
&lt;h4 id="examples">Examples&lt;/h4>
&lt;ol>
&lt;li>Ensure an image has a HEALTHCHECK defined in the image (warn if not found):&lt;/li>
&lt;/ol>
&lt;pre>&lt;code>{
&amp;quot;gate&amp;quot;: &amp;quot;dockerfile&amp;quot;,
&amp;quot;trigger&amp;quot;: &amp;quot;instruction&amp;quot;,
&amp;quot;action&amp;quot;: &amp;quot;warn&amp;quot;,
&amp;quot;parameters&amp;quot;: [
{
&amp;quot;name&amp;quot;: &amp;quot;instruction&amp;quot;,
&amp;quot;value&amp;quot;: &amp;quot;HEALTHCHECK&amp;quot;
},
{
&amp;quot;name&amp;quot;: &amp;quot;check&amp;quot;,
&amp;quot;value&amp;quot;: &amp;quot;not_exists&amp;quot;
}
]
}
&lt;/code>&lt;/pre>&lt;ol start="2">
&lt;li>Check for AWS environment variables set:&lt;/li>
&lt;/ol>
&lt;pre>&lt;code>{
&amp;quot;gate&amp;quot;: &amp;quot;dockerfile&amp;quot;,
&amp;quot;trigger&amp;quot;: &amp;quot;instruction&amp;quot;,
&amp;quot;action&amp;quot;: &amp;quot;stop&amp;quot;,
&amp;quot;parameters&amp;quot;: [
{
&amp;quot;name&amp;quot;: &amp;quot;instruction&amp;quot;,
&amp;quot;value&amp;quot;: &amp;quot;ENV&amp;quot;
},
{
&amp;quot;name&amp;quot;: &amp;quot;check&amp;quot;,
&amp;quot;value&amp;quot;: &amp;quot;like&amp;quot;
},
{
&amp;quot;name&amp;quot;: &amp;quot;value&amp;quot;,
&amp;quot;value&amp;quot;: &amp;quot;AWS_.*KEY&amp;quot;
}
]
}
&lt;/code>&lt;/pre>&lt;h3 id="trigger-effective_user">Trigger: effective_user&lt;/h3>
&lt;p>This trigger processes all &lt;code>USER&lt;/code> directives in the dockerfile or history to determine which user will be used to run the container by default (assuming no user is set explicitly at runtime). The detected value is then subject to a whitelist or blacklist filter depending on the configured parameters. Typically, this is used for blacklisting the root user.&lt;/p>
&lt;h4 id="parameters-1">Parameters&lt;/h4>
&lt;p>&lt;em>actual_dockerfile_only&lt;/em> (optional): See above&lt;/p>
&lt;p>&lt;em>users:&lt;/em> A string with a comma delimited list of username to check for&lt;/p>
&lt;p>&lt;em>type:&lt;/em> The type of check to perform. One of: &amp;lsquo;blacklist&amp;rsquo; or &amp;lsquo;whitelist&amp;rsquo;. This determines how the value of the &amp;lsquo;users&amp;rsquo; parameter is interpreted.&lt;/p>
&lt;h4 id="examples-1">Examples&lt;/h4>
&lt;ol>
&lt;li>Blacklist root user&lt;/li>
&lt;/ol>
&lt;pre>&lt;code>{
&amp;quot;gate&amp;quot;: &amp;quot;dockerfile&amp;quot;,
&amp;quot;trigger&amp;quot;: &amp;quot;effective_user&amp;quot;,
&amp;quot;action&amp;quot;: &amp;quot;stop&amp;quot;,
&amp;quot;parameters&amp;quot;: [
{
&amp;quot;name&amp;quot;: &amp;quot;users&amp;quot;,
&amp;quot;value&amp;quot;: &amp;quot;root&amp;quot;
},
{
&amp;quot;name&amp;quot;: &amp;quot;type&amp;quot;,
&amp;quot;value&amp;quot;: &amp;quot;blacklist&amp;quot;
}
]
}
&lt;/code>&lt;/pre>&lt;ol start="2">
&lt;li>Blacklist root user but only if set in actual dockerfile, not inherited from parent image&lt;/li>
&lt;/ol>
&lt;pre>&lt;code>{
&amp;quot;gate&amp;quot;: &amp;quot;dockerfile&amp;quot;,
&amp;quot;trigger&amp;quot;: &amp;quot;effective_user&amp;quot;,
&amp;quot;action&amp;quot;: &amp;quot;stop&amp;quot;,
&amp;quot;parameters&amp;quot;: [
{
&amp;quot;name&amp;quot;: &amp;quot;users&amp;quot;,
&amp;quot;value&amp;quot;: &amp;quot;root&amp;quot;
},
{
&amp;quot;name&amp;quot;: &amp;quot;type&amp;quot;,
&amp;quot;value&amp;quot;: &amp;quot;blacklist&amp;quot;
},
{
&amp;quot;name&amp;quot;: &amp;quot;actual_dockerfile_only&amp;quot;,
&amp;quot;value&amp;quot;: &amp;quot;true&amp;quot;
}
]
}
&lt;/code>&lt;/pre>&lt;ol start="3">
&lt;li>Warn if the user is not either &amp;ldquo;nginx&amp;rdquo; or &amp;ldquo;jenkins&amp;rdquo;&lt;/li>
&lt;/ol>
&lt;pre>&lt;code>{
&amp;quot;gate&amp;quot;: &amp;quot;dockerfile&amp;quot;,
&amp;quot;trigger&amp;quot;: &amp;quot;effective_user&amp;quot;,
&amp;quot;action&amp;quot;: &amp;quot;warn&amp;quot;,
&amp;quot;parameters&amp;quot;: [
{
&amp;quot;name&amp;quot;: &amp;quot;users&amp;quot;,
&amp;quot;value&amp;quot;: &amp;quot;nginx,jenkins&amp;quot;
},
{
&amp;quot;name&amp;quot;: &amp;quot;type&amp;quot;,
&amp;quot;value&amp;quot;: &amp;quot;whitelist&amp;quot;
}
]
}
&lt;/code>&lt;/pre>&lt;h3 id="trigger-exposed_ports">Trigger: exposed_ports&lt;/h3>
&lt;p>This trigger processes the set of &lt;code>EXPOSE&lt;/code> directives in the dockerfile/history to determine the set of ports that are defined to be exposed (since it can span multiple directives). It performs checks on that set to blacklist/whitelist them based on parameter settings.&lt;/p>
&lt;h4 id="parameters-2">Parameters&lt;/h4>
&lt;p>&lt;em>actual_dockerfile_only&lt;/em> (optional): See above&lt;/p>
&lt;p>&lt;em>ports:&lt;/em> String of comma delimited port numbers to be checked&lt;/p>
&lt;p>&lt;em>type:&lt;/em> The type of check to perform. One of: &amp;lsquo;blacklist&amp;rsquo; or &amp;lsquo;whitelist&amp;rsquo;. This determines how the value of the &amp;lsquo;users&amp;rsquo; parameter is interpreted&lt;/p>
&lt;h4 id="examples-2">Examples&lt;/h4>
&lt;ol>
&lt;li>Allow only ports 80 and 443. Trigger will fire on any port defined to be exposed that is not 80 or 443&lt;/li>
&lt;/ol>
&lt;pre>&lt;code>{
&amp;quot;gate&amp;quot;: &amp;quot;dockerfile&amp;quot;,
&amp;quot;trigger&amp;quot;: &amp;quot;exposed_ports&amp;quot;,
&amp;quot;action&amp;quot;: &amp;quot;warn&amp;quot;,
&amp;quot;parameters&amp;quot;: [
{
&amp;quot;name&amp;quot;: &amp;quot;ports&amp;quot;,
&amp;quot;value&amp;quot;: &amp;quot;80,443&amp;quot;
},
{
&amp;quot;name&amp;quot;: &amp;quot;type&amp;quot;,
&amp;quot;value&amp;quot;: &amp;quot;whitelist&amp;quot;
}
]
}
&lt;/code>&lt;/pre>&lt;ol start="2">
&lt;li>Blacklist ports 21 (ftp), 22 (ssh), and 53 (dns) . Trigger will fire a match on ports 21, 22, 53 if found in EXPOSE directives&lt;/li>
&lt;/ol>
&lt;pre>&lt;code>{
&amp;quot;gate&amp;quot;: &amp;quot;dockerfile&amp;quot;,
&amp;quot;trigger&amp;quot;: &amp;quot;exposed_ports&amp;quot;,
&amp;quot;action&amp;quot;: &amp;quot;warn&amp;quot;,
&amp;quot;parameters&amp;quot;: [
{
&amp;quot;name&amp;quot;: &amp;quot;ports&amp;quot;,
&amp;quot;value&amp;quot;: &amp;quot;21,22,53&amp;quot;
},
{
&amp;quot;name&amp;quot;: &amp;quot;type&amp;quot;,
&amp;quot;value&amp;quot;: &amp;quot;blacklist&amp;quot;
}
]
}
&lt;/code>&lt;/pre>&lt;h3 id="trigger-no_dockerfile_provided">Trigger: no_dockerfile_provided&lt;/h3>
&lt;p>This trigger allows checks on the way the image was added, firing if the dockerfile was not explicitly provided at analysis time. This is useful in identifying and qualifying other trigger matches.&lt;/p>
&lt;h4 id="parameters-3">Parameters&lt;/h4>
&lt;p>None&lt;/p>
&lt;h4 id="examples-3">Examples&lt;/h4>
&lt;ol>
&lt;li>Raise a warning if no dockerfile was provided at analysis time&lt;/li>
&lt;/ol>
&lt;pre>&lt;code>{
&amp;quot;gate&amp;quot;: &amp;quot;dockerfile&amp;quot;,
&amp;quot;trigger&amp;quot;: &amp;quot;no_dockerfile_provided&amp;quot;,
&amp;quot;action&amp;quot;: &amp;quot;warn&amp;quot;,
&amp;quot;parameters&amp;quot;: []
}
&lt;/code>&lt;/pre></description></item><item><title>Docs: Policy Mappings</title><link>/docs/general/concepts/policy/policy_mappings/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/docs/general/concepts/policy/policy_mappings/</guid><description>
&lt;p>Mappings in the policy bundle are a set of rules, evaluated in order, that describe matches on an image, id, digest, or tag and the corresponding sets of policies and whitelists to apply to any image that matches the rule&amp;rsquo;s criteria.&lt;/p>
&lt;p>A mapping has:&lt;/p>
&lt;ul>
&lt;li>Registry - The registry url to match, including wildcards (e.g. &amp;lsquo;docker.io&amp;rsquo;, &amp;lsquo;quay.io&amp;rsquo;, &amp;lsquo;gcr.io&amp;rsquo;, &amp;lsquo;*')&lt;/li>
&lt;li>Repository - The repository name to match, including wildcards (e.g. &amp;lsquo;library/nginx&amp;rsquo;, &amp;lsquo;mydockerhubusername/myrepositoryname&amp;rsquo;, &amp;lsquo;library/&lt;em>&amp;rsquo;, &amp;lsquo;&lt;/em>')&lt;/li>
&lt;li>Image - The way to select an image that matches the registry and repository filters
&lt;ul>
&lt;li>type: how to reference the image and the expected format of the &amp;lsquo;value&amp;rsquo; property
&lt;ul>
&lt;li>&amp;ldquo;tag&amp;rdquo; - just the tag name itself (the part after the &amp;lsquo;:&amp;rsquo; in a docker pull string: e.g. nginx:latest -&amp;gt; &amp;lsquo;latest&amp;rsquo; is the tag name)&lt;/li>
&lt;li>&amp;ldquo;id&amp;rdquo; - the image id&lt;/li>
&lt;li>&amp;ldquo;digest&amp;rdquo; - the image digest (e.g. sha256@abc123)&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>value: the value to match against, including wildcards&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;p>&lt;strong>Note:&lt;/strong> Unlike other parts of the policy bundle, Mappings are evaluated in order and will halt on the first matching rule. This is important to understand when combined with wildcard matches since it enables sophisticated matching behavior.&lt;/p>
&lt;h3 id="examples">Examples&lt;/h3>
&lt;p>Example 1, all images match a single catch-all rule:&lt;/p>
&lt;div class="highlight">&lt;pre style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4">&lt;code class="language-JSON" data-lang="JSON">&lt;span style="color:#000;font-weight:bold">[&lt;/span>
&lt;span style="color:#000;font-weight:bold">{&lt;/span>
&lt;span style="color:#204a87;font-weight:bold">&amp;#34;registry&amp;#34;&lt;/span>&lt;span style="color:#000;font-weight:bold">:&lt;/span> &lt;span style="color:#4e9a06">&amp;#34;*&amp;#34;&lt;/span>&lt;span style="color:#000;font-weight:bold">,&lt;/span>
&lt;span style="color:#204a87;font-weight:bold">&amp;#34;repository&amp;#34;&lt;/span>&lt;span style="color:#000;font-weight:bold">:&lt;/span> &lt;span style="color:#4e9a06">&amp;#34;*&amp;#34;&lt;/span>&lt;span style="color:#000;font-weight:bold">,&lt;/span>
&lt;span style="color:#204a87;font-weight:bold">&amp;#34;image&amp;#34;&lt;/span>&lt;span style="color:#000;font-weight:bold">:&lt;/span> &lt;span style="color:#000;font-weight:bold">{&lt;/span> &lt;span style="color:#204a87;font-weight:bold">&amp;#34;type&amp;#34;&lt;/span>&lt;span style="color:#000;font-weight:bold">:&lt;/span> &lt;span style="color:#4e9a06">&amp;#34;tag&amp;#34;&lt;/span>&lt;span style="color:#000;font-weight:bold">,&lt;/span> &lt;span style="color:#204a87;font-weight:bold">&amp;#34;value&amp;#34;&lt;/span>&lt;span style="color:#000;font-weight:bold">:&lt;/span> &lt;span style="color:#4e9a06">&amp;#34;*&amp;#34;&lt;/span>&lt;span style="color:#000;font-weight:bold">},&lt;/span>
&lt;span style="color:#204a87;font-weight:bold">&amp;#34;policy_ids&amp;#34;&lt;/span>&lt;span style="color:#000;font-weight:bold">:&lt;/span> &lt;span style="color:#000;font-weight:bold">[&lt;/span>&lt;span style="color:#4e9a06">&amp;#34;defaultpolicy&amp;#34;&lt;/span>&lt;span style="color:#000;font-weight:bold">],&lt;/span>
&lt;span style="color:#204a87;font-weight:bold">&amp;#34;whitelist_ids&amp;#34;&lt;/span>&lt;span style="color:#000;font-weight:bold">:&lt;/span> &lt;span style="color:#000;font-weight:bold">[&lt;/span>&lt;span style="color:#4e9a06">&amp;#34;defaultwhitelist&amp;#34;&lt;/span>&lt;span style="color:#000;font-weight:bold">]&lt;/span>
&lt;span style="color:#000;font-weight:bold">}&lt;/span>
&lt;span style="color:#000;font-weight:bold">]&lt;/span>
&lt;/code>&lt;/pre>&lt;/div>&lt;p>Example 2, all &amp;ldquo;official&amp;rdquo; images from DockerHub are evaluated against &lt;em>officialspolicy&lt;/em> and &lt;em>officialswhitelist&lt;/em> (made up names for this example), while all others from DockerHub will be evaluated against &lt;em>defaultpolicy&lt;/em> and &lt;em>defaultwhitelist&lt;/em> , and private GCR images will be evaluated against &lt;em>gcrpolicy&lt;/em> and &lt;em>gcrwhitelist&lt;/em>:&lt;/p>
&lt;div class="highlight">&lt;pre style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4">&lt;code class="language-JSON" data-lang="JSON">&lt;span style="color:#000;font-weight:bold">[&lt;/span>
&lt;span style="color:#000;font-weight:bold">{&lt;/span>
&lt;span style="color:#204a87;font-weight:bold">&amp;#34;registry&amp;#34;&lt;/span>&lt;span style="color:#000;font-weight:bold">:&lt;/span> &lt;span style="color:#4e9a06">&amp;#34;docker.io&amp;#34;&lt;/span>&lt;span style="color:#000;font-weight:bold">,&lt;/span>
&lt;span style="color:#204a87;font-weight:bold">&amp;#34;repository&amp;#34;&lt;/span>&lt;span style="color:#000;font-weight:bold">:&lt;/span> &lt;span style="color:#4e9a06">&amp;#34;library/*&amp;#34;&lt;/span>&lt;span style="color:#000;font-weight:bold">,&lt;/span>
&lt;span style="color:#204a87;font-weight:bold">&amp;#34;image&amp;#34;&lt;/span>&lt;span style="color:#000;font-weight:bold">:&lt;/span> &lt;span style="color:#000;font-weight:bold">{&lt;/span> &lt;span style="color:#204a87;font-weight:bold">&amp;#34;type&amp;#34;&lt;/span>&lt;span style="color:#000;font-weight:bold">:&lt;/span> &lt;span style="color:#4e9a06">&amp;#34;tag&amp;#34;&lt;/span>&lt;span style="color:#000;font-weight:bold">,&lt;/span> &lt;span style="color:#204a87;font-weight:bold">&amp;#34;value&amp;#34;&lt;/span>&lt;span style="color:#000;font-weight:bold">:&lt;/span> &lt;span style="color:#4e9a06">&amp;#34;*&amp;#34;&lt;/span>&lt;span style="color:#000;font-weight:bold">},&lt;/span>
&lt;span style="color:#204a87;font-weight:bold">&amp;#34;policy_ids&amp;#34;&lt;/span>&lt;span style="color:#000;font-weight:bold">:&lt;/span> &lt;span style="color:#000;font-weight:bold">[&lt;/span> &lt;span style="color:#4e9a06">&amp;#34;officialspolicy&amp;#34;&lt;/span>&lt;span style="color:#000;font-weight:bold">],&lt;/span>
&lt;span style="color:#204a87;font-weight:bold">&amp;#34;whitelist_ids&amp;#34;&lt;/span>&lt;span style="color:#000;font-weight:bold">:&lt;/span> &lt;span style="color:#000;font-weight:bold">[&lt;/span> &lt;span style="color:#4e9a06">&amp;#34;officialswhitelist&amp;#34;&lt;/span>&lt;span style="color:#000;font-weight:bold">]&lt;/span>
&lt;span style="color:#000;font-weight:bold">},&lt;/span>
&lt;span style="color:#000;font-weight:bold">{&lt;/span>
&lt;span style="color:#204a87;font-weight:bold">&amp;#34;registry&amp;#34;&lt;/span>&lt;span style="color:#000;font-weight:bold">:&lt;/span> &lt;span style="color:#4e9a06">&amp;#34;gcr.io&amp;#34;&lt;/span>&lt;span style="color:#000;font-weight:bold">,&lt;/span>
&lt;span style="color:#204a87;font-weight:bold">&amp;#34;repository&amp;#34;&lt;/span>&lt;span style="color:#000;font-weight:bold">:&lt;/span> &lt;span style="color:#4e9a06">&amp;#34;*&amp;#34;&lt;/span>&lt;span style="color:#000;font-weight:bold">,&lt;/span>
&lt;span style="color:#204a87;font-weight:bold">&amp;#34;image&amp;#34;&lt;/span>&lt;span style="color:#000;font-weight:bold">:&lt;/span> &lt;span style="color:#000;font-weight:bold">{&lt;/span> &lt;span style="color:#204a87;font-weight:bold">&amp;#34;type&amp;#34;&lt;/span>&lt;span style="color:#000;font-weight:bold">:&lt;/span> &lt;span style="color:#4e9a06">&amp;#34;tag&amp;#34;&lt;/span>&lt;span style="color:#000;font-weight:bold">,&lt;/span> &lt;span style="color:#204a87;font-weight:bold">&amp;#34;value&amp;#34;&lt;/span>&lt;span style="color:#000;font-weight:bold">:&lt;/span> &lt;span style="color:#4e9a06">&amp;#34;*&amp;#34;&lt;/span>&lt;span style="color:#000;font-weight:bold">},&lt;/span>
&lt;span style="color:#204a87;font-weight:bold">&amp;#34;policy_ids&amp;#34;&lt;/span>&lt;span style="color:#000;font-weight:bold">:&lt;/span> &lt;span style="color:#000;font-weight:bold">[&lt;/span> &lt;span style="color:#4e9a06">&amp;#34;gcrpolicy&amp;#34;&lt;/span>&lt;span style="color:#000;font-weight:bold">],&lt;/span>
&lt;span style="color:#204a87;font-weight:bold">&amp;#34;whitelist_ids&amp;#34;&lt;/span>&lt;span style="color:#000;font-weight:bold">:&lt;/span> &lt;span style="color:#000;font-weight:bold">[&lt;/span> &lt;span style="color:#4e9a06">&amp;#34;gcrwhitelist&amp;#34;&lt;/span>&lt;span style="color:#000;font-weight:bold">]&lt;/span>
&lt;span style="color:#000;font-weight:bold">},&lt;/span>
&lt;span style="color:#000;font-weight:bold">{&lt;/span>
&lt;span style="color:#204a87;font-weight:bold">&amp;#34;registry&amp;#34;&lt;/span>&lt;span style="color:#000;font-weight:bold">:&lt;/span> &lt;span style="color:#4e9a06">&amp;#34;*&amp;#34;&lt;/span>&lt;span style="color:#000;font-weight:bold">,&lt;/span>
&lt;span style="color:#204a87;font-weight:bold">&amp;#34;repository&amp;#34;&lt;/span>&lt;span style="color:#000;font-weight:bold">:&lt;/span> &lt;span style="color:#4e9a06">&amp;#34;*&amp;#34;&lt;/span>&lt;span style="color:#000;font-weight:bold">,&lt;/span>
&lt;span style="color:#204a87;font-weight:bold">&amp;#34;image&amp;#34;&lt;/span>&lt;span style="color:#000;font-weight:bold">:&lt;/span> &lt;span style="color:#000;font-weight:bold">{&lt;/span> &lt;span style="color:#204a87;font-weight:bold">&amp;#34;type&amp;#34;&lt;/span>&lt;span style="color:#000;font-weight:bold">:&lt;/span> &lt;span style="color:#4e9a06">&amp;#34;tag&amp;#34;&lt;/span>&lt;span style="color:#000;font-weight:bold">,&lt;/span> &lt;span style="color:#204a87;font-weight:bold">&amp;#34;value&amp;#34;&lt;/span>&lt;span style="color:#000;font-weight:bold">:&lt;/span> &lt;span style="color:#4e9a06">&amp;#34;*&amp;#34;&lt;/span>&lt;span style="color:#000;font-weight:bold">}&lt;/span>
&lt;span style="color:#4e9a06">&amp;#34;policy_ids&amp;#34;&lt;/span>&lt;span style="color:#000;font-weight:bold">:&lt;/span> &lt;span style="color:#000;font-weight:bold">[&lt;/span> &lt;span style="color:#4e9a06">&amp;#34;defaultpolicy&amp;#34;&lt;/span>&lt;span style="color:#000;font-weight:bold">],&lt;/span>
&lt;span style="color:#204a87;font-weight:bold">&amp;#34;whitelist_ids&amp;#34;&lt;/span>&lt;span style="color:#000;font-weight:bold">:&lt;/span> &lt;span style="color:#000;font-weight:bold">[&lt;/span> &lt;span style="color:#4e9a06">&amp;#34;defaultwhitelist&amp;#34;&lt;/span>&lt;span style="color:#000;font-weight:bold">]&lt;/span>
&lt;span style="color:#000;font-weight:bold">}&lt;/span>
&lt;span style="color:#000;font-weight:bold">]&lt;/span>
&lt;/code>&lt;/pre>&lt;/div>&lt;p>Example 3, all images from a non-known registry will be evaluated against &lt;em>defaultpolicy&lt;/em> and &lt;em>defaultwhitelist&lt;/em>, and an internal registry&amp;rsquo;s images will be evaluated against a different set (&lt;em>internalpolicy&lt;/em> and &lt;em>internalwhitelist&lt;/em>):&lt;/p>
&lt;div class="highlight">&lt;pre style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4">&lt;code class="language-JSON" data-lang="JSON">&lt;span style="color:#000;font-weight:bold">[&lt;/span>
&lt;span style="color:#000;font-weight:bold">{&lt;/span>
&lt;span style="color:#204a87;font-weight:bold">&amp;#34;registry&amp;#34;&lt;/span>&lt;span style="color:#000;font-weight:bold">:&lt;/span> &lt;span style="color:#4e9a06">&amp;#34;myregistry.mydomain.com:5000&amp;#34;&lt;/span>&lt;span style="color:#000;font-weight:bold">,&lt;/span>
&lt;span style="color:#204a87;font-weight:bold">&amp;#34;repository&amp;#34;&lt;/span>&lt;span style="color:#000;font-weight:bold">:&lt;/span> &lt;span style="color:#4e9a06">&amp;#34;*&amp;#34;&lt;/span>&lt;span style="color:#000;font-weight:bold">,&lt;/span>
&lt;span style="color:#204a87;font-weight:bold">&amp;#34;image&amp;#34;&lt;/span>&lt;span style="color:#000;font-weight:bold">:&lt;/span> &lt;span style="color:#000;font-weight:bold">{&lt;/span> &lt;span style="color:#204a87;font-weight:bold">&amp;#34;type&amp;#34;&lt;/span>&lt;span style="color:#000;font-weight:bold">:&lt;/span> &lt;span style="color:#4e9a06">&amp;#34;tag&amp;#34;&lt;/span>&lt;span style="color:#000;font-weight:bold">,&lt;/span> &lt;span style="color:#204a87;font-weight:bold">&amp;#34;value&amp;#34;&lt;/span>&lt;span style="color:#000;font-weight:bold">:&lt;/span> &lt;span style="color:#4e9a06">&amp;#34;*&amp;#34;&lt;/span>&lt;span style="color:#000;font-weight:bold">},&lt;/span>
&lt;span style="color:#204a87;font-weight:bold">&amp;#34;policy_ids&amp;#34;&lt;/span>&lt;span style="color:#000;font-weight:bold">:&lt;/span> &lt;span style="color:#000;font-weight:bold">[&lt;/span> &lt;span style="color:#4e9a06">&amp;#34;internalpolicy&amp;#34;&lt;/span>&lt;span style="color:#000;font-weight:bold">],&lt;/span>
&lt;span style="color:#204a87;font-weight:bold">&amp;#34;whitelist_ids&amp;#34;&lt;/span>&lt;span style="color:#000;font-weight:bold">:&lt;/span> &lt;span style="color:#000;font-weight:bold">[&lt;/span> &lt;span style="color:#4e9a06">&amp;#34;internalwhitelist&amp;#34;&lt;/span>&lt;span style="color:#000;font-weight:bold">]&lt;/span>
&lt;span style="color:#000;font-weight:bold">},&lt;/span>
&lt;span style="color:#000;font-weight:bold">{&lt;/span>
&lt;span style="color:#204a87;font-weight:bold">&amp;#34;registry&amp;#34;&lt;/span>&lt;span style="color:#000;font-weight:bold">:&lt;/span> &lt;span style="color:#4e9a06">&amp;#34;*&amp;#34;&lt;/span>&lt;span style="color:#000;font-weight:bold">,&lt;/span>
&lt;span style="color:#204a87;font-weight:bold">&amp;#34;repository&amp;#34;&lt;/span>&lt;span style="color:#000;font-weight:bold">:&lt;/span> &lt;span style="color:#4e9a06">&amp;#34;*&amp;#34;&lt;/span>&lt;span style="color:#000;font-weight:bold">,&lt;/span>
&lt;span style="color:#204a87;font-weight:bold">&amp;#34;image&amp;#34;&lt;/span>&lt;span style="color:#000;font-weight:bold">:&lt;/span> &lt;span style="color:#000;font-weight:bold">{&lt;/span> &lt;span style="color:#204a87;font-weight:bold">&amp;#34;type&amp;#34;&lt;/span>&lt;span style="color:#000;font-weight:bold">:&lt;/span> &lt;span style="color:#4e9a06">&amp;#34;tag&amp;#34;&lt;/span>&lt;span style="color:#000;font-weight:bold">,&lt;/span> &lt;span style="color:#204a87;font-weight:bold">&amp;#34;value&amp;#34;&lt;/span>&lt;span style="color:#000;font-weight:bold">:&lt;/span> &lt;span style="color:#4e9a06">&amp;#34;*&amp;#34;&lt;/span>&lt;span style="color:#000;font-weight:bold">}&lt;/span>
&lt;span style="color:#4e9a06">&amp;#34;policy_ids&amp;#34;&lt;/span>&lt;span style="color:#000;font-weight:bold">:&lt;/span> &lt;span style="color:#000;font-weight:bold">[&lt;/span> &lt;span style="color:#4e9a06">&amp;#34;defaultpolicy&amp;#34;&lt;/span>&lt;span style="color:#000;font-weight:bold">],&lt;/span>
&lt;span style="color:#204a87;font-weight:bold">&amp;#34;whitelist_ids&amp;#34;&lt;/span>&lt;span style="color:#000;font-weight:bold">:&lt;/span> &lt;span style="color:#000;font-weight:bold">[&lt;/span> &lt;span style="color:#4e9a06">&amp;#34;defaultwhitelist&amp;#34;&lt;/span>&lt;span style="color:#000;font-weight:bold">]&lt;/span>
&lt;span style="color:#000;font-weight:bold">}&lt;/span>
&lt;span style="color:#000;font-weight:bold">]&lt;/span>
&lt;/code>&lt;/pre>&lt;/div>&lt;h3 id="using-multiple-policies-and-whitelists">Using Multiple Policies and Whitelists&lt;/h3>
&lt;p>The result of the evaluation of the mapping section of a policy bundle is the list of policies and whitelists that will be used for actually evaluating the image. Because multiple policies and whitelists can be specified in each mapping rule, you can use granular policies and whitelists and the combined them in the mapping rules.&lt;/p>
&lt;p>Examples of schemes to use for how to split-up policies include:&lt;/p>
&lt;ul>
&lt;li>Different policies for different types of checks such that each policy only uses one or two gates (e.g. vulnerabilities, packages, dockerfile)&lt;/li>
&lt;li>Different policies for web servers, another for database servers, another for logging infrastructure, etc.&lt;/li>
&lt;li>Different policies for different parts of the stack: os-packages vs. application packages&lt;/li>
&lt;/ul></description></item><item><title>Docs: S3 Object Store Driver</title><link>/docs/install/storage/object_store/s3_driver/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/docs/install/storage/object_store/s3_driver/</guid><description>
&lt;p>Using the S3 driver, data can be stored using Amazon&amp;rsquo;s S3 storage or any Amazon S3 API compatible system.&lt;/p>
&lt;div class="highlight">&lt;pre style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4">&lt;code class="language-YAML" data-lang="YAML">&lt;span style="color:#204a87;font-weight:bold">object_store&lt;/span>&lt;span style="color:#000;font-weight:bold">:&lt;/span>&lt;span style="color:#f8f8f8;text-decoration:underline">
&lt;/span>&lt;span style="color:#f8f8f8;text-decoration:underline"> &lt;/span>&lt;span style="color:#204a87;font-weight:bold">compression&lt;/span>&lt;span style="color:#000;font-weight:bold">:&lt;/span>&lt;span style="color:#f8f8f8;text-decoration:underline">
&lt;/span>&lt;span style="color:#f8f8f8;text-decoration:underline"> &lt;/span>&lt;span style="color:#204a87;font-weight:bold">enabled&lt;/span>&lt;span style="color:#000;font-weight:bold">:&lt;/span>&lt;span style="color:#f8f8f8;text-decoration:underline"> &lt;/span>&lt;span style="color:#204a87;font-weight:bold">False&lt;/span>&lt;span style="color:#f8f8f8;text-decoration:underline">
&lt;/span>&lt;span style="color:#f8f8f8;text-decoration:underline"> &lt;/span>&lt;span style="color:#204a87;font-weight:bold">min_size_kbytes&lt;/span>&lt;span style="color:#000;font-weight:bold">:&lt;/span>&lt;span style="color:#f8f8f8;text-decoration:underline"> &lt;/span>&lt;span style="color:#0000cf;font-weight:bold">100&lt;/span>&lt;span style="color:#f8f8f8;text-decoration:underline">
&lt;/span>&lt;span style="color:#f8f8f8;text-decoration:underline"> &lt;/span>&lt;span style="color:#204a87;font-weight:bold">storage_driver&lt;/span>&lt;span style="color:#000;font-weight:bold">:&lt;/span>&lt;span style="color:#f8f8f8;text-decoration:underline">
&lt;/span>&lt;span style="color:#f8f8f8;text-decoration:underline"> &lt;/span>&lt;span style="color:#204a87;font-weight:bold">name&lt;/span>&lt;span style="color:#000;font-weight:bold">:&lt;/span>&lt;span style="color:#f8f8f8;text-decoration:underline"> &lt;/span>&lt;span style="color:#4e9a06">&amp;#39;s3&amp;#39;&lt;/span>&lt;span style="color:#f8f8f8;text-decoration:underline">
&lt;/span>&lt;span style="color:#f8f8f8;text-decoration:underline"> &lt;/span>&lt;span style="color:#204a87;font-weight:bold">config&lt;/span>&lt;span style="color:#000;font-weight:bold">:&lt;/span>&lt;span style="color:#f8f8f8;text-decoration:underline">
&lt;/span>&lt;span style="color:#f8f8f8;text-decoration:underline"> &lt;/span>&lt;span style="color:#204a87;font-weight:bold">access_key&lt;/span>&lt;span style="color:#000;font-weight:bold">:&lt;/span>&lt;span style="color:#f8f8f8;text-decoration:underline"> &lt;/span>&lt;span style="color:#4e9a06">&amp;#39;MY_ACCESS_KEY&amp;#39;&lt;/span>&lt;span style="color:#f8f8f8;text-decoration:underline">
&lt;/span>&lt;span style="color:#f8f8f8;text-decoration:underline"> &lt;/span>&lt;span style="color:#204a87;font-weight:bold">secret_key&lt;/span>&lt;span style="color:#000;font-weight:bold">:&lt;/span>&lt;span style="color:#f8f8f8;text-decoration:underline"> &lt;/span>&lt;span style="color:#4e9a06">&amp;#39;MY_SECRET_KEY&amp;#39;&lt;/span>&lt;span style="color:#f8f8f8;text-decoration:underline">
&lt;/span>&lt;span style="color:#f8f8f8;text-decoration:underline"> &lt;/span>&lt;span style="color:#8f5902;font-style:italic">#iamauto: True&lt;/span>&lt;span style="color:#f8f8f8;text-decoration:underline">
&lt;/span>&lt;span style="color:#f8f8f8;text-decoration:underline"> &lt;/span>&lt;span style="color:#204a87;font-weight:bold">url&lt;/span>&lt;span style="color:#000;font-weight:bold">:&lt;/span>&lt;span style="color:#f8f8f8;text-decoration:underline"> &lt;/span>&lt;span style="color:#4e9a06">&amp;#39;https://S3-end-point.example.com&amp;#39;&lt;/span>&lt;span style="color:#f8f8f8;text-decoration:underline">
&lt;/span>&lt;span style="color:#f8f8f8;text-decoration:underline"> &lt;/span>&lt;span style="color:#204a87;font-weight:bold">region&lt;/span>&lt;span style="color:#000;font-weight:bold">:&lt;/span>&lt;span style="color:#f8f8f8;text-decoration:underline"> &lt;/span>&lt;span style="color:#204a87;font-weight:bold">False&lt;/span>&lt;span style="color:#f8f8f8;text-decoration:underline">
&lt;/span>&lt;span style="color:#f8f8f8;text-decoration:underline"> &lt;/span>&lt;span style="color:#204a87;font-weight:bold">bucket&lt;/span>&lt;span style="color:#000;font-weight:bold">:&lt;/span>&lt;span style="color:#f8f8f8;text-decoration:underline"> &lt;/span>&lt;span style="color:#4e9a06">&amp;#39;anchorearchive&amp;#39;&lt;/span>&lt;span style="color:#f8f8f8;text-decoration:underline">
&lt;/span>&lt;span style="color:#f8f8f8;text-decoration:underline"> &lt;/span>&lt;span style="color:#204a87;font-weight:bold">create_bucket&lt;/span>&lt;span style="color:#000;font-weight:bold">:&lt;/span>&lt;span style="color:#f8f8f8;text-decoration:underline"> &lt;/span>&lt;span style="color:#204a87;font-weight:bold">True&lt;/span>&lt;span style="color:#f8f8f8;text-decoration:underline">
&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;h3 id="compression">Compression&lt;/h3>
&lt;p>The S3 driver supports compression of documents. The documents are JSON formatted and will see significant reduction in
size through compression there is an overhead incurred by running compression and decompression on every access of these
documents. The Anchore Engine can be configured to only compress documents above a certain size to reduce unnecessary
overhead. In the example below any document over 100kb in size will be compressed.&lt;/p>
&lt;h3 id="authentication">Authentication&lt;/h3>
&lt;p>The Anchore Engine can authenticate against the S3 service using one of two methods:&lt;/p>
&lt;ul>
&lt;li>
&lt;p>Amazon Access Keys
Using this method an Access Key and Secret Access key that have access to read and write to the bucket. Parameters:
access_key and secret_key&lt;/p>
&lt;/li>
&lt;li>
&lt;p>Inherit IAM Role
The Anchore Engine can be configured to inherit the IAM role from the EC2 or ECS instance that is running the Anchore
Engine. When launching the EC2 instance that will run the Anchore Engine you need to specify a role that includes the
ability to read and write from the archive bucket. To use IAM roles to authenticate the access_key and secret_access
configurations should be replaced by iamauto: True
Parameters: iamauto&lt;/p>
&lt;/li>
&lt;/ul>
&lt;h3 id="bucket">Bucket&lt;/h3>
&lt;ul>
&lt;li>The url parameter points to the endpoint for the Amazon S3 bucket&lt;/li>
&lt;li>The region parameter should be set to the AWS region hosting the bucket or False for an AWS compatible service that
does not support regions&lt;/li>
&lt;li>The bucket parameter should be set to the name of the bucket that will be used to store the archive documents.&lt;/li>
&lt;li>The create_bucket parameter is used to configure if the Anchore Engine attempts to create a bucket. If this option is
set then ensure that the IAM role or Access Keys have sufficient access to create a new bucket.&lt;/li>
&lt;/ul></description></item><item><title>Docs: Security</title><link>/docs/install/requirements/security/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/docs/install/requirements/security/</guid><description>
&lt;p>The Anchore Engine is deployed as container images that can be run manually, using Docker Compose, Kubernetes or any container platform that supports Docker compatible images.&lt;/p>
&lt;p>By default, the Anchore Engine does not require any special permissions and can be run as an unprivileged container with no access to the underlying Docker host. &lt;em>Note:&lt;/em> The Engine can be configured to pull images through the Docker Socket however this is not a recommended configuration as it grants the Anchore container added privileges and may incur a performance impact on the Docker Host.&lt;/p></description></item><item><title>Docs: Storage</title><link>/docs/install/requirements/storage/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/docs/install/requirements/storage/</guid><description>
&lt;p>The Anchore Engine uses a PostgreSQL database to store persistent data for images, tags, policies, subscriptions and other artifacts. One persistent storage volume is required for configuration information and two optional storage volumes may be provided as described below.&lt;/p>
&lt;ul>
&lt;li>&lt;strong>Configuration volume&lt;/strong>
This volume is used to provide persistent storage to the container from which it will read its configuration files and optionally certificates. &lt;em>Requirement&lt;/em>: Less than 1MB&lt;/li>
&lt;li>[Optional] &lt;strong>Temporary storage&lt;/strong>
The temporary storage volume is recommended but not required. During the analysis of images Anchore Engine downloads and extracts all of the layers required for an image. These layers are extracted and analyzed after which the layers and extracted data are deleted. If a temporary storage is not configured then the container&amp;rsquo;s ephemeral storage will be used to store temporary files, however performance is likely be improved by using a dedicated volume. A temporary storage volume may also be used for image layer caching to speed up analysis. Requirement: 3 times the uncompressed image size to be analyzed. &lt;em>Note&lt;/em>: For container hosts using OverlayFS or OverlayFS2 storage with a kernel older than 4.13 a temporary volume is required to work around a kernel driver bug.&lt;/li>
&lt;li>[Optional] &lt;strong>Object storage&lt;/strong>
The Anchore Engine stores documents containing archives of image analysis data and policies as JSON documents. By default these documents are be stored within the PostgreSQL database however the Anchore Engine can be configured to store archive documents in a filesystem (volume), S3 Object store, or Swift Object Store. &lt;em>Requirement&lt;/em>: Number of images x 10MB (estimated).&lt;/li>
&lt;/ul></description></item><item><title>Docs: Swift Archive Driver</title><link>/docs/install/storage/object_store/swift_driver/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/docs/install/storage/object_store/swift_driver/</guid><description>
&lt;p>Using the Swift driver archive documents can be stored in an OpenStack Swift Object storage service.&lt;/p>
&lt;p>The Swift driver supports three authentication methods:&lt;/p>
&lt;ul>
&lt;li>Keystone V3&lt;/li>
&lt;li>Keystone V2&lt;/li>
&lt;li>Legacy (username / password)&lt;/li>
&lt;/ul>
&lt;h3 id="common-configuration-options">Common Configuration Options&lt;/h3>
&lt;h4 id="compression">Compression&lt;/h4>
&lt;p>The Swift driver supports compression of archive documents. The archive documents are JSON formatted and will see significant reduction in size through compression there is an overhead incurred by running compression and decompression on every access of these documents. The Anchore Engine can be configured to only compress documents above a certain size to reduce unnecessary overhead. In the example below any document over 100kb in size will be compressed.&lt;/p>
&lt;h4 id="container">Container&lt;/h4>
&lt;ul>
&lt;li>The container parameter specifies the name of the container to be used.&lt;/li>
&lt;li>The create_container parameter is used to configure if the Anchore Engine attempts to create a container. If this option is set then ensure that the user has the appropriate rights to create a container.&lt;/li>
&lt;/ul>
&lt;h3 id="legacy-authentication">Legacy Authentication&lt;/h3>
&lt;div class="highlight">&lt;pre style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4">&lt;code class="language-YAML" data-lang="YAML">&lt;span style="color:#204a87;font-weight:bold">object_store&lt;/span>&lt;span style="color:#000;font-weight:bold">:&lt;/span>&lt;span style="color:#f8f8f8;text-decoration:underline">
&lt;/span>&lt;span style="color:#f8f8f8;text-decoration:underline"> &lt;/span>&lt;span style="color:#204a87;font-weight:bold">compression&lt;/span>&lt;span style="color:#000;font-weight:bold">:&lt;/span>&lt;span style="color:#f8f8f8;text-decoration:underline">
&lt;/span>&lt;span style="color:#f8f8f8;text-decoration:underline"> &lt;/span>&lt;span style="color:#204a87;font-weight:bold">enabled&lt;/span>&lt;span style="color:#000;font-weight:bold">:&lt;/span>&lt;span style="color:#f8f8f8;text-decoration:underline"> &lt;/span>&lt;span style="color:#204a87;font-weight:bold">True&lt;/span>&lt;span style="color:#f8f8f8;text-decoration:underline">
&lt;/span>&lt;span style="color:#f8f8f8;text-decoration:underline"> &lt;/span>&lt;span style="color:#204a87;font-weight:bold">min_size_kbytes&lt;/span>&lt;span style="color:#000;font-weight:bold">:&lt;/span>&lt;span style="color:#f8f8f8;text-decoration:underline"> &lt;/span>&lt;span style="color:#0000cf;font-weight:bold">100&lt;/span>&lt;span style="color:#f8f8f8;text-decoration:underline">
&lt;/span>&lt;span style="color:#f8f8f8;text-decoration:underline"> &lt;/span>&lt;span style="color:#204a87;font-weight:bold">storage_driver&lt;/span>&lt;span style="color:#000;font-weight:bold">:&lt;/span>&lt;span style="color:#f8f8f8;text-decoration:underline">
&lt;/span>&lt;span style="color:#f8f8f8;text-decoration:underline"> &lt;/span>&lt;span style="color:#204a87;font-weight:bold">name&lt;/span>&lt;span style="color:#000;font-weight:bold">:&lt;/span>&lt;span style="color:#f8f8f8;text-decoration:underline"> &lt;/span>&lt;span style="color:#4e9a06">&amp;#39;swift&amp;#39;&lt;/span>&lt;span style="color:#f8f8f8;text-decoration:underline">
&lt;/span>&lt;span style="color:#f8f8f8;text-decoration:underline"> &lt;/span>&lt;span style="color:#204a87;font-weight:bold">config&lt;/span>&lt;span style="color:#000;font-weight:bold">:&lt;/span>&lt;span style="color:#f8f8f8;text-decoration:underline">
&lt;/span>&lt;span style="color:#f8f8f8;text-decoration:underline"> &lt;/span>&lt;span style="color:#204a87;font-weight:bold">user&lt;/span>&lt;span style="color:#000;font-weight:bold">:&lt;/span>&lt;span style="color:#f8f8f8;text-decoration:underline"> &lt;/span>&lt;span style="color:#4e9a06">&amp;#39;user:password&amp;#39;&lt;/span>&lt;span style="color:#f8f8f8;text-decoration:underline">
&lt;/span>&lt;span style="color:#f8f8f8;text-decoration:underline"> &lt;/span>&lt;span style="color:#204a87;font-weight:bold">auth&lt;/span>&lt;span style="color:#000;font-weight:bold">:&lt;/span>&lt;span style="color:#f8f8f8;text-decoration:underline"> &lt;/span>&lt;span style="color:#4e9a06">&amp;#39;http://swift.example.com:8080/auth/v1.0&amp;#39;&lt;/span>&lt;span style="color:#f8f8f8;text-decoration:underline">
&lt;/span>&lt;span style="color:#f8f8f8;text-decoration:underline"> &lt;/span>&lt;span style="color:#204a87;font-weight:bold">key&lt;/span>&lt;span style="color:#000;font-weight:bold">:&lt;/span>&lt;span style="color:#f8f8f8;text-decoration:underline"> &lt;/span>&lt;span style="color:#4e9a06">&amp;#39;anchore&amp;#39;&lt;/span>&lt;span style="color:#f8f8f8;text-decoration:underline">
&lt;/span>&lt;span style="color:#f8f8f8;text-decoration:underline"> &lt;/span>&lt;span style="color:#204a87;font-weight:bold">container&lt;/span>&lt;span style="color:#000;font-weight:bold">:&lt;/span>&lt;span style="color:#f8f8f8;text-decoration:underline"> &lt;/span>&lt;span style="color:#4e9a06">&amp;#39;anchorearchive&amp;#39;&lt;/span>&lt;span style="color:#f8f8f8;text-decoration:underline">
&lt;/span>&lt;span style="color:#f8f8f8;text-decoration:underline"> &lt;/span>&lt;span style="color:#204a87;font-weight:bold">create_container&lt;/span>&lt;span style="color:#000;font-weight:bold">:&lt;/span>&lt;span style="color:#f8f8f8;text-decoration:underline"> &lt;/span>&lt;span style="color:#204a87;font-weight:bold">True&lt;/span>&lt;span style="color:#f8f8f8;text-decoration:underline">
&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;ul>
&lt;li>The user configuration option should include the colon delimited username and password. eg. &amp;lsquo;admin:secret&amp;rsquo;&lt;/li>
&lt;li>The auth parameter specifies the authentication end point for the Swift Service&lt;/li>
&lt;li>The key parameter specifies the key for the Swift Service&lt;/li>
&lt;/ul>
&lt;h3 id="keystone-v3">Keystone V3&lt;/h3>
&lt;div class="highlight">&lt;pre style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4">&lt;code class="language-YAML" data-lang="YAML">&lt;span style="color:#204a87;font-weight:bold">object_store&lt;/span>&lt;span style="color:#000;font-weight:bold">:&lt;/span>&lt;span style="color:#f8f8f8;text-decoration:underline">
&lt;/span>&lt;span style="color:#f8f8f8;text-decoration:underline"> &lt;/span>&lt;span style="color:#204a87;font-weight:bold">compression&lt;/span>&lt;span style="color:#000;font-weight:bold">:&lt;/span>&lt;span style="color:#f8f8f8;text-decoration:underline">
&lt;/span>&lt;span style="color:#f8f8f8;text-decoration:underline"> &lt;/span>&lt;span style="color:#204a87;font-weight:bold">enabled&lt;/span>&lt;span style="color:#000;font-weight:bold">:&lt;/span>&lt;span style="color:#f8f8f8;text-decoration:underline"> &lt;/span>&lt;span style="color:#204a87;font-weight:bold">True&lt;/span>&lt;span style="color:#f8f8f8;text-decoration:underline">
&lt;/span>&lt;span style="color:#f8f8f8;text-decoration:underline"> &lt;/span>&lt;span style="color:#204a87;font-weight:bold">min_size_kbytes&lt;/span>&lt;span style="color:#000;font-weight:bold">:&lt;/span>&lt;span style="color:#f8f8f8;text-decoration:underline"> &lt;/span>&lt;span style="color:#0000cf;font-weight:bold">100&lt;/span>&lt;span style="color:#f8f8f8;text-decoration:underline">
&lt;/span>&lt;span style="color:#f8f8f8;text-decoration:underline"> &lt;/span>&lt;span style="color:#204a87;font-weight:bold">storage_driver&lt;/span>&lt;span style="color:#000;font-weight:bold">:&lt;/span>&lt;span style="color:#f8f8f8;text-decoration:underline">
&lt;/span>&lt;span style="color:#f8f8f8;text-decoration:underline"> &lt;/span>&lt;span style="color:#204a87;font-weight:bold">name&lt;/span>&lt;span style="color:#000;font-weight:bold">:&lt;/span>&lt;span style="color:#f8f8f8;text-decoration:underline"> &lt;/span>&lt;span style="color:#4e9a06">&amp;#39;swift&amp;#39;&lt;/span>&lt;span style="color:#f8f8f8;text-decoration:underline">
&lt;/span>&lt;span style="color:#f8f8f8;text-decoration:underline"> &lt;/span>&lt;span style="color:#204a87;font-weight:bold">config&lt;/span>&lt;span style="color:#000;font-weight:bold">:&lt;/span>&lt;span style="color:#f8f8f8;text-decoration:underline">
&lt;/span>&lt;span style="color:#f8f8f8;text-decoration:underline"> &lt;/span>&lt;span style="color:#204a87;font-weight:bold">auth_version&lt;/span>&lt;span style="color:#000;font-weight:bold">:&lt;/span>&lt;span style="color:#f8f8f8;text-decoration:underline"> &lt;/span>&lt;span style="color:#4e9a06">&amp;#39;3&amp;#39;&lt;/span>&lt;span style="color:#f8f8f8;text-decoration:underline">
&lt;/span>&lt;span style="color:#f8f8f8;text-decoration:underline"> &lt;/span>&lt;span style="color:#204a87;font-weight:bold">os_username&lt;/span>&lt;span style="color:#000;font-weight:bold">:&lt;/span>&lt;span style="color:#f8f8f8;text-decoration:underline"> &lt;/span>&lt;span style="color:#4e9a06">&amp;#39;myusername&amp;#39;&lt;/span>&lt;span style="color:#f8f8f8;text-decoration:underline">
&lt;/span>&lt;span style="color:#f8f8f8;text-decoration:underline"> &lt;/span>&lt;span style="color:#204a87;font-weight:bold">os_password&lt;/span>&lt;span style="color:#000;font-weight:bold">:&lt;/span>&lt;span style="color:#f8f8f8;text-decoration:underline"> &lt;/span>&lt;span style="color:#4e9a06">&amp;#39;mypassword&amp;#39;&lt;/span>&lt;span style="color:#f8f8f8;text-decoration:underline">
&lt;/span>&lt;span style="color:#f8f8f8;text-decoration:underline"> &lt;/span>&lt;span style="color:#204a87;font-weight:bold">os_project_name&lt;/span>&lt;span style="color:#000;font-weight:bold">:&lt;/span>&lt;span style="color:#f8f8f8;text-decoration:underline"> &lt;/span>&lt;span style="color:#000">myproject&lt;/span>&lt;span style="color:#f8f8f8;text-decoration:underline">
&lt;/span>&lt;span style="color:#f8f8f8;text-decoration:underline"> &lt;/span>&lt;span style="color:#204a87;font-weight:bold">os_project_domain_name&lt;/span>&lt;span style="color:#000;font-weight:bold">:&lt;/span>&lt;span style="color:#f8f8f8;text-decoration:underline"> &lt;/span>&lt;span style="color:#000">example.com&lt;/span>&lt;span style="color:#f8f8f8;text-decoration:underline">
&lt;/span>&lt;span style="color:#f8f8f8;text-decoration:underline"> &lt;/span>&lt;span style="color:#204a87;font-weight:bold">os_auth_url&lt;/span>&lt;span style="color:#000;font-weight:bold">:&lt;/span>&lt;span style="color:#f8f8f8;text-decoration:underline"> &lt;/span>&lt;span style="color:#4e9a06">&amp;#39;foo.example.com:8000/auth/etc&amp;#39;&lt;/span>&lt;span style="color:#f8f8f8;text-decoration:underline">
&lt;/span>&lt;span style="color:#f8f8f8;text-decoration:underline"> &lt;/span>&lt;span style="color:#204a87;font-weight:bold">container&lt;/span>&lt;span style="color:#000;font-weight:bold">:&lt;/span>&lt;span style="color:#f8f8f8;text-decoration:underline"> &lt;/span>&lt;span style="color:#4e9a06">&amp;#39;anchorearchive&amp;#39;&lt;/span>&lt;span style="color:#f8f8f8;text-decoration:underline">
&lt;/span>&lt;span style="color:#f8f8f8;text-decoration:underline"> &lt;/span>&lt;span style="color:#204a87;font-weight:bold">create_container&lt;/span>&lt;span style="color:#000;font-weight:bold">:&lt;/span>&lt;span style="color:#f8f8f8;text-decoration:underline"> &lt;/span>&lt;span style="color:#204a87;font-weight:bold">True&lt;/span>&lt;span style="color:#f8f8f8;text-decoration:underline">
&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;ul>
&lt;li>The auth_version configuration option specified Keystone V3 authentication&lt;/li>
&lt;li>The os_username parameter specifies the username&lt;/li>
&lt;li>The os_password parameter specifies the password&lt;/li>
&lt;li>The os_project_name parameter specifies the OpenStack project name under which the Swift service is configured&lt;/li>
&lt;li>The os_project_domain_name parameter specifies the domain name for the OpenStack project&lt;/li>
&lt;li>The os_auth_url parameter specifies the URL to the OpenStack Keystone service&lt;/li>
&lt;/ul>
&lt;h3 id="keystone-v2">Keystone V2&lt;/h3>
&lt;div class="highlight">&lt;pre style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4">&lt;code class="language-YAML" data-lang="YAML">&lt;span style="color:#204a87;font-weight:bold">object_store&lt;/span>&lt;span style="color:#000;font-weight:bold">:&lt;/span>&lt;span style="color:#f8f8f8;text-decoration:underline">
&lt;/span>&lt;span style="color:#f8f8f8;text-decoration:underline"> &lt;/span>&lt;span style="color:#204a87;font-weight:bold">compression&lt;/span>&lt;span style="color:#000;font-weight:bold">:&lt;/span>&lt;span style="color:#f8f8f8;text-decoration:underline">
&lt;/span>&lt;span style="color:#f8f8f8;text-decoration:underline"> &lt;/span>&lt;span style="color:#204a87;font-weight:bold">enabled&lt;/span>&lt;span style="color:#000;font-weight:bold">:&lt;/span>&lt;span style="color:#f8f8f8;text-decoration:underline"> &lt;/span>&lt;span style="color:#204a87;font-weight:bold">true&lt;/span>&lt;span style="color:#f8f8f8;text-decoration:underline">
&lt;/span>&lt;span style="color:#f8f8f8;text-decoration:underline"> &lt;/span>&lt;span style="color:#204a87;font-weight:bold">min_size_kbyte&lt;/span>&lt;span style="color:#000;font-weight:bold">:&lt;/span>&lt;span style="color:#f8f8f8;text-decoration:underline"> &lt;/span>&lt;span style="color:#0000cf;font-weight:bold">100&lt;/span>&lt;span style="color:#f8f8f8;text-decoration:underline">
&lt;/span>&lt;span style="color:#f8f8f8;text-decoration:underline"> &lt;/span>&lt;span style="color:#204a87;font-weight:bold">storage_driver&lt;/span>&lt;span style="color:#000;font-weight:bold">:&lt;/span>&lt;span style="color:#f8f8f8;text-decoration:underline">
&lt;/span>&lt;span style="color:#f8f8f8;text-decoration:underline"> &lt;/span>&lt;span style="color:#204a87;font-weight:bold">name&lt;/span>&lt;span style="color:#000;font-weight:bold">:&lt;/span>&lt;span style="color:#f8f8f8;text-decoration:underline"> &lt;/span>&lt;span style="color:#4e9a06">&amp;#39;swift&amp;#39;&lt;/span>&lt;span style="color:#f8f8f8;text-decoration:underline">
&lt;/span>&lt;span style="color:#f8f8f8;text-decoration:underline"> &lt;/span>&lt;span style="color:#204a87;font-weight:bold">config&lt;/span>&lt;span style="color:#000;font-weight:bold">:&lt;/span>&lt;span style="color:#f8f8f8;text-decoration:underline">
&lt;/span>&lt;span style="color:#f8f8f8;text-decoration:underline"> &lt;/span>&lt;span style="color:#204a87;font-weight:bold">auth_version&lt;/span>&lt;span style="color:#000;font-weight:bold">:&lt;/span>&lt;span style="color:#f8f8f8;text-decoration:underline"> &lt;/span>&lt;span style="color:#4e9a06">&amp;#39;2&amp;#39;&lt;/span>&lt;span style="color:#f8f8f8;text-decoration:underline">
&lt;/span>&lt;span style="color:#f8f8f8;text-decoration:underline"> &lt;/span>&lt;span style="color:#204a87;font-weight:bold">os_username&lt;/span>&lt;span style="color:#000;font-weight:bold">:&lt;/span>&lt;span style="color:#f8f8f8;text-decoration:underline"> &lt;/span>&lt;span style="color:#4e9a06">&amp;#39;myusername&amp;#39;&lt;/span>&lt;span style="color:#f8f8f8;text-decoration:underline">
&lt;/span>&lt;span style="color:#f8f8f8;text-decoration:underline"> &lt;/span>&lt;span style="color:#204a87;font-weight:bold">os_password&lt;/span>&lt;span style="color:#000;font-weight:bold">:&lt;/span>&lt;span style="color:#f8f8f8;text-decoration:underline"> &lt;/span>&lt;span style="color:#4e9a06">&amp;#39;mypassword&amp;#39;&lt;/span>&lt;span style="color:#f8f8f8;text-decoration:underline">
&lt;/span>&lt;span style="color:#f8f8f8;text-decoration:underline"> &lt;/span>&lt;span style="color:#204a87;font-weight:bold">os_tenant_name&lt;/span>&lt;span style="color:#000;font-weight:bold">:&lt;/span>&lt;span style="color:#f8f8f8;text-decoration:underline"> &lt;/span>&lt;span style="color:#4e9a06">&amp;#39;mytenant&amp;#39;&lt;/span>&lt;span style="color:#f8f8f8;text-decoration:underline">
&lt;/span>&lt;span style="color:#f8f8f8;text-decoration:underline"> &lt;/span>&lt;span style="color:#204a87;font-weight:bold">os_auth_url&lt;/span>&lt;span style="color:#000;font-weight:bold">:&lt;/span>&lt;span style="color:#f8f8f8;text-decoration:underline"> &lt;/span>&lt;span style="color:#4e9a06">&amp;#39;foo.example.com:8000/auth/etc&amp;#39;&lt;/span>&lt;span style="color:#f8f8f8;text-decoration:underline">
&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;ul>
&lt;li>The auth_version configuration option specified Keystone V3 authentication&lt;/li>
&lt;li>The os_username parameter specifies the username&lt;/li>
&lt;li>The os_password parameter specifies the password&lt;/li>
&lt;li>The os_tenant_name parameter specifies the name of the OpenStack tenant under which the Swift service is configured&lt;/li>
&lt;li>The os_auth_url parameter specifies the URL to the OpenStack Keystone service&lt;/li>
&lt;/ul>
&lt;h4 id="note">Note&lt;/h4>
&lt;p>The Anchore Engine archive drivers users the OpenStack Python SwiftClient library. The config section is passed to the SwiftClient library allowing any advanced parameters supported by the Swift library to be configured.&lt;/p></description></item><item><title>Docs: Feed Synchronization</title><link>/docs/usage/cli_usage/feeds/feed_synchronization/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/docs/usage/cli_usage/feeds/feed_synchronization/</guid><description>
&lt;p>When the Anchore Engine runs it will begin to synchronize security feed data from the Anchore feed service.&lt;/p>
&lt;p>CVE data for Linux distributions such as Alpine, CentOS, Debian, Oracle, Red Hat and Ubuntu will be downloaded. The initial sync may take anywhere from 10 to 60 minutes depending on the speed of your network connection.&lt;/p>
&lt;h3 id="checking-feed-status">Checking Feed Status&lt;/h3>
&lt;p>Starting with Anchore Engine version 0.2.0 the status of the feed synchronization can be retrieved through the API and Anchore CLI.&lt;/p>
&lt;pre>&lt;code>anchore-cli system feeds list
Feed Group LastSync RecordCount
vulnerabilities alpine:3.3 2018-04-25T11:51:33.567214Z 457
vulnerabilities alpine:3.4 2018-04-25T11:51:33.976689Z 594
vulnerabilities alpine:3.5 2018-04-25T11:51:24.447436Z 649
vulnerabilities alpine:3.6 2018-04-25T11:51:32.413834Z 632
vulnerabilities alpine:3.7 2018-04-25T11:51:36.313911Z 689
vulnerabilities centos:5 2018-04-25T11:51:22.453408Z 1270
vulnerabilities centos:6 2018-04-25T11:51:22.966213Z 1245
vulnerabilities centos:7 2018-04-25T11:51:35.102044Z 621
vulnerabilities debian:10 2018-04-25T11:51:37.509069Z 16858
vulnerabilities debian:7 2018-04-25T11:51:20.383254Z 20225
vulnerabilities debian:8 2018-04-25T11:51:21.275382Z 19027
vulnerabilities debian:9 2018-04-25T11:51:23.704236Z 17662
vulnerabilities debian:unstable 2018-04-25T11:51:25.831878Z 17859
vulnerabilities ol:5 2018-04-25T11:51:24.931268Z 1213
vulnerabilities ol:6 2018-04-25T11:51:28.358076Z 1276
vulnerabilities ol:7 2018-04-25T11:51:28.733646Z 685
vulnerabilities ubuntu:12.04 2018-04-25T11:51:34.452081Z 14945
vulnerabilities ubuntu:12.10 2018-04-25T11:51:35.517364Z 5652
vulnerabilities ubuntu:13.04 2018-04-25T11:51:35.923466Z 4127
vulnerabilities ubuntu:14.04 2018-04-25T11:51:29.495143Z 15311
vulnerabilities ubuntu:14.10 2018-04-25T11:51:33.162533Z 4456
vulnerabilities ubuntu:15.04 2018-04-25T11:51:30.617371Z 5676
vulnerabilities ubuntu:15.10 2018-04-25T11:51:31.957883Z 6511
vulnerabilities ubuntu:16.04 2018-04-25T11:51:26.467438Z 12288
vulnerabilities ubuntu:16.10 2018-04-25T11:51:27.961046Z 8646
vulnerabilities ubuntu:17.04 2018-04-25T11:51:39.485986Z 9156
vulnerabilities ubuntu:17.10 2018-04-25T11:51:22.047635Z 7169
&lt;/code>&lt;/pre>&lt;p>This command will report list the feeds synchronized by the Anchore engine, last sync time and current record count.&lt;/p>
&lt;p>Note: Time is reported as UTC, not local time.&lt;/p>
&lt;h3 id="manually-initiating-feed-sync">Manually initiating feed sync&lt;/h3>
&lt;p>After the initial sync has completed the engine will run an incremental sync at a user defined period, by default every 4 hours. At any time a feed sync can be initiated through the API or CLI.&lt;/p>
&lt;p>A sync operation can be manually initiated by running the system feeds sync command however this should not be required under normal operation.&lt;/p>
&lt;p>&lt;code>anchore-cli system feeds sync&lt;/code>&lt;/p>
&lt;h3 id="performing-full-resync">Performing full resync&lt;/h3>
&lt;p>The Anchore Engine can be instructed to flush the current feed data and perform a full synchronization.&lt;/p>
&lt;p>Under normal circumstances this operation should not be required since the Anchore Engine performs regular incremental sync.&lt;/p>
&lt;p>This process may take anywhere from 10 to 60 minutes depending on the speed of your network connection.&lt;/p>
&lt;p>&lt;code>anchore-cli system feeds sync --flush&lt;/code>&lt;/p>
&lt;p>The CLI will issue a warning and prompt for confirmation before proceeding with a sync.&lt;/p></description></item><item><title>Docs: Upgrading the Anchore CLI</title><link>/docs/install/anchore_cli/cli_upgrade/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/docs/install/anchore_cli/cli_upgrade/</guid><description>
&lt;p>The Anchore CLI is published as a Python Package that can be installed from source from the Python PyPI package repository on any platform supporting PyPi. Upgrades to the Anchore CLI are performed using the identical method used for installation.&lt;/p>
&lt;p>&lt;code>$ pip install --user --upgrade anchorecli&lt;/code>&lt;/p>
&lt;p>To check if an update is available from the PyPI package repository run the following command:&lt;/p>
&lt;pre>&lt;code>$ pip search anchorecli
anchorecli (0.2.0) - Anchore Service CLI
INSTALLED: 0.1.10
LATEST: 0.2.0
&lt;/code>&lt;/pre>&lt;p>In this example the pip search command shows that we have anchorecli version 0.1.10 installed however the latest available version is 0.2.0.&lt;/p>
&lt;p>&lt;strong>Note:&lt;/strong> Python package names cannot include a dash so while the command name is anchore-cli the PyPi packages is anchorecli.&lt;/p></description></item><item><title>Docs: Viewing Security Vulnerabilities</title><link>/docs/usage/cli_usage/images/viewing_security_vulnerabilities/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/docs/usage/cli_usage/images/viewing_security_vulnerabilities/</guid><description>
&lt;p>The &lt;code>image vuln&lt;/code> command can be used to return a list of vulnerabilities found in the container image.&lt;/p>
&lt;p>&lt;code>anchore-cli image vuln INPUT_IMAGE VULN_TYPE&lt;/code>&lt;/p>
&lt;p>The &lt;code>INPUT_IMAGE&lt;/code> can be specified in one of the following formats:&lt;/p>
&lt;ul>
&lt;li>Image Digest&lt;/li>
&lt;li>Image ID&lt;/li>
&lt;li>registry/repo:tag&lt;/li>
&lt;/ul>
&lt;p>The &lt;code>VULN_TYPE&lt;/code> currently supports:&lt;/p>
&lt;ul>
&lt;li>os: Vulnerabilities against operating system packages (RPM, DPKG, APK, etc.)&lt;/li>
&lt;li>non-os: NPM, GEM, .NET (NuGet), Java Archive (jar, war, ear) and Python PIP CVEs.&lt;/li>
&lt;li>all: Combination report containing both &amp;lsquo;os&amp;rsquo; and &amp;lsquo;non-os&amp;rsquo; vulnerability records.&lt;/li>
&lt;/ul>
&lt;p>The system has been designed to incorporate 3rd party feeds for other vulnerabilites.&lt;/p>
&lt;h3 id="examples">Examples&lt;/h3>
&lt;p>To generate a report of OS package (RPM/DEB/APK) vulnerabilities found in the image including CVE identifier, Vulnerable Package, Severity Level, Vulnerability details and version of fixed package (if available).&lt;/p>
&lt;p>&lt;code>anchore-cli image vuln debian:latest os&lt;/code>&lt;/p>
&lt;p>Currently the following the anchore-engine draws vulnerability data specifically matched to the following OS distros:&lt;/p>
&lt;ul>
&lt;li>Alpine&lt;/li>
&lt;li>CentOS&lt;/li>
&lt;li>Debian&lt;/li>
&lt;li>Oracle Linux&lt;/li>
&lt;li>Red Hat Enterprise Linux&lt;/li>
&lt;li>Red Hat Universal Base Image (UBI)&lt;/li>
&lt;li>Ubuntu&lt;/li>
&lt;li>Amazon Linux 2&lt;/li>
&lt;li>Google Distroless&lt;/li>
&lt;/ul>
&lt;p>To generate a report of language package (NPM/GEM/Java/Python/NuGet) vulnerabilities, the system draws vulnerability data from the NVD data feed, and vulnerability reports can be viewed using the &amp;lsquo;non-os&amp;rsquo; vulnerability type:&lt;/p>
&lt;p>&lt;code>anchore-cli image vuln node:latest non-os&lt;/code>&lt;/p>
&lt;p>To generate a list of all vulnerabilities that can be found, regardless of whether they are against an OS or non-OS package type, the &amp;lsquo;all&amp;rsquo; vulnerability type can be used:&lt;/p>
&lt;p>&lt;code>anchore-cli image vuln node:latest all&lt;/code>&lt;/p>
&lt;p>Finally, for any of the above queries, these commands (and other anchore-cli commands) can be passed the &lt;code>--json&lt;/code> flag to output the data in JSON format:&lt;/p>
&lt;p>&lt;code>anchore-cli --json image vuln node:latest all&lt;/code>&lt;/p>
&lt;h3 id="next-steps">Next Steps&lt;/h3>
&lt;ul>
&lt;li>&lt;a href="/docs/usage/cli_usage/policies/evaluating_images_against_policies/">Evaluate the image&lt;/a> against policies you create.&lt;/li>
&lt;li>Subscribe to receive &lt;a href="/docs/usage/cli_usage/subscriptions/">notifications&lt;/a> when the image is updated, when the policy status changes or when new vulnerabilities are detected.&lt;/li>
&lt;/ul></description></item><item><title>Docs: Whitelists</title><link>/docs/general/concepts/policy/whitelists/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/docs/general/concepts/policy/whitelists/</guid><description>
&lt;p>Whitelists provide a mechanism within a policy bundle to explicitly override a policy-rule match. A whitelist is a named set of exclusion rules that match trigger outputs.&lt;/p>
&lt;p>Example whitelist:&lt;/p>
&lt;div class="highlight">&lt;pre style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4">&lt;code class="language-JSON" data-lang="JSON">&lt;span style="color:#000;font-weight:bold">{&lt;/span>
&lt;span style="color:#204a87;font-weight:bold">&amp;#34;id&amp;#34;&lt;/span>&lt;span style="color:#000;font-weight:bold">:&lt;/span> &lt;span style="color:#4e9a06">&amp;#34;whitelist1&amp;#34;&lt;/span>&lt;span style="color:#000;font-weight:bold">,&lt;/span>
&lt;span style="color:#204a87;font-weight:bold">&amp;#34;name&amp;#34;&lt;/span>&lt;span style="color:#000;font-weight:bold">:&lt;/span> &lt;span style="color:#4e9a06">&amp;#34;My First Whitelist&amp;#34;&lt;/span>&lt;span style="color:#000;font-weight:bold">,&lt;/span>
&lt;span style="color:#204a87;font-weight:bold">&amp;#34;comment&amp;#34;&lt;/span>&lt;span style="color:#000;font-weight:bold">:&lt;/span> &lt;span style="color:#4e9a06">&amp;#34;A whitelist for my first try&amp;#34;&lt;/span>&lt;span style="color:#000;font-weight:bold">,&lt;/span>
&lt;span style="color:#204a87;font-weight:bold">&amp;#34;version&amp;#34;&lt;/span>&lt;span style="color:#000;font-weight:bold">:&lt;/span> &lt;span style="color:#4e9a06">&amp;#34;1_0&amp;#34;&lt;/span>&lt;span style="color:#000;font-weight:bold">,&lt;/span>
&lt;span style="color:#204a87;font-weight:bold">&amp;#34;items&amp;#34;&lt;/span>&lt;span style="color:#000;font-weight:bold">:&lt;/span> &lt;span style="color:#000;font-weight:bold">[&lt;/span>
&lt;span style="color:#000;font-weight:bold">{&lt;/span>
&lt;span style="color:#204a87;font-weight:bold">&amp;#34;gate&amp;#34;&lt;/span>&lt;span style="color:#000;font-weight:bold">:&lt;/span> &lt;span style="color:#4e9a06">&amp;#34;vulnerabilities&amp;#34;&lt;/span>&lt;span style="color:#000;font-weight:bold">,&lt;/span>
&lt;span style="color:#204a87;font-weight:bold">&amp;#34;trigger_id&amp;#34;&lt;/span>&lt;span style="color:#000;font-weight:bold">:&lt;/span> &lt;span style="color:#4e9a06">&amp;#34;CVE-2018-0737+*&amp;#34;&lt;/span>&lt;span style="color:#000;font-weight:bold">,&lt;/span>
&lt;span style="color:#204a87;font-weight:bold">&amp;#34;id&amp;#34;&lt;/span>&lt;span style="color:#000;font-weight:bold">:&lt;/span> &lt;span style="color:#4e9a06">&amp;#34;rule1&amp;#34;&lt;/span>&lt;span style="color:#000;font-weight:bold">,&lt;/span>
&lt;span style="color:#204a87;font-weight:bold">&amp;#34;expires_on&amp;#34;&lt;/span>&lt;span style="color:#000;font-weight:bold">:&lt;/span> &lt;span style="color:#4e9a06">&amp;#34;2019-12-30T12:00:00Z&amp;#34;&lt;/span>
&lt;span style="color:#000;font-weight:bold">}&lt;/span>
&lt;span style="color:#000;font-weight:bold">]&lt;/span>
&lt;span style="color:#000;font-weight:bold">}&lt;/span>
&lt;/code>&lt;/pre>&lt;/div>&lt;p>The components:&lt;/p>
&lt;ul>
&lt;li>Gate: The gate to whitelist matches from (ensures trigger_ids are not matched in the wrong context)&lt;/li>
&lt;li>Trigger Id: The specific trigger result to match and whitelists. This id is gate/trigger specific as each trigger may have its own trigger_id format. We&amp;rsquo;ll use the most common for this example: the CVE trigger ids produced by the vulnerability-&amp;gt;package gate-trigger. The trigger_id specified may include wildcards for partial matches.&lt;/li>
&lt;li>id: an identifier for the rule, must only be unique within the whitelist object itself&lt;/li>
&lt;li>Expires On: (optional) specifies when a particular whitelist item expires. This is a UTC RFC3339 date-time string. If the rule matches, but is expired, the policy engine will NOT whitelist according to that match.&lt;/li>
&lt;/ul>
&lt;p>The whitelist is processed if it is specified in the mapping rule that was matched during bundle evaluation and is applied to the results of the policy evaluation defined in that same mapping rule. If a whitelist item matches a specific policy trigger output, then the action for that output is set to go and the policy evaluation result notes that the trigger output was matched for a whitelist item by associating it with the whitelist id and item id of the match.&lt;/p>
&lt;p>An example of a whitelisted match from a snippet of a policy evaluation result (See Policies for more information on the format of the policy evaluation result). This a single row entry from the result:&lt;/p>
&lt;div class="highlight">&lt;pre style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4">&lt;code class="language-JSON" data-lang="JSON">&lt;span style="color:#000;font-weight:bold">[&lt;/span>
&lt;span style="color:#4e9a06">&amp;#34;0e2811757f931e2259e09784938f0b0990e7889a37d15efbbe63912fa39ff8b0&amp;#34;&lt;/span>&lt;span style="color:#000;font-weight:bold">,&lt;/span>
&lt;span style="color:#4e9a06">&amp;#34;docker.io/node:latest&amp;#34;&lt;/span>&lt;span style="color:#000;font-weight:bold">,&lt;/span>
&lt;span style="color:#4e9a06">&amp;#34;CVE-2018-0737+openssl&amp;#34;&lt;/span>&lt;span style="color:#000;font-weight:bold">,&lt;/span>
&lt;span style="color:#4e9a06">&amp;#34;vulnerabilities&amp;#34;&lt;/span>&lt;span style="color:#000;font-weight:bold">,&lt;/span>
&lt;span style="color:#4e9a06">&amp;#34;package&amp;#34;&lt;/span>&lt;span style="color:#000;font-weight:bold">,&lt;/span>
&lt;span style="color:#4e9a06">&amp;#34;MEDIUM Vulnerability found in os package type (dpkg) - openssl (fixed in: 1.0.1t-1+deb8u9) - (CVE-2018-0737 - https://security-tracker.debian.org/tracker/CVE-2018-0737)&amp;#34;&lt;/span>&lt;span style="color:#000;font-weight:bold">,&lt;/span>
&lt;span style="color:#4e9a06">&amp;#34;go&amp;#34;&lt;/span>&lt;span style="color:#000;font-weight:bold">,&lt;/span>
&lt;span style="color:#000;font-weight:bold">{&lt;/span>
&lt;span style="color:#204a87;font-weight:bold">&amp;#34;matched_rule_id&amp;#34;&lt;/span>&lt;span style="color:#000;font-weight:bold">:&lt;/span> &lt;span style="color:#4e9a06">&amp;#34;rule1&amp;#34;&lt;/span>&lt;span style="color:#000;font-weight:bold">,&lt;/span>
&lt;span style="color:#204a87;font-weight:bold">&amp;#34;whitelist_id&amp;#34;&lt;/span>&lt;span style="color:#000;font-weight:bold">:&lt;/span> &lt;span style="color:#4e9a06">&amp;#34;whitelist1&amp;#34;&lt;/span>&lt;span style="color:#000;font-weight:bold">,&lt;/span>
&lt;span style="color:#204a87;font-weight:bold">&amp;#34;whitelist_name&amp;#34;&lt;/span>&lt;span style="color:#000;font-weight:bold">:&lt;/span> &lt;span style="color:#4e9a06">&amp;#34;My First Whitelist&amp;#34;&lt;/span>
&lt;span style="color:#000;font-weight:bold">},&lt;/span>
&lt;span style="color:#4e9a06">&amp;#34;myfirstpolicy&amp;#34;&lt;/span>
&lt;span style="color:#000;font-weight:bold">]&lt;/span>
&lt;/code>&lt;/pre>&lt;/div>&lt;p>The items in order are:&lt;/p>
&lt;ul>
&lt;li>Image ID&lt;/li>
&lt;li>Tag used for evaluation&lt;/li>
&lt;li>Trigger ID of the policy rule match&lt;/li>
&lt;li>Gate name&lt;/li>
&lt;li>Trigger name&lt;/li>
&lt;li>Trigger Check Output message&lt;/li>
&lt;li>&lt;strong>Whitelist result object&lt;/strong> - This shows that the match was whitelisted by our example whitelist policy and its rule.
Policy Id&lt;/li>
&lt;/ul>
&lt;p>&lt;strong>Note:&lt;/strong> Whitelists are evaluated only as far as necessary. Once a policy rule match has been whitelisted by one whitelist item, it will not be checked again for whitelist matches. But, whitelist items may be evaluated out-of-order for performance optimization, so if multiple whitelist items match the same policy rule match any one of them may be the item that is actually matched against a given trigger_id.&lt;/p></description></item><item><title>Docs: Working with Amazon ECR Registry Credentials</title><link>/docs/usage/cli_usage/registries/ecr_configuration/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/docs/usage/cli_usage/registries/ecr_configuration/</guid><description>
&lt;p>Amazon AWS typically uses keys instead of traditional usernames &amp;amp; passwords. These keys consist of an access key ID and a secret access key. While it is possible to use the aws ecr get-login command to create an access token, this will expire after 12 hours so it is not appropriate for use with Anchore Engine, otherwise a user would need to update their registry credentials regularly. So when adding an Amazon ECR registry to Anchore Engine you should pass the aws_access_key_id and aws_secret_access_key.&lt;/p>
&lt;pre>&lt;code>anchore-cli registry add /
1234567890.dkr.ecr.us-east-1.amazonaws.com /
MY_AWS_ACCESS_KEY_ID /
MY_AWS_SECRET_ACCESS_KEY /
--registry-type=awsecr
&lt;/code>&lt;/pre>&lt;p>The registry-type parameter instructs Anchore Engine to handle these credentials as AWS credentials rather than traditional usernames and passwords. Currently the Anchore Engine supports two types of registry authentication standard username and password for most Docker V2 registries and Amazon ECR. In this example we specified the registry type on the command line however if this parameter is omitted then the CLI will attempt to guess the registry type from the URL which uses a standard format.&lt;/p>
&lt;p>The Anchore Engine will use the AWS access key and secret access keys to generate authentication tokens to access the Amazon ECR registry, the Anchore Engine will manage regeneration of these tokens which typically expire after 12 hours.&lt;/p>
&lt;p>In addition to supporting AWS access key credentials Anchore also supports the use of IAM roles for authenticating with Amazon ECR if the Anchore Engine is run on an EC2 instance.&lt;/p>
&lt;p>In this case you can configure the Anchore Engine to inherit the IAM role from the EC2 instance hosting the engine.&lt;/p>
&lt;p>When launching the EC2 instance that will run the Anchore Engine you need to specify a role that includes the &lt;em>AmazonEC2ContainerRegistryReadOnly&lt;/em> policy.&lt;/p>
&lt;p>While this is best performed using a CloudFormation template, you can manually configure from the launch instance wizard.&lt;/p>
&lt;p>&lt;strong>Step 1:&lt;/strong> Select &lt;em>Create new IAM role.&lt;/em>&lt;/p>
&lt;p>&lt;img src="https://anchore.com/wp-content/uploads/2018/01/pasted-image-0.png" alt="logo">&lt;/p>
&lt;p>&lt;strong>Step 2:&lt;/strong> Under type of trusted entity select &lt;em>EC2&lt;/em>.&lt;/p>
&lt;p>&lt;img src="https://anchore.com/wp-content/uploads/2018/01/pasted-image-0-5.png" alt="logo">&lt;/p>
&lt;p>Ensure that the &lt;em>AmazonEC2ContainerRegistryReadOnly&lt;/em> policy is selected.&lt;/p>
&lt;p>&lt;strong>Step 3:&lt;/strong> Attach Permissions to the Role.&lt;/p>
&lt;p>&lt;img src="https://anchore.com/wp-content/uploads/2018/01/pasted-image-0-2.png" alt="logo">&lt;/p>
&lt;p>&lt;strong>Step 4:&lt;/strong> Name the role.&lt;/p>
&lt;p>Give a name to the role and add this role to the Instance you are launching.&lt;/p>
&lt;p>On the running EC2 instance you can manually verify that the instance has inherited the correct role by running the following command:&lt;/p>
&lt;pre>&lt;code># curl http://169.254.169.254/latest/meta-data/iam/info
{
&amp;quot;Code&amp;quot; : &amp;quot;Success&amp;quot;,
&amp;quot;LastUpdated&amp;quot; : &amp;quot;2018-01-1218:45:12Z&amp;quot;,
&amp;quot;InstanceProfileArn&amp;quot; : &amp;quot;arn:aws:iam::123456789012:instance-profile/ECR-ReadOnly&amp;quot;,
&amp;quot;InstanceProfileId&amp;quot; : &amp;quot;ABCDEFGHIJKLMNOP”
}
&lt;/code>&lt;/pre>&lt;p>&lt;strong>Step 5:&lt;/strong> Enable IAM Authentication in the Anchore Engine.&lt;/p>
&lt;p>By default the support for inheriting the IAM role is disabled.&lt;/p>
&lt;p>To enable IAM based authentication add the following entry to the top of the Anchore Engine config.yaml file:&lt;/p>
&lt;p>&lt;code>allow_awsecr_iam_auto: True&lt;/code>&lt;/p>
&lt;p>&lt;strong>Step 6:&lt;/strong> Add the Registry using the &lt;em>AWSAUTO&lt;/em> user.&lt;/p>
&lt;p>When IAM support is enabled instead of passing the access key and secret access key use “awsauto” for both username and password. This will instruct the Anchore Engine to inherit the role from the underlying EC2 instance.&lt;/p>
&lt;pre>&lt;code>anchore-cli registry add /
1234567890.dkr.ecr.us-east-1.amazonaws.com /
awsauto /
awsauto /
--registry-type=awsecr
&lt;/code>&lt;/pre></description></item><item><title>Docs: Working with Azure Registry Credentials</title><link>/docs/usage/cli_usage/registries/acr_configuration/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/docs/usage/cli_usage/registries/acr_configuration/</guid><description>
&lt;p>To use an Azure Registry, you can configure Anchore to use either the admin credential(s) or a service principal. Refer to Azure documentation for differences and how to setup each. When you&amp;rsquo;ve chosen a credential type, use the following to determine which registry command options correspond to each value for your credential type&lt;/p>
&lt;ul>
&lt;li>
&lt;p>Admin Account&lt;/p>
&lt;ul>
&lt;li>Registry: The login server (Ex. myregistry1.azurecr.io)&lt;/li>
&lt;li>Type: Set to docker_v2&lt;/li>
&lt;li>Username: The username in the &amp;lsquo;az acr credentials show &amp;ndash;name &lt;!-- raw HTML omitted -->&amp;rsquo; output&lt;/li>
&lt;li>Password: The password or password2 value from the &amp;lsquo;az acr credentials show&amp;rsquo; command result&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>
&lt;p>Service Principle&lt;/p>
&lt;ul>
&lt;li>Registry: The login server (Ex. myregistry1.azurecr.io)
Type: Set to docker_v2
Username: The service principal app id
Password: The service principal password&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;p>To add an azure registry credential, invoke anchore-cli as follows:&lt;/p>
&lt;p>&lt;code>anchore-cli registry add --registry-type &amp;lt;Type&amp;gt; &amp;lt;Registry&amp;gt; &amp;lt;Username&amp;gt; &amp;lt;Password&amp;gt;&lt;/code>&lt;/p>
&lt;p>Once a registry has been added, any image that is added (e.g. &lt;code>anchore-cli image add &amp;lt;Registry&amp;gt;/some/repo:sometag&lt;/code>) will use the provided credential to download/inspect and analyze the image.&lt;/p></description></item><item><title>Docs: Working with Google Container Registry (GCR) Credentials</title><link>/docs/usage/cli_usage/registries/gcr_configuration/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/docs/usage/cli_usage/registries/gcr_configuration/</guid><description>
&lt;p>When working with Google Container Registry it is recommended that you use JSON keys rather than the short lived access tokens.&lt;/p>
&lt;p>JSON key files are long-lived and are tightly scoped to individual projects and resources. You can read more about JSON credentials in Google&amp;rsquo;s documentation at the following URL: &lt;a href="https://cloud.google.com/container-registry/docs/advanced-authentication#using_a_json_key_file">Google Container Registry advanced authentication&lt;/a>&lt;/p>
&lt;p>Once a JSON key file has been created with permissions to read from the container registry then the registry should be added with the username &lt;strong>_json_key&lt;/strong> and the password should be the contents of the key file.&lt;/p>
&lt;p>In the following example a file named key.json in the current directory contains the JSON key with readonly access to the my-repo repository within the my-project Google Cloud project.&lt;/p>
&lt;p>&lt;code>anchore-cli registry add us.gcr.io _json_key &amp;quot;$(cat key.json)&amp;quot;&lt;/code>&lt;/p></description></item><item><title>Docs: Adding Custom Certificate Authority</title><link>/docs/install/configuration/custom_certs/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/docs/install/configuration/custom_certs/</guid><description>
&lt;p>If a custom CA certificate is required to access an external resource then the Trust Store in the Anchore Engine container needs to be updated in two places.&lt;/p>
&lt;ol>
&lt;li>The operating system provided trust store.&lt;/li>
&lt;li>The Certifi trust store.&lt;/li>
&lt;/ol>
&lt;p>The operating system trust store is read by the skopeo utility and python requests library that is used to access container registries to read manifests and pull image layers.&lt;/p>
&lt;p>If your container registry users a custom CA then you can update the trust store to trust the certificate or use the &amp;ndash;insecure option when configuring the registry.&lt;/p>
&lt;p>To add a certificate to the operating system trust store the CA certificate should be placed in the /etc location that is appropriate for the container image being used.&lt;/p>
&lt;ul>
&lt;li>For anchore 0.2.X and earlier, the base container is CentOS 7, which stores certs in &lt;code>/etc/pki/ca-trust/source/anchors/&lt;/code> and requires user to run update-ca-trust command as root to update the system certs.&lt;/li>
&lt;li>For anchore 0.3.X, the base container is Ubuntu 18.04, which stores certs in &lt;code>/usr/local/share/ca-certificates/&amp;lt;new_directory&amp;gt;/&amp;lt;new_cert_file&amp;gt;.crt&lt;/code> and requires the user to run update-ca-certificates command as root to update the system certs.&lt;/li>
&lt;li>For anchore 0.4.X and newer, the base container is Red Hat Universal Base Image 7, which stores certs in &lt;code>/etc/pki/ca-trust/source/anchors/&lt;/code> and requires user to run update-ca-trust command as root to update the system certs.&lt;/li>
&lt;li>For anchore 0.7.X and newer, the base container is Red Hat Universal Base Image 8, which stores certs in &lt;code>/etc/pki/ca-trust/source/anchors/&lt;/code> and requires user to run update-ca-trust command as root to update the system certs.&lt;/li>
&lt;/ul>
&lt;p>&lt;a href="https://pypi.org/project/certifi/">Certifi&lt;/a> is a curated list of trusted certificate authorities that is used by the Python requests HTTP client library. The Python requests library is used by Anchore Engine for all HTTP interactions, including when communicating with Anchore Feed service, when webhooks are sent to a TLS enabled endpoint and inbetween Anchore Engine services if TLS has been configured. To update the Certifi trust store the CA certificate should be appended onto the cacert.pem file provided by the Certifi library.&lt;/p>
&lt;ul>
&lt;li>For anchore 0.2.X and earlier, the base container is CentOS 7, certifi&amp;rsquo;s cacert.pem is installed in &lt;code>/usr/lib/python2.7/site-packages/certifi/cacert.pem&lt;/code>&lt;/li>
&lt;li>For anchore 0.3.X, the base container is Ubuntu 18.04, certifi&amp;rsquo;s cacert.pem is installed in &lt;code>/usr/local/lib/python3.6/dist-packages/certifi/cacert.pem&lt;/code>&lt;/li>
&lt;li>For anchore 0.4.X and newer, the base container is Red Hat Universal Base Image 7, certifi&amp;rsquo;s cacert.pem is installed in &lt;code>/opt/rh/rh-python36/root/usr/lib/python3.6/site-packages/certifi/cacert.pem&lt;/code>&lt;/li>
&lt;li>For anchore 0.7.X and newer, the base container is Red Hat Universal Base Image 8, certifi&amp;rsquo;s cacert.pem is installed in &lt;code>/usr/local/lib/python3.6/site-packages/certifi/cacert.pem&lt;/code>&lt;/li>
&lt;li>For anchore 0.9.x and newer, the base container is Red Hat Universal Base Image 8, certifi&amp;rsquo;s cacert.pem is installed in &lt;code>/usr/local/lib/python3.8/site-packages/certifi/cacert.pem&lt;/code>&lt;/li>
&lt;/ul>
&lt;p>The following Dockerfile illustrates an example of how this general process can be automated to produce your own anchore-engine container (based on version 0.2.4 of anchore-engine) with new custom CA cert installed. A similiar process can be used for anchore-engine 0.4.X or newer, with the commands adjusted as appropriate with the differences described above.&lt;/p>
&lt;p>&lt;strong>Dockerfile&lt;/strong>&lt;/p>
&lt;pre>&lt;code>FROM docker.io/anchore/anchore-engine:v0.4.0
USER root:root
COPY ./custom-ca.pem /etc/pki/ca-trust/source/anchors/
RUN update-ca-trust
RUN /usr/bin/cat /etc/pki/ca-trust/source/anchors/custom-ca.pem &amp;gt;&amp;gt; /opt/rh/rh-python36/root/usr/lib/python3.6/site-packages/certifi/cacert.pem
USER anchore:anchore
&lt;/code>&lt;/pre>&lt;p>&lt;strong>Build Custom Image&lt;/strong>&lt;/p>
&lt;pre>&lt;code>$ sudo docker build -t anchore/anchore-engine:v0.4.0custom .
&lt;/code>&lt;/pre></description></item><item><title>Docs: Jenkins</title><link>/docs/usage/integration/ci_cd/jenkins/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/docs/usage/integration/ci_cd/jenkins/</guid><description>
&lt;p>We keep the latest Anchore/Jenkins integration documentation hosted alongside the official Anchore Jenkins plugin module, &lt;a href="https://plugins.jenkins.io/anchore-container-scanner">here&lt;/a>.&lt;/p></description></item><item><title>Docs: Malware Scanning</title><link>/docs/general/concepts/images/analysis/malware_scanning/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/docs/general/concepts/images/analysis/malware_scanning/</guid><description>
&lt;h2 id="overview">Overview&lt;/h2>
&lt;p>Anchore Engine now supports the use of the open-source &lt;a href="https://clamav.net">ClamAV&lt;/a> malware scanner to detect malicious code embedded in container images.
This scan occurs only at analysis time when the image content itself is available, and the scan results are available via the Engine API as well as for consumption
in new policy gates to allow gating of image with malware findings.&lt;/p>
&lt;h2 id="signature-db-updates">Signature DB Updates&lt;/h2>
&lt;p>Each analyzer service will run a malware signature update before analyzing each image. This does add some latency to the overall analysis time but ensures the signatures
are as up-to-date as possible for each image analyzed. The update behavior can be disabled if you prefer to manage the freshness of the db via another route, such as a shared filesystem
mounted to all analyzer nodes that is updated on a schedule. See the configuration section for details on disabling the db update.&lt;/p>
&lt;p>The status of the db update is present in each scan output for each image.&lt;/p>
&lt;h2 id="scan-results">Scan Results&lt;/h2>
&lt;p>The &lt;code>malware&lt;/code> content type is a list of scan results. Each result is the run of a malware scanner, by default &lt;code>clamav&lt;/code>.&lt;/p>
&lt;p>The list of files found to contain malware signature matches is in the &lt;code>findings&lt;/code> property of each scan result. An empty array value indicates no matches found.&lt;/p>
&lt;p>The &lt;code>metadata&lt;/code> property provides generic metadata specific to the scanner. For the ClamAV implementation, this includes the version data about the signature db used and
if the db update was enabled during the scan. If the db update is disabled, then the &lt;code>db_version&lt;/code> property of the metadata will not have values since the only way to get
the version metadata is during a db update.&lt;/p>
&lt;pre>&lt;code>{
&amp;quot;content&amp;quot;: [
{
&amp;quot;findings&amp;quot;: [
{
&amp;quot;path&amp;quot;: &amp;quot;/somebadfile&amp;quot;,
&amp;quot;signature&amp;quot;: &amp;quot;Unix.Trojan.MSShellcode-40&amp;quot;
},
{
&amp;quot;path&amp;quot;: &amp;quot;/somedir/somepath/otherbadfile&amp;quot;,
&amp;quot;signature&amp;quot;: &amp;quot;Unix.Trojan.MSShellcode-40&amp;quot;
}
],
&amp;quot;metadata&amp;quot;: {
&amp;quot;db_update_enabled&amp;quot;: true,
&amp;quot;db_version&amp;quot;: {
&amp;quot;bytecode&amp;quot;: &amp;quot;331&amp;quot;,
&amp;quot;daily&amp;quot;: &amp;quot;25890&amp;quot;,
&amp;quot;main&amp;quot;: &amp;quot;59&amp;quot;
}
},
&amp;quot;scanner&amp;quot;: &amp;quot;clamav&amp;quot;
}
],
&amp;quot;content_type&amp;quot;: &amp;quot;malware&amp;quot;,
&amp;quot;imageDigest&amp;quot;: &amp;quot;sha256:0eb874fcad5414762a2ca5b2496db5291aad7d3b737700d05e45af43bad3ce4d&amp;quot;
}
&lt;/code>&lt;/pre>&lt;h2 id="policy-rules">Policy Rules&lt;/h2>
&lt;p>A policy gate called &lt;code>malware&lt;/code> is available with a &lt;code>scans&lt;/code> trigger that will fire for each file and signature combination found in the image so that you can fail an evaluation of an image
if malware was detected during the analysis scan.&lt;/p>
&lt;p>See &lt;a href="/docs/usage/cli_usage/policies/policy_checks/">policy checks&lt;/a> for more details&lt;/p></description></item><item><title>Docs: GitHub</title><link>/docs/usage/integration/ci_cd/github/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/docs/usage/integration/ci_cd/github/</guid><description>
&lt;p>Image Scanning can be easily integrated into your GitHub Actions pipeline using the official Anchore Engine Scan Action.&lt;/p>
&lt;p>Documentation on how to use the action can be found in the official GitHub repository &lt;a href="https://github.com/anchore/scan-action">anchore scan-action&lt;/a>&lt;/p>
&lt;p>Check out our Anchore Scan Action &lt;a href="https://www.brighttalk.com/webcast/17878/378430?utm_source=brighttalk-portal&amp;amp;utm_medium=web&amp;amp;utm_content=anchore%20github&amp;amp;utm_campaign=webcasts-search-results-feed">webinar demo&lt;/a>&lt;/p></description></item><item><title>Docs: Anchore Engine Inline Image Analysis/Engine Import</title><link>/docs/usage/integration/ci_cd/inline_analysis/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/docs/usage/integration/ci_cd/inline_analysis/</guid><description>
&lt;p>&lt;em>&lt;strong>Warning&lt;/strong>: The Anchore Inline Scan script is deprecated and will reach &lt;strong>EOL on Jan 10, 2022&lt;/strong>. Please update your integrations to use &lt;a href="https://github.com/anchore/grype">Grype&lt;/a> for CI-based vulnerability scanning or &lt;a href="https://github.com/anchore/syft">Syft&lt;/a>. We will be updating our integrations that use inline_scan during that time to use Grype directly. Until that time all integrations will continue to function and get updated vulnerability data.&lt;/em>&lt;/p>
&lt;p>&lt;em>&lt;strong>Until Jan 10, 2022&lt;/strong>: we will continue building inline_scan images based on v0.10.x of Anchore Engine and they will be updated daily for latest feed data.
On Jan 10, 2022, we will stop building new versions of the images with updated vulnerability data and the data will be stale.&lt;/em>&lt;/p>
&lt;p>&lt;em>&lt;strong>After Jan 10, 2022&lt;/strong>: users should be transitioned to &lt;a href="https://github.com/anchore/grype">Grype&lt;/a> or Grype-based integrations.&lt;/em>&lt;/p>
&lt;h2 id="introduction">Introduction&lt;/h2>
&lt;p>For use cases where it is desirable to perform image analysis for a locally build container image, and import the image analysis to an existing Anchore Engine installation, we support a methodology using the inline_scan tool. With this technique, you can &amp;lsquo;add&amp;rsquo; an image to your anchore engine service by analyzing any image that is available locally (say, on the docker host on which the image was built). You can then import the analysis data into anchore engine, rather than the regular method where images are pulled from a registry when added to anchore engine.&lt;/p>
&lt;p>The only requirements to run the inline_scan script with the &amp;lsquo;analyze&amp;rsquo; operation is the ability to execute Docker commands, network connectivity to an anchore engine API endpoint &amp;amp; bash. We host a versioned copy of this script that can be downloaded directly with curl and executed in a bash pipeline.&lt;/p>
&lt;ul>
&lt;li>&lt;em>Note - For the rest of this document, &lt;code>USER&lt;/code>, &lt;code>PASS&lt;/code>, and &lt;code>URL&lt;/code> refer to an anchore engine user, password, and engine endpoint URL (http://&lt;!-- raw HTML omitted -->:&lt;!-- raw HTML omitted -->/v1) respectively.&lt;/em>&lt;/li>
&lt;/ul>
&lt;p>To run the script on your workstation, use the following command syntax.&lt;/p>
&lt;p>&lt;code>curl -s https://ci-tools.anchore.io/inline_scan-latest | bash -s -- analyze -u &amp;lt;USER&amp;gt; -p &amp;lt;PASS&amp;gt; -r &amp;lt;URL&amp;gt; [ OPTIONS ] &amp;lt;FULL_IMAGE_TAG&amp;gt;&lt;/code>&lt;/p>
&lt;h3 id="inline-analysis-options">Inline Analysis Options&lt;/h3>
&lt;pre>&lt;code>-r &amp;lt;TEXT&amp;gt; [required] URL to remote Anchore Engine API endpoint (ex: -r 'https://anchore.example.com:8228/v1')
-u &amp;lt;TEXT&amp;gt; [required] Username for remote Anchore Engine auth (ex: -u 'admin')
-p &amp;lt;TEXT&amp;gt; [required] Password for remote Anchore Engine auth (ex: -p 'foobar')
-a &amp;lt;TEXT&amp;gt; [optional] Add annotations (ex: -a 'key=value,key=value')
-d &amp;lt;PATH&amp;gt; [optional] Specify image digest (ex: -d 'sha256:&amp;lt;64 hex characters&amp;gt;')
-f &amp;lt;PATH&amp;gt; [optional] Path to Dockerfile (ex: -f ./Dockerfile)
-i &amp;lt;TEXT&amp;gt; [optional] Specify image ID used within Anchore Engine (ex: -i '&amp;lt;64 hex characters&amp;gt;')
-m &amp;lt;PATH&amp;gt; [optional] Path to Docker image manifest (ex: -m ./manifest.json)
-t &amp;lt;TEXT&amp;gt; [optional] Specify timeout for image analysis in seconds. Defaults to 300s. (ex: -t 500)
-g [optional] Generate an image digest from docker save tarball
-P [optional] Pull docker image from registry
-V [optional] Increase verbosity
&lt;/code>&lt;/pre>&lt;h3 id="image-identity-selection">Image Identity Selection&lt;/h3>
&lt;p>In order to perform local analysis and import the image correctly into your existing anchore engine deployment, special attention should be paid to the image identifiers (image id, digest, manifest, full tag name) when performing local analysis. Since image digests are generated from an image &amp;lsquo;manifest&amp;rsquo; which is generated by a container registry, this technique requires that you either specify a digest, ask for one to be randomly &amp;lsquo;generated&amp;rsquo;, or supply a valid manifest alongside the image when it is being imported. An image ID can also be supplied if one is available that you would prefer to use. Best practice is to supply these identifiers in whichever way is most appropriate for your use case, resulting in the information being associated with the imported image correctly such that you can refer to it later using these identifiers.&lt;/p>
&lt;h3 id="analyzing-a-local-image">Analyzing a local image&lt;/h3>
&lt;p>For a simple example, we show here how to perform a local docker build, then analyze the image (specifying the Dockerfile) and upload the analysis results to a remote anchore engine installation. The end result is that the image is &amp;lsquo;added&amp;rsquo; to the specified anchore engine service as if it were &amp;lsquo;added&amp;rsquo; using the regular &lt;code>anchore-cli image add&lt;/code> process.&lt;/p>
&lt;p>Note that in this scenario, the image has never been pushed to any container image registry, meaning that this technique is an alternate path for ensuring that images are added to anchore engine for later vulnerability scanning, policy evaluations, and any other functionality of anchore engine that can be performed against analyzed images.&lt;/p>
&lt;pre>&lt;code># docker build -t localbuild/example-image:latest -f Dockerfile .
# curl -s https://ci-tools.anchore.io/inline_scan-latest | bash -s -- analyze -u &amp;lt;USER&amp;gt; -p &amp;lt;PASS&amp;gt; -r &amp;lt;URL&amp;gt; -f ./Dockerfile -g localbuild/example-image:latest
&lt;/code>&lt;/pre>&lt;p>Alternatively, we can specify a digest and image ID, instead of asking the analyzer to generate one randomly:&lt;/p>
&lt;pre>&lt;code># docker build -t localbuild/example-image:latest -f Dockerfile .
# docker images localbuild/example-image:latest --digests --no-trunc
...get the full image ID of the image, in this case 363f10f9...
...get the image digest from the local image if available, in this case sha256:d212a12aa....
# curl -s https://ci-tools.anchore.io/inline_scan-latest | bash -s -- analyze -u &amp;lt;USER&amp;gt; -p &amp;lt;PASS&amp;gt; -r &amp;lt;URL&amp;gt; -f ./Dockerfile -i 363f10f920d05943659ffa0b9ac9a98582bd71841b58c0a50fd596d6285404b2 -d sha256:d212a12aa728ccb4baf06fcc83dc77392d90018d13c9b40717cf455e09aeeef3 localbuild/example-image:latest
# anchore-cli image get localbuild/example-image:latest
Image Digest: sha256:d212a12aa728ccb4baf06fcc83dc77392d90018d13c9b40717cf455e09aeeef3
...
&lt;/code>&lt;/pre>&lt;h4 id="analyzing-an-image-from-a-registry">Analyzing an image from a registry&lt;/h4>
&lt;p>Here we show an example of an image (docker.io/alpine:latest) that is available in a registry (thus the manifest is accessible). We get the manifest from the registry, using skopeo, by first getting the parent digest - which will be used to get the digest of the image that matches your desired architecture- then getting the actual image digest. Finally, we perform analysis/import using the inline_scan tool.&lt;/p>
&lt;pre>&lt;code># skopeo inspect --raw docker://docker.io/alpine:latest &amp;gt; alpine_parent_digest.json
# cat ./alpine_parent_digest.json
...find and note the digest for the image with the architecture we want (sha256:acd3... in this example)
# skopeo inspect --raw docker://docker.io/alpine@sha256:acd3ca9941a85e8ed16515bfc5328e4e2f8c128caa72959a58a127b7801ee01f &amp;gt; alpine_manifest.json
# curl -s https://ci-tools.anchore.io/inline_scan-latest | bash -s -- analyze -u &amp;lt;USER&amp;gt; -p &amp;lt;PASS&amp;gt; -r &amp;lt;URL&amp;gt; -P -m ./alpine_manifest.json docker.io/alpine:latest
# anchore-cli image get docker.io/alpine:latest
Image Digest: sha256:acd3ca9941a85e8ed16515bfc5328e4e2f8c128caa72959a58a127b7801ee01f
&lt;/code>&lt;/pre></description></item><item><title>Docs: Anchore Engine Inline Scanning</title><link>/docs/usage/integration/ci_cd/inline_scanning/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/docs/usage/integration/ci_cd/inline_scanning/</guid><description>
&lt;p>&lt;em>&lt;strong>Warning&lt;/strong>: The Anchore Inline Scan script is deprecated and will reach &lt;strong>EOL on Jan 10, 2022&lt;/strong>. Please update your integrations to use &lt;a href="https://github.com/anchore/grype">Grype&lt;/a> for CI-based vulnerability scanning or &lt;a href="https://github.com/anchore/syft">Syft&lt;/a>. We will be updating our integrations that use inline_scan during that time to use Grype directly. Until that time all integrations will continue to function and get updated vulnerability data.&lt;/em>&lt;/p>
&lt;p>&lt;em>&lt;strong>Until Jan 10, 2022&lt;/strong>: we will continue building inline_scan images based on v0.10.x of Anchore Engine and they will be updated daily for latest feed data.
On Jan 10, 2022, we will stop building new versions of the images with updated vulnerability data and the data will be stale.&lt;/em>&lt;/p>
&lt;p>&lt;em>&lt;strong>After Jan 10, 2022&lt;/strong>: users should be transitioned to &lt;a href="https://github.com/anchore/grype">Grype&lt;/a> or Grype-based integrations.&lt;/em>&lt;/p>
&lt;h2 id="introduction">Introduction&lt;/h2>
&lt;p>&lt;code>curl -s https://ci-tools.anchore.io/inline_scan-latest | bash -s -- -p alpine:latest&lt;/code>&lt;/p>
&lt;p>To make using our inline-scan container as easy as possible, we have provided a simple wrapper script called inline_scan. The only requirements to run the inline_scan script is the ability to execute Docker commands &amp;amp; bash. We host a versioned copy of this script that can be downloaded directly with curl and executed in a bash pipeline.&lt;/p>
&lt;p>To run the script on your workstation, use the following command syntax.&lt;/p>
&lt;p>&lt;code>curl -s https://ci-tools.anchore.io/inline_scan-latest | bash -s -- [options] IMAGE_NAME(s)&lt;/code>&lt;/p>
&lt;h3 id="inline-scan-options">Inline Scan Options&lt;/h3>
&lt;pre>&lt;code>-b &amp;lt;PATH&amp;gt; [optional] Path to local Anchore policy bundle (ex: -b ./policy_bundle.json)
-d &amp;lt;PATH&amp;gt; [optional] Path to local Dockerfile (ex: -d ./dockerfile)
-v &amp;lt;PATH&amp;gt; [optional] Path to directory, all image archives in directory will be scanned (ex: -v /tmp/scan_images/)
-t &amp;lt;TEXT&amp;gt; [optional] Specify timeout for image scanning in seconds. Defaults to 300s. (ex: -t 500)
-f [optional] Exit script upon failed Anchore policy evaluation
-p [optional] Pull remote docker images
-r [optional] Generate analysis reports in your current working directory
-V [optional] Increase verbosity
&lt;/code>&lt;/pre>&lt;h3 id="usage">Usage&lt;/h3>
&lt;p>Pull multiple images from DockerHub, scan them all and generate individual reports in ./anchore-reports.&lt;/p>
&lt;p>&lt;code>curl -s https://ci-tools.anchore.io/inline_scan-latest | bash -s -- -p -r alpine:latest ubuntu:latest centos:latest&lt;/code>&lt;/p>
&lt;p>Perform a local docker build, then pass the Dockerfile to anchore inline scan. Use a custom policy bundle to ensure Dockerfile compliance, failing the script if anchore policy evaluation does not pass.&lt;/p>
&lt;pre>&lt;code>docker build -t example-image:latest -f Dockerfile .
curl -s https://ci-tools.anchore.io/inline_scan-latest | bash -s -- -f -d Dockerfile -b .anchore-policy.json example-image:latest
&lt;/code>&lt;/pre>&lt;p>Save multiple docker image archives to a directory, then mount the entire directory for analysis using a timeout of 500s.&lt;/p>
&lt;pre>&lt;code>cd example1/
docker build -t example1:latest .
cd ../example2
docker build -t example2:latest .
cd ..
mkdir images/
docker save example1:latest -o images/example1+latest.tar
docker save example2:latest -o images/example2+latest.tar
curl -s https://ci-tools.anchore.io/inline_scan-latest | bash -s -- -v ./images -t 500
&lt;/code>&lt;/pre>&lt;h3 id="ci-implementations">CI Implementations&lt;/h3>
&lt;p>All of the following examples can be found in this Github repository - &lt;a href="https://github.com/Btodhunter/ci-demos">https://github.com/Btodhunter/ci-demos&lt;/a>&lt;/p>
&lt;h4 id="circleci">CircleCI&lt;/h4>
&lt;p>This workflow requires the $DOCKER_USER &amp;amp; $DOCKER_PASS environment variables to be set in a context called dockerhub in your CircleCI account settings at settings -&amp;gt; context -&amp;gt; create&lt;/p>
&lt;p>config.yml - &lt;a href="https://github.com/Btodhunter/ci-demos/blob/master/.circleci/config.yml">https://github.com/Btodhunter/ci-demos/blob/master/.circleci/config.yml&lt;/a>&lt;/p>
&lt;pre>&lt;code>version: 2.1
jobs:
build_scan_image:
docker:
- image: docker:stable
environment:
IMAGE_NAME: btodhunter/anchore-ci-demo
IMAGE_TAG: circleci
steps:
- checkout
- setup_remote_docker
- run:
name: Build image
command: docker build -t &amp;quot;${IMAGE_NAME}:ci&amp;quot; .
- run:
name: Scan image
command: |
apk add curl bash
curl -s https://ci-tools.anchore.io/inline_scan-latest | bash -s -- -r &amp;quot;${IMAGE_NAME}:ci&amp;quot;
- run:
name: Push to DockerHub
command: |
echo &amp;quot;$DOCKER_PASS&amp;quot; | docker login -u &amp;quot;$DOCKER_USER&amp;quot; --password-stdin
docker tag &amp;quot;${IMAGE_NAME}:ci&amp;quot; &amp;quot;${IMAGE_NAME}:${IMAGE_TAG}&amp;quot;
docker push &amp;quot;${IMAGE_NAME}:${IMAGE_TAG}&amp;quot;
- store_artifacts:
path: anchore-reports/
workflows:
scan_image:
jobs:
- build_scan_image:
context: dockerhub
&lt;/code>&lt;/pre>&lt;h4 id="gitlab">GitLab&lt;/h4>
&lt;p>GitLab allows docker command execution through a docker:dind service container. This job pushes the image to the GitLab registry, using built-in environment variables for specifying the image name and registry login credentials.&lt;/p>
&lt;p>.gitlab-ci.yml - &lt;a href="https://github.com/Btodhunter/ci-demos/blob/master/.gitlab-ci.yml">https://github.com/Btodhunter/ci-demos/blob/master/.gitlab-ci.yml&lt;/a>&lt;/p>
&lt;pre>&lt;code>variables:
IMAGE_NAME: ${CI_REGISTRY_IMAGE}/build:${CI_COMMIT_REF_SLUG}-${CI_COMMIT_SHA}
stages:
- build
container_build:
stage: build
image: docker:stable
services:
- docker:stable-dind
variables:
DOCKER_DRIVER: overlay2
script:
- echo &amp;quot;$CI_JOB_TOKEN&amp;quot; | docker login -u gitlab-ci-token --password-stdin &amp;quot;${CI_REGISTRY}&amp;quot;
- docker build -t &amp;quot;$IMAGE_NAME&amp;quot; .
- apk add bash curl
- curl -s https://ci-tools.anchore.io/inline_scan-latest | bash -s -- -r -t 500 &amp;quot;$IMAGE_NAME&amp;quot;
- docker push &amp;quot;$IMAGE_NAME&amp;quot;
artifacts:
name: ${CI_JOB_NAME}-${CI_COMMIT_REF_NAME}
paths:
- anchore-reports/*
&lt;/code>&lt;/pre>&lt;h4 id="codeship">CodeShip&lt;/h4>
&lt;p>This job requires creating an encrypted environment variable file for loading the $DOCKER_USER &amp;amp; $DOCKER_PASS variables into your job. See - &lt;a href="https://documentation.codeship.com/pro/builds-and-configuration/environment-variables/#encrypted-environment-variables">https://documentation.codeship.com/pro/builds-and-configuration/environment-variables/#encrypted-environment-variables&lt;/a>&lt;/p>
&lt;p>codeship-services.yml - &lt;a href="https://github.com/Btodhunter/ci-demos/blob/master/codeship-services.yml">https://github.com/Btodhunter/ci-demos/blob/master/codeship-services.yml&lt;/a>&lt;/p>
&lt;pre>&lt;code>anchore:
add_docker: true
image: docker:stable-git
environment:
IMAGE_NAME: btodhunter/anchore-ci-demo
IMAGE_TAG: codeship
encrypted_env_file: env.encrypted
&lt;/code>&lt;/pre>&lt;p>codeship-steps.yml - &lt;a href="https://github.com/Btodhunter/ci-demos/blob/master/codeship-steps.yml">https://github.com/Btodhunter/ci-demos/blob/master/codeship-steps.yml&lt;/a>&lt;/p>
&lt;pre>&lt;code>- name: build-scan
service: anchore
command: sh -c 'apk add bash curl &amp;amp;&amp;amp;
mkdir -p /build &amp;amp;&amp;amp;
cd /build &amp;amp;&amp;amp;
git clone https://github.com/Btodhunter/ci-demos.git . &amp;amp;&amp;amp;
docker build -t &amp;quot;${IMAGE_NAME}:ci&amp;quot; . &amp;amp;&amp;amp;
curl -s https://ci-tools.anchore.io/inline_scan-latest | bash -s -- -f -b .anchore_policy.json &amp;quot;${IMAGE_NAME}:ci&amp;quot; &amp;amp;&amp;amp;
echo &amp;quot;$DOCKER_PASS&amp;quot; | docker login -u &amp;quot;$DOCKER_USER&amp;quot; --password-stdin &amp;amp;&amp;amp;
docker tag &amp;quot;${IMAGE_NAME}:ci&amp;quot; &amp;quot;${IMAGE_NAME}:${IMAGE_TAG}&amp;quot; &amp;amp;&amp;amp;
docker push &amp;quot;${IMAGE_NAME}:${IMAGE_TAG}&amp;quot;'
&lt;/code>&lt;/pre>&lt;h4 id="jenkins-pipeline">Jenkins pipeline&lt;/h4>
&lt;p>To allow pushing to a private registry, the dockerhub-creds credentials must be created in the Jenkins server settings at - Jenkins -&amp;gt; Credentials -&amp;gt; System -&amp;gt; Global credentials -&amp;gt; Add Credentials&lt;/p>
&lt;p>This example was tested against the Jenkins installation detailed here, using the declarative pipeline syntax - &lt;a href="https://jenkins.io/doc/tutorials/build-a-multibranch-pipeline-project/#run-jenkins-in-docker">https://jenkins.io/doc/tutorials/build-a-multibranch-pipeline-project/#run-jenkins-in-docker&lt;/a>&lt;/p>
&lt;p>Jenkinsfile - &lt;a href="https://github.com/Btodhunter/ci-demos/blob/master/Jenkinsfile">https://github.com/Btodhunter/ci-demos/blob/master/Jenkinsfile&lt;/a>&lt;/p>
&lt;pre>&lt;code>pipeline{
agent {
docker {
image 'docker:stable'
}
}
environment {
IMAGE_NAME = 'btodhunter/anchore-ci-demo'
IMAGE_TAG = 'jenkins'
}
stages {
stage('Build Image') {
steps {
sh 'docker build -t ${IMAGE_NAME}:ci .'
}
}
stage('Scan') {
steps {
sh 'apk add bash curl'
sh 'curl -s https://ci-tools.anchore.io/inline_scan-latest | bash -s -- -d Dockerfile -b .anchore_policy.json ${IMAGE_NAME}:ci'
}
}
stage('Push Image') {
steps {
withDockerRegistry([credentialsId: &amp;quot;dockerhub-creds&amp;quot;, url: &amp;quot;&amp;quot;]){
sh 'docker tag ${IMAGE_NAME}:ci ${IMAGE_NAME}:${IMAGE_TAG}'
sh 'docker push ${IMAGE_NAME}:${IMAGE_TAG}'
}
}
}
}
}
&lt;/code>&lt;/pre>&lt;h4 id="travisci">TravisCI&lt;/h4>
&lt;p>The $DOCKER_USER &amp;amp; $DOCKER_PASS environment variables must be setup in the TravisCI console at repository -&amp;gt; settings -&amp;gt; environment variables&lt;/p>
&lt;p>.travis.yml - &lt;a href="https://github.com/Btodhunter/ci-demos/blob/master/.travis.yml">https://github.com/Btodhunter/ci-demos/blob/master/.travis.yml&lt;/a>&lt;/p>
&lt;pre>&lt;code>language: node_js
services:
- docker
env:
- IMAGE_NAME=&amp;quot;btodhunter/anchore-ci-demo&amp;quot; IMAGE_TAG=&amp;quot;travisci&amp;quot;
script:
- docker build -t &amp;quot;${IMAGE_NAME}:ci&amp;quot; .
- curl -s https://ci-tools.anchore.io/inline_scan-latest | bash -s -- &amp;quot;${IMAGE_NAME}:ci&amp;quot;
- echo &amp;quot;$DOCKER_PASS&amp;quot; | docker login -u &amp;quot;$DOCKER_USER&amp;quot; --password-stdin
- docker tag &amp;quot;${IMAGE_NAME}:ci&amp;quot; &amp;quot;${IMAGE_NAME}:${IMAGE_TAG}&amp;quot;
- docker push &amp;quot;${IMAGE_NAME}:${IMAGE_TAG}&amp;quot;
&lt;/code>&lt;/pre>&lt;h4 id="aws-codebuild">AWS CodeBuild&lt;/h4>
&lt;p>The $DOCKER_USER, $DOCKER_PASS, $IMAGE_NAME, &amp;amp; $IMAGE_TAG environment variables must be set in the CodeBuild console at Build Projects -&amp;gt; &amp;lt;PROJECT_NAME&amp;gt; -&amp;gt; Edit Environment -&amp;gt; Additional Config -&amp;gt; Environment Variables&lt;/p>
&lt;p>buildspec.yml - &lt;a href="https://github.com/Btodhunter/ci-demos/blob/master/buildspec.yml">https://github.com/Btodhunter/ci-demos/blob/master/buildspec.yml&lt;/a>&lt;/p>
&lt;pre>&lt;code>version: 0.2
phases:
build:
commands:
- docker build -t ${IMAGE_NAME}:${IMAGE_TAG} .
post_build:
commands:
- curl -s https://ci-tools.anchore.io/inline_scan-latest | bash -s -- ${IMAGE_NAME}:${IMAGE_TAG}
- echo $DOCKER_PASS | docker login -u $DOCKER_USER --password-stdin
- docker push ${IMAGE_NAME}:${IMAGE_TAG}
&lt;/code>&lt;/pre></description></item><item><title>Docs: Configuring TLS / SSL</title><link>/docs/install/configuration/tls_ssl_config/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/docs/install/configuration/tls_ssl_config/</guid><description>
&lt;p>Communication with the Anchore Engine and between Anchore Engine service can be secured with TLS/SSL. This can be performed in two ways&lt;/p>
&lt;ul>
&lt;li>Externally through a load balancing, ingress controller or reverse proxy such as NGINX&lt;/li>
&lt;li>Natively within the Anchore Engine&lt;/li>
&lt;/ul>
&lt;p>For most use cases an external service such as a proxy or load balancer will provide the simplest approach, especially when keys need to be rotated, new instances added, etc.&lt;/p>
&lt;p>The Anchore Engine is comprised of 6 services, typically only the external API service (apiext) and Kubernetes webhook service are published externally, all other services are used for internal communication.&lt;/p>
&lt;p>Transport Level Security (TLS/SSL) is enabled on a per-service basis. In the sample configuration the SSL/TLS configuration options are commented out.&lt;/p>
&lt;p>In the following example the external API service is configured to listen on port 443 and is configured with a certificate for its external hostname anchore.example.com&lt;/p>
&lt;p>Each service published in the Anchore Engine configuration (apiext, catalog, simplequeue, analyzer, policy_engine and kubernetes_webhook) can be configured to use transport level security.&lt;/p>
&lt;div class="highlight">&lt;pre style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4">&lt;code class="language-YAML" data-lang="YAML">&lt;span style="color:#204a87;font-weight:bold">services&lt;/span>&lt;span style="color:#000;font-weight:bold">:&lt;/span>&lt;span style="color:#f8f8f8;text-decoration:underline">
&lt;/span>&lt;span style="color:#f8f8f8;text-decoration:underline"> &lt;/span>&lt;span style="color:#204a87;font-weight:bold">apiext&lt;/span>&lt;span style="color:#000;font-weight:bold">:&lt;/span>&lt;span style="color:#f8f8f8;text-decoration:underline">
&lt;/span>&lt;span style="color:#f8f8f8;text-decoration:underline"> &lt;/span>&lt;span style="color:#204a87;font-weight:bold">enabled&lt;/span>&lt;span style="color:#000;font-weight:bold">:&lt;/span>&lt;span style="color:#f8f8f8;text-decoration:underline"> &lt;/span>&lt;span style="color:#204a87;font-weight:bold">True&lt;/span>&lt;span style="color:#f8f8f8;text-decoration:underline">
&lt;/span>&lt;span style="color:#f8f8f8;text-decoration:underline"> &lt;/span>&lt;span style="color:#204a87;font-weight:bold">endpoint_hostname&lt;/span>&lt;span style="color:#000;font-weight:bold">:&lt;/span>&lt;span style="color:#f8f8f8;text-decoration:underline"> &lt;/span>&lt;span style="color:#4e9a06">&amp;#39;anchore.example.com&amp;#39;&lt;/span>&lt;span style="color:#f8f8f8;text-decoration:underline">
&lt;/span>&lt;span style="color:#f8f8f8;text-decoration:underline"> &lt;/span>&lt;span style="color:#204a87;font-weight:bold">listen&lt;/span>&lt;span style="color:#000;font-weight:bold">:&lt;/span>&lt;span style="color:#f8f8f8;text-decoration:underline"> &lt;/span>&lt;span style="color:#4e9a06">&amp;#39;0.0.0.0&amp;#39;&lt;/span>&lt;span style="color:#f8f8f8;text-decoration:underline">
&lt;/span>&lt;span style="color:#f8f8f8;text-decoration:underline"> &lt;/span>&lt;span style="color:#204a87;font-weight:bold">port&lt;/span>&lt;span style="color:#000;font-weight:bold">:&lt;/span>&lt;span style="color:#f8f8f8;text-decoration:underline"> &lt;/span>&lt;span style="color:#0000cf;font-weight:bold">443&lt;/span>&lt;span style="color:#f8f8f8;text-decoration:underline">
&lt;/span>&lt;span style="color:#f8f8f8;text-decoration:underline"> &lt;/span>&lt;span style="color:#204a87;font-weight:bold">ssl_enable&lt;/span>&lt;span style="color:#000;font-weight:bold">:&lt;/span>&lt;span style="color:#f8f8f8;text-decoration:underline"> &lt;/span>&lt;span style="color:#204a87;font-weight:bold">True&lt;/span>&lt;span style="color:#f8f8f8;text-decoration:underline">
&lt;/span>&lt;span style="color:#f8f8f8;text-decoration:underline"> &lt;/span>&lt;span style="color:#204a87;font-weight:bold">ssl_cert&lt;/span>&lt;span style="color:#000;font-weight:bold">:&lt;/span>&lt;span style="color:#f8f8f8;text-decoration:underline"> &lt;/span>&lt;span style="color:#4e9a06">&amp;#39;/config/anchore-ex.crt&amp;#39;&lt;/span>&lt;span style="color:#f8f8f8;text-decoration:underline">
&lt;/span>&lt;span style="color:#f8f8f8;text-decoration:underline"> &lt;/span>&lt;span style="color:#204a87;font-weight:bold">ssl_key&lt;/span>&lt;span style="color:#000;font-weight:bold">:&lt;/span>&lt;span style="color:#f8f8f8;text-decoration:underline"> &lt;/span>&lt;span style="color:#4e9a06">&amp;#39;/config/anchore-ex.key&amp;#39;&lt;/span>&lt;span style="color:#f8f8f8;text-decoration:underline">
&lt;/span>&lt;span style="color:#f8f8f8;text-decoration:underline"> &lt;/span>&lt;span style="color:#204a87;font-weight:bold">ssl_chain&lt;/span>&lt;span style="color:#000;font-weight:bold">:&lt;/span>&lt;span style="color:#f8f8f8;text-decoration:underline"> &lt;/span>&lt;span style="color:#4e9a06">&amp;#39;/config/anchore-ex.crt&amp;#39;&lt;/span>&lt;span style="color:#f8f8f8;text-decoration:underline">
&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;table>
&lt;thead>
&lt;tr>
&lt;th style="text-align:left">Setting&lt;/th>
&lt;th style="text-align:left">Notes&lt;/th>
&lt;/tr>
&lt;/thead>
&lt;tbody>
&lt;tr>
&lt;td style="text-align:left">enabled&lt;/td>
&lt;td style="text-align:left">If the service is enabled&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td style="text-align:left">endpoint_hostname&lt;/td>
&lt;td style="text-align:left">DNS name of service&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td style="text-align:left">listen&lt;/td>
&lt;td style="text-align:left">IP address of interface on which the service should listen (use &amp;lsquo;0.0.0.0&amp;rsquo; for all - default)&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td style="text-align:left">port&lt;/td>
&lt;td style="text-align:left">Port on which service should listen.&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td style="text-align:left">ssl_enable&lt;/td>
&lt;td style="text-align:left">Enable transport level security&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td style="text-align:left">ssl_cert&lt;/td>
&lt;td style="text-align:left">name, including full path of private key file.&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td style="text-align:left">ssl_chain&lt;/td>
&lt;td style="text-align:left">[optional] name, including full path of certificate chain&lt;/td>
&lt;/tr>
&lt;/tbody>
&lt;/table>
&lt;p>The certificate files should be placed on a path accessible to the Anchore Engine service, for example in the /config directory which is typically mapped as a volume into the container. Note that the location outside the container will depend on your configuration - for example if you are volume mounting &amp;lsquo;/path/to/aevolume/config/&amp;rsquo; on the docker host to &amp;lsquo;/config&amp;rsquo; within the container, you&amp;rsquo;ll need to place the ssl files in &amp;lsquo;/path/to/aevolume/config/&amp;rsquo; on the docker host, so that they are accessible in &amp;lsquo;/config/&amp;rsquo; inside the container, before starting the service.&lt;/p>
&lt;p>The ssl_chain file is optional and may be required by some certificate authorities. If your certificate authority provides a chain certificate then include it within the configuration.&lt;/p>
&lt;p>&lt;strong>Note:&lt;/strong> While a certificate may be purchased from a well-known and trusted certificate authority in some cases the certificate is signed by an intermediate certificate which is not included within a TLS/SSL clients trust stores. In these cases the intermediate certificate is signed by the certificate authority however without the full &amp;lsquo;chain&amp;rsquo; showing the provenance and integrity of the certificate the TLS/SSL client may not trust the certificate.&lt;/p>
&lt;p>Any certificates used by the Anchore Engine services need to be trusted by all other Anchore Engine services.&lt;/p>
&lt;p>If an internal certificate authority is used the root certificate for the internal CA can be added to the Anchore engine using the following procedure or SSL verification can be disabled by setting the following parameter:&lt;/p>
&lt;p>&lt;code>internal_ssl_verify: True&lt;/code>&lt;/p></description></item><item><title>Docs: Configuring User Credential Storage</title><link>/docs/install/configuration/user_credential_storage/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/docs/install/configuration/user_credential_storage/</guid><description>
&lt;h2 id="overview">Overview&lt;/h2>
&lt;p>When using the Anchore internal DB to manage user identities (external management is optional in the Enterprise version), all user information is stored in
the Anchore DB. The credentials can be stored plaintext in the DB, which allows efficient usage internally for dev/test systems, or the credentials can be
stored in hashed form using the Argon2 hashing algorithm.&lt;/p>
&lt;p>Hashed passwords are much more secure, but are expensive to compare and cannot be used for internal service communication since they cannot be reversed. Anchore
provides a token based authentication mechanism as well (a simplified Password-Grant flow of Oauth2) to mitigate the performance issue, but it requires that
all Anchore services be deployed with a shared secret in the configuration or a public/private keypair common to all services.&lt;/p>
&lt;h2 id="passwords">Passwords&lt;/h2>
&lt;p>The configuration of how passwords are stored is set in the &lt;code>user_authentication&lt;/code> section of the &lt;em>config.yaml&lt;/em> file and &lt;em>must&lt;/em> be consistent across all components of an Anchore Engine deployment. Mismatch
in this configuration between components of the system will result in the system not being able to communicate internally.&lt;/p>
&lt;pre>&lt;code>user_authentication:
hashed_passwords: true|false
&lt;/code>&lt;/pre>&lt;p>By default, &lt;code>hashed_passwords&lt;/code> is set to &lt;code>false&lt;/code>. This supports upgrade from previous versions of Anchore as well as usage for installations without a shared key or public/private keys for Anchore. When oauth
is not configured in the system, Anchore must be able to use HTTP Basic authentication between internal services and thus requires credentials that can be read.&lt;/p>
&lt;h2 id="bearer-tokensoauth2">Bearer Tokens/OAuth2&lt;/h2>
&lt;p>If Anchore is configured to support bearer tokens, the tokens are generated and returned to the user but never persisted in the database. All tokens expire, and currently
Anchore does not support refresh tokens, upon expiration a user must re-authenticate with the username and password to get a new token. Users must still have password credentials, however.
Password persistence and protection configuration still applies as in the Password section above.&lt;/p>
&lt;h2 id="configuring-hashed-passwords-and-oauth">Configuring Hashed Passwords and OAuth&lt;/h2>
&lt;p>NOTE: password storage configuration must be done at the time of deployment, it cannot be modified at runtime or after installation with an existing DB since
it will invalidate all existing credentials, including internal system credentials and the system will not be functional. You must choose the mechanism
at system deployment time.&lt;/p>
&lt;p>Set in &lt;em>config.yaml&lt;/em> for all components of the deployment:&lt;/p>
&lt;p>Option 1: Use a shared secret for signing/verifying oauth tokens&lt;/p>
&lt;pre>&lt;code>user_authentication:
oauth:
enabled: true
hashed_passwords: true
keys:
secret: mysecretvalue
&lt;/code>&lt;/pre>
&lt;p>Option 2: Use a public/private key pair, delivered as pem files on the filesystem of the containers anchore runs in:&lt;/p>
&lt;pre>&lt;code>user_authentication:
oauth:
enabled: true
hashed_passwords: true
keys:
private_key_path: &amp;lt;path to private key pem file&amp;gt;
public_key_path: &amp;lt;path to public key pem file&amp;gt;
&lt;/code>&lt;/pre>
&lt;p>Using environment variables with the &lt;em>config.yaml&lt;/em> bundled into the Anchore provided anchore-engine image is also an option.
NOTE: These are &lt;em>only&lt;/em> valid when using the &lt;em>config.yaml&lt;/em> provided in the image due to that file referencing them explicitly as replacement values.&lt;/p>
&lt;pre>&lt;code>ANCHORE_AUTH_SECRET = the string to use as a secret
ANCHORE_AUTH_PUBKEY = path to public key file
ANCHORE_AUTH_PRIVKEY = path to the private key file
ANCHORE_OAUTH_ENABLED = boolean to enable/disable oauth support
ANCHORE_OAUTH_TOKEN_EXPIRATION = the integer value to set number of seconds a token should be valid (default is 3600/1 hr)
ANCHORE_AUTH_ENABLE_HASHED_PASSWORDS = boolean to enable/disable hashed password storage in the anchore db instead of clear text
&lt;/code>&lt;/pre></description></item><item><title>Docs: Gitlab</title><link>/docs/usage/integration/ci_cd/gitlab/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/docs/usage/integration/ci_cd/gitlab/</guid><description>
&lt;h3 id="adding-anchore-scanning-to-gitlab">Adding Anchore Scanning to Gitlab&lt;/h3>
&lt;p>The &amp;lsquo;on premises&amp;rsquo; solution requires a functional installation of Anchore Engine running on a system that is accessible from your GitLab runners.&lt;/p>
&lt;h4 id="on-premises-solution">On Premises Solution:&lt;/h4>
&lt;p>This sample job can run a Gitlab Runner including shared runners on Gitlab.com.
The Docker executor is not required and no special privileges are required for scanning.&lt;/p>
&lt;p>The runner will require network access to two end points:&lt;/p>
&lt;p>Registry that contains the anchore/anchore-cli:latest
By default that is hosted on DockerHub however the image can be pushed to any registry&lt;/p>
&lt;p>Network access to communicate to an Anchore Engine service. Typically on port 8228&lt;/p>
&lt;p>A running Anchore Engine is required, this does not need to be run within the Gitlab infrastructure as long as the HTTP(s) endpoint of the Anchore Engine is accessible by the Github runner.&lt;/p>
&lt;p>If the Anchore Engine will require credentials to pull the image to be analyzed from a Docker registry then the credentials should be added to Anchore Engine using the following procedures.&lt;/p>
&lt;p>An example job is shown below and is attached at the bottom of this page named anchore-on-prem-gitlab.txt&lt;/p>
&lt;pre>&lt;code>anchore_scan:
image: anchore/engine-cli:latest
variables:
ANCHORE_CLI_URL: &amp;quot;http://anchore.example.com:8228/v1&amp;quot;
ANCHORE_CLI_USER: &amp;quot;admin&amp;quot;
ANCHORE_CLI_PASS: &amp;quot;foobar&amp;quot;
ANCHORE_CLI_SSL_VERIFY: &amp;quot;false&amp;quot;
ANCHORE_SCAN_IMAGE: docker.io/library/debian
ANCHORE_TIMEOUT: 300
ANCHORE_FAIL_ON_POLICY: &amp;quot;false&amp;quot;
script:
- echo &amp;quot;Adding image to Anchore engine at ${ANCHORE_CLI_URL}&amp;quot;
- anchore-cli image add ${ANCHORE_SCAN_IMAGE}
- echo &amp;quot;Waiting for analysis to complete&amp;quot;
- anchore-cli image wait ${ANCHORE_SCAN_IMAGE} --timeout ${ANCHORE_TIMEOUT}
- echo &amp;quot;Analysis complete&amp;quot;
- echo &amp;quot;Producing reports&amp;quot;
- anchore-cli --json image content ${ANCHORE_SCAN_IMAGE} os &amp;gt; image-packages.json
- anchore-cli --json image content ${ANCHORE_SCAN_IMAGE} npm &amp;gt; image-npm.json
- anchore-cli --json image content ${ANCHORE_SCAN_IMAGE} gem &amp;gt; image-gem.json
- anchore-cli --json image content ${ANCHORE_SCAN_IMAGE} python &amp;gt; image-python.json
- anchore-cli --json image content ${ANCHORE_SCAN_IMAGE} java &amp;gt; image-java.json
- anchore-cli --json image content ${ANCHORE_SCAN_IMAGE} nuget &amp;gt; image-nuget.json
- anchore-cli --json image vuln ${ANCHORE_SCAN_IMAGE} all &amp;gt; image-vulnerabilities.json
- anchore-cli --json image get ${ANCHORE_SCAN_IMAGE} &amp;gt; image-details.json
- anchore-cli --json evaluate check ${ANCHORE_SCAN_IMAGE} --detail &amp;gt; image-policy.json || true
- if [ &amp;quot;${ANCHORE_FAIL_ON_POLICY}&amp;quot; == &amp;quot;true&amp;quot; ] ; then anchore-cli evaluate check ${ANCHORE_SCAN_IMAGE} ; fi
artifacts:
name: &amp;quot;$CI_JOB_NAME&amp;quot;
paths:
- image-policy.json
- image-details.json
- image-vulnerabilities.json
- image-java.json
- image-nuget.json
- image-python.json
- image-gem.json
- image-npm.json
- image-packages.json
&lt;/code>&lt;/pre>&lt;p>The container to be scanned should have been pushed to a registry from which the Anchore Engine can pull the image.&lt;/p>
&lt;p>The first step of the job uses the Anchore CLI to instruct the Anchore Engine to analyze the image. The analysis process may take anywhere from 20 second to a few minutes depending on the size of the image, storage performance and network connectivity. During this period the Anchore Engine will:&lt;/p>
&lt;ul>
&lt;li>Download all the layers of the image to the Anchore Engine&lt;/li>
&lt;li>Extract the layers to a temporary location&lt;/li>
&lt;li>Analyze the image including reading package data, scanning for secrets or other sensitive information, recording file data such as a digests (checksum) of all files in the image including details such as file size and ownership&lt;/li>
&lt;li>Add analysis data to the Anchore database&lt;/li>
&lt;li>Delete temporary files&lt;/li>
&lt;/ul>
&lt;p>The job will poll the Anchore Engine every 10 seconds to check if the image has been analyzed and will repeat this until the maximum number of retries specified has been reached.&lt;/p>
&lt;p>The job will output 8 JSON artifacts for storage within the Job&amp;rsquo;s workspace.&lt;/p>
&lt;p>If the ANCHORE_FAIL_ON_POLICY is set to true then if the policy evaluation result is fail the entire job will fail.&lt;/p></description></item><item><title>Docs: Configuring Content Hints</title><link>/docs/install/configuration/content_hints/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/docs/install/configuration/content_hints/</guid><description>
&lt;p>For an overview of the content hints and overrides features, see the &lt;a href="/docs/general/concepts/images/analysis/content_hints/">feature overview&lt;/a>&lt;/p>
&lt;h2 id="enabling-content-hints">Enabling Content Hints&lt;/h2>
&lt;p>This feature is disabled by default to ensure that images may not exercise this feature without the admin&amp;rsquo;s explicit approval.&lt;/p>
&lt;p>In the each analyzer&amp;rsquo;s &lt;code>config.yaml&lt;/code> file (by default at &lt;code>/config/config.yaml&lt;/code>):&lt;/p>
&lt;p>Set the &lt;code>enable_hints: true&lt;/code> setting in the &lt;code>analyzer&lt;/code> service section of config.yaml.&lt;/p>
&lt;p>If using the default config.yaml included in the image, you may instead set an environment variable (e.g for use in our provided config for Docker Compose for &lt;a href="/docs/quickstart/">Quickstart&lt;/a>):&lt;/p>
&lt;p>&lt;code>ANCHORE_HINTS_ENABLED=true&lt;/code> environment variable for the analyzer service.&lt;/p>
&lt;p>For Helm: see the Helm installation instructions for enabling the hints file mechanism when deploying with Helm.&lt;/p></description></item><item><title>Docs: Content Hints</title><link>/docs/general/concepts/images/analysis/content_hints/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/docs/general/concepts/images/analysis/content_hints/</guid><description>
&lt;p>Anchore Engine includes the ability to read a user-supplied &amp;lsquo;hints&amp;rsquo; file to allow users to add software artifacts to Anchore&amp;rsquo;s
analysis report. The hints file, if present, contains records that describe a software package characteristics explicitly,
and are then added to the software bill of materials (SBOM). For example, if the owner of a CI/CD container build process
knows that there are some
software packages installed explicitly in a container image, but Anchore&amp;rsquo;s regular analyzers fail to identify them, this mechanism
can be used to include that information in the image&amp;rsquo;s SBOM, exactly as if the packages were discovered normally.&lt;/p>
&lt;p>Hints cannot be used to modify the findings of Anchore&amp;rsquo;s analyzer beyond adding new packages to the report. If a user specifies
a package in the hints file that is found by Anchore&amp;rsquo;s image analyzers, the hint is ignored and a warning message is logged
to notify the user of the conflict.&lt;/p>
&lt;h3 id="configuration">Configuration&lt;/h3>
&lt;p>See &lt;a href="/docs/install/configuration/content_hints/">Configuring Content Hints&lt;/a>&lt;/p>
&lt;p>Once enabled, the analyzer services will look for a file with a specific name, location and format located within the container image - &lt;code>/anchore_hints.json&lt;/code>.&lt;br>
The format of the file is illustrated using some examples, below.&lt;/p>
&lt;h3 id="os-package-records">OS Package Records&lt;/h3>
&lt;p>OS Packages are those that will represent packages installed using OS / Distro style package managers. Currently supported package types are &lt;code>rpm, dpkg, apkg&lt;/code>
for RedHat, Debian, and Alpine flavored package managers respectively. Note that, for OS Packages, the name of the package is unique per SBOM, meaning
that only one package named &amp;lsquo;somepackage&amp;rsquo; can exist in an image&amp;rsquo;s SBOM, and specifying a name in the hints file that conflicts with one with the same name
discovered by the Anchore analyzers will result in the record from the hints file taking precedence (override).&lt;/p>
&lt;ul>
&lt;li>Minimum required values for a package record in anchore_hints.json&lt;/li>
&lt;/ul>
&lt;pre>&lt;code> {
&amp;quot;name&amp;quot;: &amp;quot;musl&amp;quot;,
&amp;quot;version&amp;quot;: &amp;quot;1.1.20-r8&amp;quot;,
&amp;quot;type&amp;quot;: &amp;quot;apkg&amp;quot;
}
&lt;/code>&lt;/pre>&lt;ul>
&lt;li>Complete record demonstrating all of the available characteristics of a software package that can be specified&lt;/li>
&lt;/ul>
&lt;pre>&lt;code> {
&amp;quot;name&amp;quot;: &amp;quot;musl&amp;quot;,
&amp;quot;version&amp;quot;: &amp;quot;1.1.20&amp;quot;,
&amp;quot;release&amp;quot;: &amp;quot;r8&amp;quot;,
&amp;quot;origin&amp;quot;: &amp;quot;Timo Ter\u00e4s &amp;lt;timo.teras@iki.fi&amp;gt;&amp;quot;,
&amp;quot;license&amp;quot;: &amp;quot;MIT&amp;quot;,
&amp;quot;size&amp;quot;: &amp;quot;61440&amp;quot;,
&amp;quot;source&amp;quot;: &amp;quot;musl&amp;quot;,
&amp;quot;files&amp;quot;: [&amp;quot;/lib/ld-musl-x86_64.so.1&amp;quot;, &amp;quot;/lib/libc.musl-x86_64.so.1&amp;quot;, &amp;quot;/lib&amp;quot;],
&amp;quot;type&amp;quot;: &amp;quot;apkg&amp;quot;
}
&lt;/code>&lt;/pre>&lt;h3 id="non-oslanguage-package-records">Non-OS/Language Package Records&lt;/h3>
&lt;p>Non-OS / language package records are similar in form to the OS package records, but with some extra/different characteristics being supplied, namely
the &lt;code>location&lt;/code> field. Since multiple non-os packages can be installed that have the same name, the location field is particularly important as it
is used to distinguish between package records that might otherwise be identical. Valid types for non-os packages are currently &lt;code>java, python, gem, npm, nuget, go, binary&lt;/code>.&lt;br>
For the latest types that are available, see the &lt;code>anchore-cli image content &amp;lt;someimage&amp;gt;&lt;/code> output, which lists available types for any given deployment of Anchore Engine.&lt;/p>
&lt;ul>
&lt;li>Minimum required values for a package record in anchore_hints.json&lt;/li>
&lt;/ul>
&lt;pre>&lt;code> {
&amp;quot;name&amp;quot;: &amp;quot;wicked&amp;quot;,
&amp;quot;version&amp;quot;: &amp;quot;0.6.1&amp;quot;,
&amp;quot;type&amp;quot;: &amp;quot;gem&amp;quot;
}
&lt;/code>&lt;/pre>&lt;ul>
&lt;li>Complete record demonstrating all of the available characteristics of a software package that can be specified&lt;/li>
&lt;/ul>
&lt;pre>&lt;code> {
&amp;quot;name&amp;quot;: &amp;quot;wicked&amp;quot;,
&amp;quot;version&amp;quot;: &amp;quot;0.6.1&amp;quot;,
&amp;quot;location&amp;quot;: &amp;quot;/app/gems/specifications/wicked-0.9.0.gemspec&amp;quot;,
&amp;quot;origin&amp;quot;: &amp;quot;schneems&amp;quot;,
&amp;quot;license&amp;quot;: &amp;quot;MIT&amp;quot;,
&amp;quot;source&amp;quot;: &amp;quot;http://github.com/schneems/wicked&amp;quot;,
&amp;quot;files&amp;quot;: [&amp;quot;README.md&amp;quot;],
&amp;quot;type&amp;quot;: &amp;quot;gem&amp;quot;
}
&lt;/code>&lt;/pre>&lt;h3 id="putting-it-all-together">Putting it all together&lt;/h3>
&lt;p>Using the above examples, a complete anchore_hints.json file, when discovered by Anchore Engine located in &lt;code>/anchore_hints.json&lt;/code> inside any container image, is provided here:&lt;/p>
&lt;pre>&lt;code>{
&amp;quot;packages&amp;quot;: [
{
&amp;quot;name&amp;quot;: &amp;quot;musl&amp;quot;,
&amp;quot;version&amp;quot;: &amp;quot;1.1.20-r8&amp;quot;,
&amp;quot;type&amp;quot;: &amp;quot;apkg&amp;quot;
},
{
&amp;quot;name&amp;quot;: &amp;quot;wicked&amp;quot;,
&amp;quot;version&amp;quot;: &amp;quot;0.6.1&amp;quot;,
&amp;quot;type&amp;quot;: &amp;quot;gem&amp;quot;
}
]
}
&lt;/code>&lt;/pre>&lt;p>With such a hints file in an image based for example on &lt;code>alpine:latest&lt;/code>, the resulting image content would report these two package/version records
as part of the SBOM for the analyzed image, when viewed using &lt;code>anchore-cli image content &amp;lt;image&amp;gt; os&lt;/code> and &lt;code>anchore-cli image content &amp;lt;image&amp;gt; gem&lt;/code>
to view the &lt;code>musl&lt;/code> and &lt;code>wicked&lt;/code> package records, respectively.&lt;/p>
&lt;h5 id="note-about-using-the-hints-file-feature">Note about using the hints file feature&lt;/h5>
&lt;p>The hints file feature is disabled by default, and is meant to be used in very specific circumstances where a trusted entity is entrusted with creating
and installing, or removing an anchore_hints.json file from all containers being built. It is not meant to be enabled when the container image builds
are not explicitly controlled, as the entity that is building container images could override any SBOM entry that Anchore would normally discover, which
affects the vulnerability/policy status of an image. For this reason, the feature is disabled by default and must be explicitly enabled in configuration
only if appropriate for your use case .&lt;/p></description></item><item><title>Docs: Max Image Size</title><link>/docs/install/configuration/max_image_size/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/docs/install/configuration/max_image_size/</guid><description>
&lt;h2 id="setting-size-filter">Setting Size Filter&lt;/h2>
&lt;p>As of v0.9.1, Anchore Engine can be configured to have a size limit for images being added for analysis. Images that exceed the configured maximum size will not be added to Anchore and the catalog service will log an error message providing details of the failure. This size limit is applied when adding images to anchore via the &lt;a href="/docs/usage/cli_usage/images/#adding-an-image">api/cli&lt;/a>, &lt;a href="/docs/usage/cli_usage/subscriptions/#tag-updates">tag subscriptions&lt;/a>, and &lt;a href="/docs/usage/cli_usage/repositories/#watching-repositories">repository watchers&lt;/a>.&lt;/p>
&lt;p>The max size feature is disabled by default but can be enabled via &lt;code>max_compressed_image_size_mb&lt;/code> in the configuration file, which represents the size limit in MB of the compressed image. Values less than 0 will disable the feature and allow images of any size to be added to Anchore. A value of 0 will be enforced and prevent any images from being added. Non-integer values will cause bootstrap of the service to fail. If using compose with the default config, this can be set through the &lt;code>ANCHORE_MAX_COMPRESSED_IMAGE_SIZE_MB&lt;/code> env variable on the catalog service. If using helm, it can be defined in the values file via &lt;code>anchoreGlobal.maxCompressedImageSizeMB&lt;/code>&lt;/p></description></item><item><title>Docs: Network Proxies</title><link>/docs/install/configuration/network_proxies/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/docs/install/configuration/network_proxies/</guid><description>
&lt;p>As covered in the Network sections of the requirements document the Anchore Engine requires three categories of network connectivity.&lt;/p>
&lt;ul>
&lt;li>
&lt;p>Registry Access
Network connectivity, including DNS resolution, to the registries from which the Anchore Engine needs to download images.&lt;/p>
&lt;/li>
&lt;li>
&lt;p>Feed Service
The Anchore Engine synchronizes feed data such as operating system vulnerabilities (CVEs) from the Anchore Cloud Service. Only a single end point is required for this synchronization, host: ancho.re TCP port: 443&lt;/p>
&lt;/li>
&lt;li>
&lt;p>Access to Anchore Internal Services
The Anchore Engine is comprised of six smaller micro-services that can be deployed in a single container or scaled out to handle load. Each Anchore service should be able to connect the other services over the network.&lt;/p>
&lt;/li>
&lt;/ul>
&lt;p>In environments were access to the public internet is restricted then a proxy server may be required to allow the Anchore Engine to connect to the Anchore Cloud Feed Service or to a publicly hosted container registry.&lt;/p>
&lt;p>The Anchore Engine can be configured to access a proxy server by using environment variables that are read by the Anchore Engine at run time.&lt;/p>
&lt;ul>
&lt;li>
&lt;p>https_proxy:
Address of the proxy service to use for HTTPS traffic in the following form: {PROTOCOL}://{IP or HOSTNAME}:{PORT}
eg. &lt;a href="https://proxy.corp.example.com">https://proxy.corp.example.com&lt;/a>:8128&lt;/p>
&lt;/li>
&lt;li>
&lt;p>http_proxy: &lt;br>
Address of the proxy service to use for HTTP traffic in the following form: {PROTOCOL}://{IP or HOSTNAME}:{PORT}&lt;br>
eg. &lt;a href="http://proxy.corp.example.com">http://proxy.corp.example.com&lt;/a>:8128&lt;/p>
&lt;/li>
&lt;li>
&lt;p>no_proxy:&lt;br>
Comma delimited list of hostnames or IP address which should be accessed directly without using the proxy service.
eg. localhost,127.0.0.1,registry,example.com&lt;/p>
&lt;/li>
&lt;/ul>
&lt;h3 id="notes">Notes:&lt;/h3>
&lt;ul>
&lt;li>Do not use double quotes (&amp;quot;) around the proxy variable values.&lt;/li>
&lt;/ul>
&lt;h3 id="authentication">Authentication&lt;/h3>
&lt;p>For proxy servers that require authentication the username and password can be provided as part of the URL:&lt;/p>
&lt;p>eg. https_proxy=https://user:password@proxy.corp.example.com:8128&lt;/p>
&lt;p>If the username or password contains and non-url safe characters then these should be urlencoded.&lt;/p>
&lt;p>For example:&lt;/p>
&lt;p>&lt;code>The password F@oBar! would be encoded as F%40oBar%21&lt;/code>&lt;/p>
&lt;h3 id="setting-environment-variables">Setting Environment Variables&lt;/h3>
&lt;p>Docker Compose: &lt;a href="https://docs.docker.com/compose/environment-variables/">https://docs.docker.com/compose/environment-variables/&lt;/a>&lt;/p>
&lt;p>Kubernetes: &lt;a href="https://kubernetes.io/docs/tasks/inject-data-application/define-environment-variable-container/">https://kubernetes.io/docs/tasks/inject-data-application/define-environment-variable-container/&lt;/a>&lt;/p>
&lt;h3 id="deployment-architecture-notes">Deployment Architecture Notes&lt;/h3>
&lt;p>When setting up a network proxy, keep in mind that you will need to explicitly allow inter-service communication within the anchore engine deployment to bypass the proxy, and potentially other hostnames as well (e.g. internal registries) to ensure that traffic is directed correctly. In general, all anchore engine service endpoints (the URLs for enabled services in the output of an &amp;lsquo;anchore-cli system status&amp;rsquo; command) as well as any internal registries (the hostnames you may have set up with &amp;lsquo;anchore-cli registry add &lt;!-- raw HTML omitted --> &amp;hellip;&amp;rsquo; or as part of an un-credentialed image add &amp;lsquo;anchore-cli image add &lt;a href="registry:port">registry:port&lt;/a>/&amp;hellip;.'), should not be proxied (i.e. added to the no_proxy list, as described above).&lt;/p>
&lt;p>If you wish to tune this further, below is a list of each component that makes an external URL fetch for various purposes:&lt;/p>
&lt;ul>
&lt;li>catalog: makes connections to image registries (any host added via &amp;lsquo;anchore-cli registry add&amp;rsquo; or directly via &amp;lsquo;anchore-cli image add&amp;rsquo;)&lt;/li>
&lt;li>analyzer: same as catalog&lt;/li>
&lt;li>policy_engine: by default, makes HTTPS connection to &lt;a href="https://ancho.re">https://ancho.re&lt;/a> feed service, unless on-prem feed service is deployed&lt;/li>
&lt;/ul></description></item><item><title>Docs: Using Environment Variables in Anchore</title><link>/docs/install/configuration/using_env_vars/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/docs/install/configuration/using_env_vars/</guid><description>
&lt;p>Environment variable references may be used in the Anchore config.yaml file to set values that need to be configurable during deployment.&lt;/p>
&lt;p>Using this mechanism a common configuration file can be used with multiple Anchore Engine instances with key values being passed using environment variables.&lt;/p>
&lt;p>The config.yaml configuration file is read by the Anchore Engine any references to variables prefixed with ANCHORE will be replaced by the value of the matching environment variable.&lt;/p>
&lt;p>For example in the sample configuration file the &lt;em>host_id&lt;/em> parameter is set be appending the ANCHORE_HOST_ID variable to the string &lt;em>dockerhostid&lt;/em>&lt;/p>
&lt;p>&lt;code>host_id: 'dockerhostid-${ANCHORE_HOST_ID}'&lt;/code>&lt;/p>
&lt;p>Notes:&lt;/p>
&lt;ol>
&lt;li>Only variables prefixed with ANCHORE will be replaced&lt;/li>
&lt;li>If an environment variable is referenced in the configuration file but not set in the environment then a warning will be logged&lt;/li>
&lt;li>It is recommend to use curly braces, for example ${ANCHORE_PARAM} to avoid potentially ambiguous cases&lt;/li>
&lt;/ol>
&lt;h3 id="passing-environment-variables-as-a-file">Passing Environment Variables as a File&lt;/h3>
&lt;p>Environment Variables may also be passed as a file contained key value pairs.&lt;/p>
&lt;pre>&lt;code>ANCHORE_HOST_ID=myservice1
ANCHORE_LOG_LEVEL=DEBUG
&lt;/code>&lt;/pre>&lt;p>The Anchore Engine will check for an environment variable named &lt;em>ANCHORE_ENV_FILE&lt;/em> if this variable is set the the Anchore Engine will attempt to read a file at the location specified in this variable.&lt;/p>
&lt;p>The Anchore environment file is read before any other Anchore environment variables so any ANCHORE variables passed in the environment will override the values set in the environment file.&lt;/p></description></item><item><title>Docs: Anchore Policy Checks</title><link>/docs/general/concepts/policy/policy_checks/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/docs/general/concepts/policy/policy_checks/</guid><description>
&lt;h2 id="introduction">Introduction&lt;/h2>
&lt;p>In this document, we describe the current anchore gates (and related triggers/parameters) that are supported within anchore policy bundles. If you have a running anchore engine, this information can also be obtained using the CLI:&lt;/p>
&lt;p>&lt;code># anchore-cli policy describe (--gate &amp;lt;gatename&amp;gt; ( --trigger &amp;lt;triggername))&lt;/code>&lt;/p>
&lt;h3 id="gate-dockerfile">Gate: dockerfile&lt;/h3>
&lt;p>Checks against the content of a dockerfile if provided, or a guessed dockerfile based on docker layer history if the dockerfile is not provided.&lt;/p>
&lt;table>
&lt;thead>
&lt;tr>
&lt;th style="text-align:left">Trigger Name&lt;/th>
&lt;th style="text-align:left">Description&lt;/th>
&lt;th style="text-align:left">Parameter&lt;/th>
&lt;th style="text-align:left">Description&lt;/th>
&lt;th style="text-align:left">Example&lt;/th>
&lt;/tr>
&lt;/thead>
&lt;tbody>
&lt;tr>
&lt;td style="text-align:left">instruction&lt;/td>
&lt;td style="text-align:left">Triggers if any directives in the list are found to match the described condition in the dockerfile.&lt;/td>
&lt;td style="text-align:left">instruction&lt;/td>
&lt;td style="text-align:left">The Dockerfile instruction to check.&lt;/td>
&lt;td style="text-align:left">from&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td style="text-align:left">instruction&lt;/td>
&lt;td style="text-align:left">Triggers if any directives in the list are found to match the described condition in the dockerfile.&lt;/td>
&lt;td style="text-align:left">check&lt;/td>
&lt;td style="text-align:left">The type of check to perform.&lt;/td>
&lt;td style="text-align:left">=&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td style="text-align:left">instruction&lt;/td>
&lt;td style="text-align:left">Triggers if any directives in the list are found to match the described condition in the dockerfile.&lt;/td>
&lt;td style="text-align:left">value&lt;/td>
&lt;td style="text-align:left">The value to check the dockerfile instruction against.&lt;/td>
&lt;td style="text-align:left">scratch&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td style="text-align:left">instruction&lt;/td>
&lt;td style="text-align:left">Triggers if any directives in the list are found to match the described condition in the dockerfile.&lt;/td>
&lt;td style="text-align:left">actual_dockerfile_only&lt;/td>
&lt;td style="text-align:left">Only evaluate against a user-provided dockerfile, skip evaluation on inferred/guessed dockerfiles. Default is False.&lt;/td>
&lt;td style="text-align:left">true&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td style="text-align:left">effective_user&lt;/td>
&lt;td style="text-align:left">Checks if the effective user matches the provided user names, either as a whitelist or blacklist depending on the type parameter setting.&lt;/td>
&lt;td style="text-align:left">users&lt;/td>
&lt;td style="text-align:left">User names to check against as the effective user (last user entry) in the images history.&lt;/td>
&lt;td style="text-align:left">root,docker&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td style="text-align:left">effective_user&lt;/td>
&lt;td style="text-align:left">Checks if the effective user matches the provided user names, either as a whitelist or blacklist depending on the type parameter setting.&lt;/td>
&lt;td style="text-align:left">type&lt;/td>
&lt;td style="text-align:left">How to treat the provided user names.&lt;/td>
&lt;td style="text-align:left">blacklist&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td style="text-align:left">exposed_ports&lt;/td>
&lt;td style="text-align:left">Evaluates the set of ports exposed. Allows configuring whitelist or blacklist behavior. If type=whitelist, then any ports found exposed that are not in the list will cause the trigger to fire. If type=blacklist, then any ports exposed that are in the list will cause the trigger to fire.&lt;/td>
&lt;td style="text-align:left">ports&lt;/td>
&lt;td style="text-align:left">List of port numbers.&lt;/td>
&lt;td style="text-align:left">80,8080,8088&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td style="text-align:left">exposed_ports&lt;/td>
&lt;td style="text-align:left">Evaluates the set of ports exposed. Allows configuring whitelist or blacklist behavior. If type=whitelist, then any ports found exposed that are not in the list will cause the trigger to fire. If type=blacklist, then any ports exposed that are in the list will cause the trigger to fire.&lt;/td>
&lt;td style="text-align:left">type&lt;/td>
&lt;td style="text-align:left">Whether to use port list as a whitelist or blacklist.&lt;/td>
&lt;td style="text-align:left">blacklist&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td style="text-align:left">exposed_ports&lt;/td>
&lt;td style="text-align:left">Evaluates the set of ports exposed. Allows configuring whitelist or blacklist behavior. If type=whitelist, then any ports found exposed that are not in the list will cause the trigger to fire. If type=blacklist, then any ports exposed that are in the list will cause the trigger to fire.&lt;/td>
&lt;td style="text-align:left">actual_dockerfile_only&lt;/td>
&lt;td style="text-align:left">Only evaluate against a user-provided dockerfile, skip evaluation on inferred/guessed dockerfiles. Default is False.&lt;/td>
&lt;td style="text-align:left">true&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td style="text-align:left">no_dockerfile_provided&lt;/td>
&lt;td style="text-align:left">Triggers if anchore analysis was performed without supplying the actual image Dockerfile.&lt;/td>
&lt;td style="text-align:left">&lt;/td>
&lt;td style="text-align:left">&lt;/td>
&lt;td style="text-align:left">&lt;/td>
&lt;/tr>
&lt;/tbody>
&lt;/table>
&lt;h3 id="gate-files">Gate: files&lt;/h3>
&lt;p>Checks against files in the analyzed image including file content, file names, and filesystem attributes.&lt;/p>
&lt;table>
&lt;thead>
&lt;tr>
&lt;th style="text-align:left">Trigger Name&lt;/th>
&lt;th style="text-align:left">Description&lt;/th>
&lt;th style="text-align:left">Parameter&lt;/th>
&lt;th style="text-align:left">Description&lt;/th>
&lt;th style="text-align:left">Example&lt;/th>
&lt;/tr>
&lt;/thead>
&lt;tbody>
&lt;tr>
&lt;td style="text-align:left">content_regex_match&lt;/td>
&lt;td style="text-align:left">Triggers for each file where the content search analyzer has found a match using configured regexes in the analyzer_config.yaml &amp;ldquo;content_search&amp;rdquo; section. If the parameter is set, the trigger will only fire for files that matched the named regex. Refer to your analyzer_config.yaml for the regex values.&lt;/td>
&lt;td style="text-align:left">regex_name&lt;/td>
&lt;td style="text-align:left">Regex string that also appears in the FILECHECK_CONTENTMATCH analyzer parameter in analyzer configuration, to limit the check to. If set, will only fire trigger when the specific named regex was found in a file.&lt;/td>
&lt;td style="text-align:left">.&lt;em>password.&lt;/em>&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td style="text-align:left">name_match&lt;/td>
&lt;td style="text-align:left">Triggers if a file exists in the container that has a filename that matches the provided regex. This does have a performance impact on policy evaluation.&lt;/td>
&lt;td style="text-align:left">regex&lt;/td>
&lt;td style="text-align:left">Regex to apply to file names for match.&lt;/td>
&lt;td style="text-align:left">.*.pem&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td style="text-align:left">attribute_match&lt;/td>
&lt;td style="text-align:left">Triggers if a filename exists in the container that has attributes that match those which are provided . This check has a performance impact on policy evaluation.&lt;/td>
&lt;td style="text-align:left">filename&lt;/td>
&lt;td style="text-align:left">Filename to check against provided checksum.&lt;/td>
&lt;td style="text-align:left">/etc/passwd&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td style="text-align:left">attribute_match&lt;/td>
&lt;td style="text-align:left">Triggers if a filename exists in the container that has attributes that match those which are provided . This check has a performance impact on policy evaluation.&lt;/td>
&lt;td style="text-align:left">checksum_algorithm&lt;/td>
&lt;td style="text-align:left">Checksum algorithm&lt;/td>
&lt;td style="text-align:left">sha256&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td style="text-align:left">attribute_match&lt;/td>
&lt;td style="text-align:left">Triggers if a filename exists in the container that has attributes that match those which are provided . This check has a performance impact on policy evaluation.&lt;/td>
&lt;td style="text-align:left">checksum&lt;/td>
&lt;td style="text-align:left">Checksum of file.&lt;/td>
&lt;td style="text-align:left">832cd0f75b227d13aac82b1f70b7f90191a4186c151f9db50851d209c45ede11&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td style="text-align:left">attribute_match&lt;/td>
&lt;td style="text-align:left">Triggers if a filename exists in the container that has attributes that match those which are provided . This check has a performance impact on policy evaluation.&lt;/td>
&lt;td style="text-align:left">checksum_match&lt;/td>
&lt;td style="text-align:left">Checksum operation to perform.&lt;/td>
&lt;td style="text-align:left">equals&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td style="text-align:left">attribute_match&lt;/td>
&lt;td style="text-align:left">Triggers if a filename exists in the container that has attributes that match those which are provided . This check has a performance impact on policy evaluation.&lt;/td>
&lt;td style="text-align:left">mode&lt;/td>
&lt;td style="text-align:left">File mode of file.&lt;/td>
&lt;td style="text-align:left">00644&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td style="text-align:left">attribute_match&lt;/td>
&lt;td style="text-align:left">Triggers if a filename exists in the container that has attributes that match those which are provided . This check has a performance impact on policy evaluation.&lt;/td>
&lt;td style="text-align:left">mode_op&lt;/td>
&lt;td style="text-align:left">File mode operation to perform.&lt;/td>
&lt;td style="text-align:left">equals&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td style="text-align:left">attribute_match&lt;/td>
&lt;td style="text-align:left">Triggers if a filename exists in the container that has attributes that match those which are provided . This check has a performance impact on policy evaluation.&lt;/td>
&lt;td style="text-align:left">skip_missing&lt;/td>
&lt;td style="text-align:left">If set to true, do not fire this trigger if the file is not present. If set to false, fire this trigger ignoring the other parameter settings.&lt;/td>
&lt;td style="text-align:left">true&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td style="text-align:left">suid_or_guid_set&lt;/td>
&lt;td style="text-align:left">Fires for each file found to have suid or sgid bit set.&lt;/td>
&lt;td style="text-align:left">&lt;/td>
&lt;td style="text-align:left">&lt;/td>
&lt;td style="text-align:left">&lt;/td>
&lt;/tr>
&lt;/tbody>
&lt;/table>
&lt;h3 id="gate-passwd_file">Gate: passwd_file&lt;/h3>
&lt;p>Content checks for /etc/passwd for things like usernames, group ids, shells, or full entries.&lt;/p>
&lt;table>
&lt;thead>
&lt;tr>
&lt;th style="text-align:left">Trigger Name&lt;/th>
&lt;th style="text-align:left">Description&lt;/th>
&lt;th style="text-align:left">Parameter&lt;/th>
&lt;th style="text-align:left">Description&lt;/th>
&lt;th style="text-align:left">Example&lt;/th>
&lt;/tr>
&lt;/thead>
&lt;tbody>
&lt;tr>
&lt;td style="text-align:left">content_not_available&lt;/td>
&lt;td style="text-align:left">Triggers if the /etc/passwd file is not present/stored in the evaluated image.&lt;/td>
&lt;td style="text-align:left">&lt;/td>
&lt;td style="text-align:left">&lt;/td>
&lt;td style="text-align:left">&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td style="text-align:left">blacklist_usernames&lt;/td>
&lt;td style="text-align:left">Triggers if specified username is found in the /etc/passwd file&lt;/td>
&lt;td style="text-align:left">user_names&lt;/td>
&lt;td style="text-align:left">List of usernames that will cause the trigger to fire if found in /etc/passwd.&lt;/td>
&lt;td style="text-align:left">daemon,ftp&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td style="text-align:left">blacklist_userids&lt;/td>
&lt;td style="text-align:left">Triggers if specified user id is found in the /etc/passwd file&lt;/td>
&lt;td style="text-align:left">user_ids&lt;/td>
&lt;td style="text-align:left">List of userids (numeric) that will cause the trigger to fire if found in /etc/passwd.&lt;/td>
&lt;td style="text-align:left">0,1&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td style="text-align:left">blacklist_groupids&lt;/td>
&lt;td style="text-align:left">Triggers if specified group id is found in the /etc/passwd file&lt;/td>
&lt;td style="text-align:left">group_ids&lt;/td>
&lt;td style="text-align:left">List of groupids (numeric) that will cause the trigger ot fire if found in /etc/passwd.&lt;/td>
&lt;td style="text-align:left">999,20&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td style="text-align:left">blacklist_shells&lt;/td>
&lt;td style="text-align:left">Triggers if specified login shell for any user is found in the /etc/passwd file&lt;/td>
&lt;td style="text-align:left">shells&lt;/td>
&lt;td style="text-align:left">List of shell commands to blacklist.&lt;/td>
&lt;td style="text-align:left">/bin/bash,/bin/zsh&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td style="text-align:left">blacklist_full_entry&lt;/td>
&lt;td style="text-align:left">Triggers if entire specified passwd entry is found in the /etc/passwd file.&lt;/td>
&lt;td style="text-align:left">entry&lt;/td>
&lt;td style="text-align:left">Full entry to match in /etc/passwd.&lt;/td>
&lt;td style="text-align:left">ftp:x:14:50:FTP User:/var/ftp:/sbin/nologin&lt;/td>
&lt;/tr>
&lt;/tbody>
&lt;/table>
&lt;h3 id="gate-packages">Gate: packages&lt;/h3>
&lt;p>Distro package checks&lt;/p>
&lt;table>
&lt;thead>
&lt;tr>
&lt;th style="text-align:left">Trigger Name&lt;/th>
&lt;th style="text-align:left">Description&lt;/th>
&lt;th style="text-align:left">Parameter&lt;/th>
&lt;th style="text-align:left">Description&lt;/th>
&lt;th style="text-align:left">Example&lt;/th>
&lt;/tr>
&lt;/thead>
&lt;tbody>
&lt;tr>
&lt;td style="text-align:left">required_package&lt;/td>
&lt;td style="text-align:left">Triggers if the specified package and optionally a specific version is not found in the image.&lt;/td>
&lt;td style="text-align:left">name&lt;/td>
&lt;td style="text-align:left">Name of package that must be found installed in image.&lt;/td>
&lt;td style="text-align:left">libssl&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td style="text-align:left">required_package&lt;/td>
&lt;td style="text-align:left">Triggers if the specified package and optionally a specific version is not found in the image.&lt;/td>
&lt;td style="text-align:left">version&lt;/td>
&lt;td style="text-align:left">Optional version of package for exact version match.&lt;/td>
&lt;td style="text-align:left">1.10.3rc3&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td style="text-align:left">required_package&lt;/td>
&lt;td style="text-align:left">Triggers if the specified package and optionally a specific version is not found in the image.&lt;/td>
&lt;td style="text-align:left">version_match_type&lt;/td>
&lt;td style="text-align:left">The type of comparison to use for version if a version is provided.&lt;/td>
&lt;td style="text-align:left">exact&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td style="text-align:left">verify&lt;/td>
&lt;td style="text-align:left">Check package integrity against package db in the image. Triggers for changes or removal or content in all or the selected &amp;ldquo;dirs&amp;rdquo; parameter if provided, and can filter type of check with the &amp;ldquo;check_only&amp;rdquo; parameter.&lt;/td>
&lt;td style="text-align:left">only_packages&lt;/td>
&lt;td style="text-align:left">List of package names to limit verification.&lt;/td>
&lt;td style="text-align:left">libssl,openssl&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td style="text-align:left">verify&lt;/td>
&lt;td style="text-align:left">Check package integrity against package db in the image. Triggers for changes or removal or content in all or the selected &amp;ldquo;dirs&amp;rdquo; parameter if provided, and can filter type of check with the &amp;ldquo;check_only&amp;rdquo; parameter.&lt;/td>
&lt;td style="text-align:left">only_directories&lt;/td>
&lt;td style="text-align:left">List of directories to limit checks so as to avoid checks on all dir.&lt;/td>
&lt;td style="text-align:left">/usr,/var/lib&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td style="text-align:left">verify&lt;/td>
&lt;td style="text-align:left">Check package integrity against package db in the image. Triggers for changes or removal or content in all or the selected &amp;ldquo;dirs&amp;rdquo; parameter if provided, and can filter type of check with the &amp;ldquo;check_only&amp;rdquo; parameter.&lt;/td>
&lt;td style="text-align:left">check&lt;/td>
&lt;td style="text-align:left">Check to perform instead of all.&lt;/td>
&lt;td style="text-align:left">changed&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td style="text-align:left">blacklist&lt;/td>
&lt;td style="text-align:left">Triggers if the evaluated image has a package installed that matches the named package optionally with a specific version as well.&lt;/td>
&lt;td style="text-align:left">name&lt;/td>
&lt;td style="text-align:left">Package name to blacklist.&lt;/td>
&lt;td style="text-align:left">openssh-server&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td style="text-align:left">blacklist&lt;/td>
&lt;td style="text-align:left">Triggers if the evaluated image has a package installed that matches the named package optionally with a specific version as well.&lt;/td>
&lt;td style="text-align:left">version&lt;/td>
&lt;td style="text-align:left">Specific version of package to blacklist.&lt;/td>
&lt;td style="text-align:left">1.0.1&lt;/td>
&lt;/tr>
&lt;/tbody>
&lt;/table>
&lt;h3 id="gate-vulnerabilities">Gate: vulnerabilities&lt;/h3>
&lt;p>CVE/Vulnerability checks.&lt;/p>
&lt;table>
&lt;thead>
&lt;tr>
&lt;th style="text-align:left">Trigger Name&lt;/th>
&lt;th style="text-align:left">Description&lt;/th>
&lt;th style="text-align:left">Parameter&lt;/th>
&lt;th style="text-align:left">Description&lt;/th>
&lt;th style="text-align:left">Example&lt;/th>
&lt;/tr>
&lt;/thead>
&lt;tbody>
&lt;tr>
&lt;td style="text-align:left">package&lt;/td>
&lt;td style="text-align:left">Triggers if a found vulnerability in an image meets the comparison criteria.&lt;/td>
&lt;td style="text-align:left">package_type&lt;/td>
&lt;td style="text-align:left">Only trigger for specific package type.&lt;/td>
&lt;td style="text-align:left">all&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td style="text-align:left">package&lt;/td>
&lt;td style="text-align:left">Triggers if a found vulnerability in an image meets the comparison criteria.&lt;/td>
&lt;td style="text-align:left">severity_comparison&lt;/td>
&lt;td style="text-align:left">The type of comparison to perform for severity evaluation.&lt;/td>
&lt;td style="text-align:left">&amp;gt;&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td style="text-align:left">package&lt;/td>
&lt;td style="text-align:left">Triggers if a found vulnerability in an image meets the comparison criteria.&lt;/td>
&lt;td style="text-align:left">severity&lt;/td>
&lt;td style="text-align:left">Severity to compare against.&lt;/td>
&lt;td style="text-align:left">high&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td style="text-align:left">package&lt;/td>
&lt;td style="text-align:left">Triggers if a found vulnerability in an image meets the comparison criteria.&lt;/td>
&lt;td style="text-align:left">cvss_v3_base_score_comparison&lt;/td>
&lt;td style="text-align:left">The type of comparison to perform for CVSS v3 base score evaluation.&lt;/td>
&lt;td style="text-align:left">&amp;gt;&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td style="text-align:left">package&lt;/td>
&lt;td style="text-align:left">Triggers if a found vulnerability in an image meets the comparison criteria.&lt;/td>
&lt;td style="text-align:left">cvss_v3_base_score&lt;/td>
&lt;td style="text-align:left">CVSS v3 base score to compare against.&lt;/td>
&lt;td style="text-align:left">None&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td style="text-align:left">package&lt;/td>
&lt;td style="text-align:left">Triggers if a found vulnerability in an image meets the comparison criteria.&lt;/td>
&lt;td style="text-align:left">cvss_v3_exploitability_score_comparison&lt;/td>
&lt;td style="text-align:left">The type of comparison to perform for CVSS v3 exploitability sub score evaluation.&lt;/td>
&lt;td style="text-align:left">&amp;gt;&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td style="text-align:left">package&lt;/td>
&lt;td style="text-align:left">Triggers if a found vulnerability in an image meets the comparison criteria.&lt;/td>
&lt;td style="text-align:left">cvss_v3_exploitability_score&lt;/td>
&lt;td style="text-align:left">CVSS v3 exploitability sub score to compare against.&lt;/td>
&lt;td style="text-align:left">None&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td style="text-align:left">package&lt;/td>
&lt;td style="text-align:left">Triggers if a found vulnerability in an image meets the comparison criteria.&lt;/td>
&lt;td style="text-align:left">cvss_v3_impact_score_comparison&lt;/td>
&lt;td style="text-align:left">The type of comparison to perform for CVSS v3 impact sub score evaluation.&lt;/td>
&lt;td style="text-align:left">&amp;gt;&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td style="text-align:left">package&lt;/td>
&lt;td style="text-align:left">Triggers if a found vulnerability in an image meets the comparison criteria.&lt;/td>
&lt;td style="text-align:left">cvss_v3_impact_score&lt;/td>
&lt;td style="text-align:left">CVSS v3 impact sub score to compare against.&lt;/td>
&lt;td style="text-align:left">None&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td style="text-align:left">package&lt;/td>
&lt;td style="text-align:left">Triggers if a found vulnerability in an image meets the comparison criteria.&lt;/td>
&lt;td style="text-align:left">fix_available&lt;/td>
&lt;td style="text-align:left">If present, the fix availability for the vulnerability record must match the value of this parameter.&lt;/td>
&lt;td style="text-align:left">true&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td style="text-align:left">package&lt;/td>
&lt;td style="text-align:left">Triggers if a found vulnerability in an image meets the comparison criteria.&lt;/td>
&lt;td style="text-align:left">vendor_only&lt;/td>
&lt;td style="text-align:left">If True, an available fix for this CVE must not be explicitly marked as wont be addressed by the vendor&lt;/td>
&lt;td style="text-align:left">true&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td style="text-align:left">package&lt;/td>
&lt;td style="text-align:left">Triggers if a found vulnerability in an image meets the comparison criteria.&lt;/td>
&lt;td style="text-align:left">max_days_since_creation&lt;/td>
&lt;td style="text-align:left">If provided, this CVE must be older than the days provided to trigger.&lt;/td>
&lt;td style="text-align:left">7&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td style="text-align:left">package&lt;/td>
&lt;td style="text-align:left">Triggers if a found vulnerability in an image meets the comparison criteria.&lt;/td>
&lt;td style="text-align:left">max_days_since_fix&lt;/td>
&lt;td style="text-align:left">If provided (only evaluated when fix_available option is also set to true), the fix first observed time must be older than days provided, to trigger.&lt;/td>
&lt;td style="text-align:left">30&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td style="text-align:left">package&lt;/td>
&lt;td style="text-align:left">Triggers if a found vulnerability in an image meets the comparison criteria.&lt;/td>
&lt;td style="text-align:left">vendor_cvss_v3_base_score_comparison&lt;/td>
&lt;td style="text-align:left">The type of comparison to perform for vendor specified CVSS v3 base score evaluation.&lt;/td>
&lt;td style="text-align:left">&amp;gt;&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td style="text-align:left">package&lt;/td>
&lt;td style="text-align:left">Triggers if a found vulnerability in an image meets the comparison criteria.&lt;/td>
&lt;td style="text-align:left">vendor_cvss_v3_base_score&lt;/td>
&lt;td style="text-align:left">Vendor CVSS v3 base score to compare against.&lt;/td>
&lt;td style="text-align:left">None&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td style="text-align:left">package&lt;/td>
&lt;td style="text-align:left">Triggers if a found vulnerability in an image meets the comparison criteria.&lt;/td>
&lt;td style="text-align:left">vendor_cvss_v3_exploitability_score_comparison&lt;/td>
&lt;td style="text-align:left">The type of comparison to perform for vendor specified CVSS v3 exploitability sub score evaluation.&lt;/td>
&lt;td style="text-align:left">&amp;gt;&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td style="text-align:left">package&lt;/td>
&lt;td style="text-align:left">Triggers if a found vulnerability in an image meets the comparison criteria.&lt;/td>
&lt;td style="text-align:left">vendor_cvss_v3_exploitability_score&lt;/td>
&lt;td style="text-align:left">Vendor CVSS v3 exploitability sub score to compare against.&lt;/td>
&lt;td style="text-align:left">None&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td style="text-align:left">package&lt;/td>
&lt;td style="text-align:left">Triggers if a found vulnerability in an image meets the comparison criteria.&lt;/td>
&lt;td style="text-align:left">vendor_cvss_v3_impact_score_comparison&lt;/td>
&lt;td style="text-align:left">The type of comparison to perform for vendor specified CVSS v3 impact sub score evaluation.&lt;/td>
&lt;td style="text-align:left">&amp;gt;&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td style="text-align:left">package&lt;/td>
&lt;td style="text-align:left">Triggers if a found vulnerability in an image meets the comparison criteria.&lt;/td>
&lt;td style="text-align:left">vendor_cvss_v3_impact_score&lt;/td>
&lt;td style="text-align:left">Vendor CVSS v3 impact sub score to compare against.&lt;/td>
&lt;td style="text-align:left">None&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td style="text-align:left">blacklist&lt;/td>
&lt;td style="text-align:left">Triggers if any of a list of specified vulnerabilities has been detected in the image.&lt;/td>
&lt;td style="text-align:left">vulnerability_ids&lt;/td>
&lt;td style="text-align:left">List of vulnerability IDs, will cause the trigger to fire if any are detected.&lt;/td>
&lt;td style="text-align:left">CVE-2019-1234&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td style="text-align:left">blacklist&lt;/td>
&lt;td style="text-align:left">Triggers if any of a list of specified vulnerabilities has been detected in the image.&lt;/td>
&lt;td style="text-align:left">vulnerability_ids&lt;/td>
&lt;td style="text-align:left">If set to True, discard matches against this vulnerability if vendor has marked as will not fix in the vulnerability record.&lt;/td>
&lt;td style="text-align:left">True&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td style="text-align:left">stale_feed_data&lt;/td>
&lt;td style="text-align:left">Triggers if the CVE data is older than the window specified by the parameter MAXAGE (unit is number of days).&lt;/td>
&lt;td style="text-align:left">max_days_since_sync&lt;/td>
&lt;td style="text-align:left">Fire the trigger if the last sync was more than this number of days ago.&lt;/td>
&lt;td style="text-align:left">10&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td style="text-align:left">vulnerability_data_unavailable&lt;/td>
&lt;td style="text-align:left">Triggers if vulnerability data is unavailable for the image&amp;rsquo;s distro.&lt;/td>
&lt;td style="text-align:left">&lt;/td>
&lt;td style="text-align:left">&lt;/td>
&lt;td style="text-align:left">&lt;/td>
&lt;/tr>
&lt;/tbody>
&lt;/table>
&lt;h3 id="gate-licenses">Gate: licenses&lt;/h3>
&lt;p>License checks against found software licenses in the container image&lt;/p>
&lt;table>
&lt;thead>
&lt;tr>
&lt;th style="text-align:left">Trigger Name&lt;/th>
&lt;th style="text-align:left">Description&lt;/th>
&lt;th style="text-align:left">Parameter&lt;/th>
&lt;th style="text-align:left">Description&lt;/th>
&lt;th style="text-align:left">Example&lt;/th>
&lt;/tr>
&lt;/thead>
&lt;tbody>
&lt;tr>
&lt;td style="text-align:left">blacklist_exact_match&lt;/td>
&lt;td style="text-align:left">Triggers if the evaluated image has a package installed with software distributed under the specified (exact match) license(s).&lt;/td>
&lt;td style="text-align:left">licenses&lt;/td>
&lt;td style="text-align:left">List of license names to blacklist exactly.&lt;/td>
&lt;td style="text-align:left">GPLv2+,GPL-3+,BSD-2-clause&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td style="text-align:left">blacklist_partial_match&lt;/td>
&lt;td style="text-align:left">triggers if the evaluated image has a package installed with software distributed under the specified (substring match) license(s)&lt;/td>
&lt;td style="text-align:left">licenses&lt;/td>
&lt;td style="text-align:left">List of strings to do substring match for blacklist.&lt;/td>
&lt;td style="text-align:left">LGPL,BSD&lt;/td>
&lt;/tr>
&lt;/tbody>
&lt;/table>
&lt;h3 id="gate-ruby_gems">Gate: ruby_gems&lt;/h3>
&lt;p>Ruby Gem Checks&lt;/p>
&lt;table>
&lt;thead>
&lt;tr>
&lt;th style="text-align:left">Trigger Name&lt;/th>
&lt;th style="text-align:left">Description&lt;/th>
&lt;th style="text-align:left">Parameter&lt;/th>
&lt;th style="text-align:left">Description&lt;/th>
&lt;th style="text-align:left">Example&lt;/th>
&lt;/tr>
&lt;/thead>
&lt;tbody>
&lt;tr>
&lt;td style="text-align:left">newer_version_found_in_feed&lt;/td>
&lt;td style="text-align:left">Triggers if an installed GEM is not the latest version according to GEM data feed.&lt;/td>
&lt;td style="text-align:left">&lt;/td>
&lt;td style="text-align:left">&lt;/td>
&lt;td style="text-align:left">&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td style="text-align:left">not_found_in_feed&lt;/td>
&lt;td style="text-align:left">Triggers if an installed GEM is not in the official GEM database, according to GEM data feed.&lt;/td>
&lt;td style="text-align:left">&lt;/td>
&lt;td style="text-align:left">&lt;/td>
&lt;td style="text-align:left">&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td style="text-align:left">version_not_found_in_feed&lt;/td>
&lt;td style="text-align:left">Triggers if an installed GEM version is not listed in the official GEM feed as a valid version.&lt;/td>
&lt;td style="text-align:left">&lt;/td>
&lt;td style="text-align:left">&lt;/td>
&lt;td style="text-align:left">&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td style="text-align:left">blacklist&lt;/td>
&lt;td style="text-align:left">Triggers if the evaluated image has a GEM package installed that matches the specified name and version.&lt;/td>
&lt;td style="text-align:left">name&lt;/td>
&lt;td style="text-align:left">Gem name to blacklist.&lt;/td>
&lt;td style="text-align:left">time_diff&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td style="text-align:left">blacklist&lt;/td>
&lt;td style="text-align:left">Triggers if the evaluated image has a GEM package installed that matches the specified name and version.&lt;/td>
&lt;td style="text-align:left">version&lt;/td>
&lt;td style="text-align:left">Optional version to blacklist specifically.&lt;/td>
&lt;td style="text-align:left">0.2.9&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td style="text-align:left">feed_data_unavailable&lt;/td>
&lt;td style="text-align:left">Triggers if anchore does not have access to the GEM data feed.&lt;/td>
&lt;td style="text-align:left">&lt;/td>
&lt;td style="text-align:left">&lt;/td>
&lt;td style="text-align:left">&lt;/td>
&lt;/tr>
&lt;/tbody>
&lt;/table>
&lt;h3 id="gate-npms">Gate: npms&lt;/h3>
&lt;p>NPM Checks&lt;/p>
&lt;table>
&lt;thead>
&lt;tr>
&lt;th style="text-align:left">Trigger Name&lt;/th>
&lt;th style="text-align:left">Description&lt;/th>
&lt;th style="text-align:left">Parameter&lt;/th>
&lt;th style="text-align:left">Description&lt;/th>
&lt;th style="text-align:left">Example&lt;/th>
&lt;/tr>
&lt;/thead>
&lt;tbody>
&lt;tr>
&lt;td style="text-align:left">newer_version_in_feed&lt;/td>
&lt;td style="text-align:left">Triggers if an installed NPM is not the latest version according to NPM data feed.&lt;/td>
&lt;td style="text-align:left">&lt;/td>
&lt;td style="text-align:left">&lt;/td>
&lt;td style="text-align:left">&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td style="text-align:left">unknown_in_feeds&lt;/td>
&lt;td style="text-align:left">Triggers if an installed NPM is not in the official NPM database, according to NPM data feed.&lt;/td>
&lt;td style="text-align:left">&lt;/td>
&lt;td style="text-align:left">&lt;/td>
&lt;td style="text-align:left">&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td style="text-align:left">version_not_in_feeds&lt;/td>
&lt;td style="text-align:left">Triggers if an installed NPM version is not listed in the official NPM feed as a valid version.&lt;/td>
&lt;td style="text-align:left">&lt;/td>
&lt;td style="text-align:left">&lt;/td>
&lt;td style="text-align:left">&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td style="text-align:left">blacklisted_name_version&lt;/td>
&lt;td style="text-align:left">Triggers if the evaluated image has an NPM package installed that matches the name and optionally a version specified in the parameters.&lt;/td>
&lt;td style="text-align:left">name&lt;/td>
&lt;td style="text-align:left">Npm package name to blacklist.&lt;/td>
&lt;td style="text-align:left">time_diff&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td style="text-align:left">blacklisted_name_version&lt;/td>
&lt;td style="text-align:left">Triggers if the evaluated image has an NPM package installed that matches the name and optionally a version specified in the parameters.&lt;/td>
&lt;td style="text-align:left">version&lt;/td>
&lt;td style="text-align:left">Npm package version to blacklist specifically.&lt;/td>
&lt;td style="text-align:left">0.2.9&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td style="text-align:left">feed_data_unavailable&lt;/td>
&lt;td style="text-align:left">Triggers if the engine does not have access to the NPM data feed.&lt;/td>
&lt;td style="text-align:left">&lt;/td>
&lt;td style="text-align:left">&lt;/td>
&lt;td style="text-align:left">&lt;/td>
&lt;/tr>
&lt;/tbody>
&lt;/table>
&lt;h3 id="gate-secret_scans">Gate: secret_scans&lt;/h3>
&lt;p>Checks for secrets and content found in the image using configured regexes found in the &amp;ldquo;secret_search&amp;rdquo; section of analyzer_config.yaml.&lt;/p>
&lt;table>
&lt;thead>
&lt;tr>
&lt;th style="text-align:left">Trigger Name&lt;/th>
&lt;th style="text-align:left">Description&lt;/th>
&lt;th style="text-align:left">Parameter&lt;/th>
&lt;th style="text-align:left">Description&lt;/th>
&lt;th style="text-align:left">Example&lt;/th>
&lt;/tr>
&lt;/thead>
&lt;tbody>
&lt;tr>
&lt;td style="text-align:left">content_regex_checks&lt;/td>
&lt;td style="text-align:left">Triggers if the secret content search analyzer has found any matches with the configured and named regexes. Checks can be configured to trigger if a match is found or is not found (selected using match_type parameter). Matches are filtered by the content_regex_name and filename_regex if they are set. The content_regex_name shoud be a value from the &amp;ldquo;secret_search&amp;rdquo; section of the analyzer_config.yaml.&lt;/td>
&lt;td style="text-align:left">content_regex_name&lt;/td>
&lt;td style="text-align:left">Name of content regexps configured in the analyzer that match if found in the image, instead of matching all. Names available by default are: [&amp;lsquo;AWS_ACCESS_KEY&amp;rsquo;, &amp;lsquo;AWS_SECRET_KEY&amp;rsquo;, &amp;lsquo;PRIV_KEY&amp;rsquo;, &amp;lsquo;DOCKER_AUTH&amp;rsquo;, &amp;lsquo;API_KEY&amp;rsquo;].&lt;/td>
&lt;td style="text-align:left">AWS_ACCESS_KEY&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td style="text-align:left">content_regex_checks&lt;/td>
&lt;td style="text-align:left">Triggers if the secret content search analyzer has found any matches with the configured and named regexes. Checks can be configured to trigger if a match is found or is not found (selected using match_type parameter). Matches are filtered by the content_regex_name and filename_regex if they are set. The content_regex_name shoud be a value from the &amp;ldquo;secret_search&amp;rdquo; section of the analyzer_config.yaml.&lt;/td>
&lt;td style="text-align:left">filename_regex&lt;/td>
&lt;td style="text-align:left">Regexp to filter the content matched files by.&lt;/td>
&lt;td style="text-align:left">/etc/.*&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td style="text-align:left">content_regex_checks&lt;/td>
&lt;td style="text-align:left">Triggers if the secret content search analyzer has found any matches with the configured and named regexes. Checks can be configured to trigger if a match is found or is not found (selected using match_type parameter). Matches are filtered by the content_regex_name and filename_regex if they are set. The content_regex_name shoud be a value from the &amp;ldquo;secret_search&amp;rdquo; section of the analyzer_config.yaml.&lt;/td>
&lt;td style="text-align:left">match_type&lt;/td>
&lt;td style="text-align:left">Set to define the type of match - trigger if match is found (default) or not found.&lt;/td>
&lt;td style="text-align:left">found&lt;/td>
&lt;/tr>
&lt;/tbody>
&lt;/table>
&lt;h3 id="gate-metadata">Gate: metadata&lt;/h3>
&lt;p>Checks against image metadata, such as size, OS, distro, architecture, etc.&lt;/p>
&lt;table>
&lt;thead>
&lt;tr>
&lt;th style="text-align:left">Trigger Name&lt;/th>
&lt;th style="text-align:left">Description&lt;/th>
&lt;th style="text-align:left">Parameter&lt;/th>
&lt;th style="text-align:left">Description&lt;/th>
&lt;th style="text-align:left">Example&lt;/th>
&lt;/tr>
&lt;/thead>
&lt;tbody>
&lt;tr>
&lt;td style="text-align:left">attribute&lt;/td>
&lt;td style="text-align:left">Triggers if a named image metadata value matches the given condition.&lt;/td>
&lt;td style="text-align:left">attribute&lt;/td>
&lt;td style="text-align:left">Attribute name to be checked.&lt;/td>
&lt;td style="text-align:left">size&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td style="text-align:left">attribute&lt;/td>
&lt;td style="text-align:left">Triggers if a named image metadata value matches the given condition.&lt;/td>
&lt;td style="text-align:left">check&lt;/td>
&lt;td style="text-align:left">The operation to perform the evaluation.&lt;/td>
&lt;td style="text-align:left">&amp;gt;&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td style="text-align:left">attribute&lt;/td>
&lt;td style="text-align:left">Triggers if a named image metadata value matches the given condition.&lt;/td>
&lt;td style="text-align:left">value&lt;/td>
&lt;td style="text-align:left">Value used in comparison.&lt;/td>
&lt;td style="text-align:left">1073741824&lt;/td>
&lt;/tr>
&lt;/tbody>
&lt;/table>
&lt;h3 id="gate-always">Gate: always&lt;/h3>
&lt;p>Triggers that fire unconditionally if present in policy, useful for things like testing and blacklisting.&lt;/p>
&lt;table>
&lt;thead>
&lt;tr>
&lt;th style="text-align:left">Trigger Name&lt;/th>
&lt;th style="text-align:left">Description&lt;/th>
&lt;th style="text-align:left">Parameter&lt;/th>
&lt;th style="text-align:left">Description&lt;/th>
&lt;th style="text-align:left">Example&lt;/th>
&lt;/tr>
&lt;/thead>
&lt;tbody>
&lt;tr>
&lt;td style="text-align:left">always&lt;/td>
&lt;td style="text-align:left">Fires if present in a policy being evaluated. Useful for things like blacklisting images or testing mappings and whitelists by using this trigger in combination with policy mapping rules.&lt;/td>
&lt;td style="text-align:left">&lt;/td>
&lt;td style="text-align:left">&lt;/td>
&lt;td style="text-align:left">&lt;/td>
&lt;/tr>
&lt;/tbody>
&lt;/table>
&lt;h3 id="gate-retrieved_files">Gate: retrieved_files&lt;/h3>
&lt;p>Checks against content and/or presence of files retrieved at analysis time from an image&lt;/p>
&lt;table>
&lt;thead>
&lt;tr>
&lt;th style="text-align:left">Trigger Name&lt;/th>
&lt;th style="text-align:left">Description&lt;/th>
&lt;th style="text-align:left">Parameter&lt;/th>
&lt;th style="text-align:left">Description&lt;/th>
&lt;th style="text-align:left">Example&lt;/th>
&lt;/tr>
&lt;/thead>
&lt;tbody>
&lt;tr>
&lt;td style="text-align:left">content_not_available&lt;/td>
&lt;td style="text-align:left">Triggers if the specified file is not present/stored in the evaluated image.&lt;/td>
&lt;td style="text-align:left">path&lt;/td>
&lt;td style="text-align:left">The path of the file to verify has been retrieved during analysis&lt;/td>
&lt;td style="text-align:left">/etc/httpd.conf&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td style="text-align:left">content_regex&lt;/td>
&lt;td style="text-align:left">Evaluation of regex on retrieved file content&lt;/td>
&lt;td style="text-align:left">path&lt;/td>
&lt;td style="text-align:left">The path of the file to verify has been retrieved during analysis&lt;/td>
&lt;td style="text-align:left">/etc/httpd.conf&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td style="text-align:left">content_regex&lt;/td>
&lt;td style="text-align:left">Evaluation of regex on retrieved file content&lt;/td>
&lt;td style="text-align:left">check&lt;/td>
&lt;td style="text-align:left">The type of check to perform with the regex&lt;/td>
&lt;td style="text-align:left">match&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td style="text-align:left">content_regex&lt;/td>
&lt;td style="text-align:left">Evaluation of regex on retrieved file content&lt;/td>
&lt;td style="text-align:left">regex&lt;/td>
&lt;td style="text-align:left">The regex to evaluate against the content of the file&lt;/td>
&lt;td style="text-align:left">.&lt;em>SSlEnabled.&lt;/em>&lt;/td>
&lt;/tr>
&lt;/tbody>
&lt;/table>
&lt;h3 id="gate-malware">Gate: Malware&lt;/h3>
&lt;table>
&lt;thead>
&lt;tr>
&lt;th>Trigger&lt;/th>
&lt;th>Description&lt;/th>
&lt;th>Parameters&lt;/th>
&lt;/tr>
&lt;/thead>
&lt;tbody>
&lt;tr>
&lt;td>scans&lt;/td>
&lt;td>Triggers if any malware scanner has found any matches in the image.&lt;/td>
&lt;td>&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>scan_not_run&lt;/td>
&lt;td>Triggers if no scan was found for the image.&lt;/td>
&lt;td>&lt;/td>
&lt;/tr>
&lt;/tbody>
&lt;/table>
&lt;h3 id="next-steps">Next Steps&lt;/h3>
&lt;p>Now that you have a good grasp on the core concepts and architecture, check out the &lt;a href="/docs/install/requirements/">Requirements&lt;/a> section for running Anchore.&lt;/p></description></item><item><title>Docs: Configuring Malware Scans of Images</title><link>/docs/install/configuration/malware/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/docs/install/configuration/malware/</guid><description>
&lt;h2 id="malware-scanning-overview">Malware Scanning Overview&lt;/h2>
&lt;p>See &lt;a href="/docs/general/concepts/images/analysis/malware_scanning/">Malware Scanning&lt;/a> for an overview of the feature and how it works. This section is for configuration of scan behavior.&lt;/p>
&lt;p>Customizing the &lt;code>analyzer_config.yaml&lt;/code> requires a restart of the analyzer container. The typical process is to mount it externally into &lt;code>/config/analyzer_config.yaml&lt;/code> from a host volume or as a ConfigMap in Kubernetes and
all analyzers in the deployment share the same configuration.&lt;/p>
&lt;h2 id="enabling--disabling-malware-scans">Enabling &amp;amp; Disabling Malware Scans&lt;/h2>
&lt;p>Each analyzer needs to have it&amp;rsquo;s analyzer_config.yaml file updated to include:&lt;/p>
&lt;div class="highlight">&lt;pre style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4">&lt;code class="language-yaml" data-lang="yaml">&lt;span style="color:#204a87;font-weight:bold">malware&lt;/span>&lt;span style="color:#000;font-weight:bold">:&lt;/span>&lt;span style="color:#f8f8f8;text-decoration:underline">
&lt;/span>&lt;span style="color:#f8f8f8;text-decoration:underline"> &lt;/span>&lt;span style="color:#204a87;font-weight:bold">clamav&lt;/span>&lt;span style="color:#000;font-weight:bold">:&lt;/span>&lt;span style="color:#f8f8f8;text-decoration:underline">
&lt;/span>&lt;span style="color:#f8f8f8;text-decoration:underline"> &lt;/span>&lt;span style="color:#204a87;font-weight:bold">enabled&lt;/span>&lt;span style="color:#000;font-weight:bold">:&lt;/span>&lt;span style="color:#f8f8f8;text-decoration:underline"> &lt;/span>&lt;span style="color:#204a87;font-weight:bold">true&lt;/span>&lt;span style="color:#f8f8f8;text-decoration:underline">
&lt;/span>&lt;span style="color:#f8f8f8;text-decoration:underline"> &lt;/span>&lt;span style="color:#204a87;font-weight:bold">db_update_enabled&lt;/span>&lt;span style="color:#000;font-weight:bold">:&lt;/span>&lt;span style="color:#f8f8f8;text-decoration:underline"> &lt;/span>&lt;span style="color:#204a87;font-weight:bold">true&lt;/span>&lt;span style="color:#f8f8f8;text-decoration:underline">
&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>malware.clamav.enabled = true will enable the analyzer that runs the scan. If not enabled, the analyzer will run but will not execute a ClamAV scan so no scan results
will be reported.&lt;/p>
&lt;blockquote>
&lt;p>:warning: &lt;strong>Malware Scanning currently only supports image sizes up to 4gb.&lt;/strong>&lt;/p>
&lt;/blockquote>
&lt;h2 id="disabling-db-updates-for-clamav">Disabling DB Updates for ClamAV&lt;/h2>
&lt;p>The &lt;code>db_update_enabled&lt;/code> property of the malware.clamav object shown above in the analyzer_config.yaml controls whether the analyzer will invoke a &lt;code>refreshclam&lt;/code> call prior to each
analysis execution. By default it is enabled and should be left on for up-to-date scan results. The db version is returned in the metadata section of the scan results available from the engine API.&lt;/p>
&lt;p>You can disable the update if you want to mount an external volume to provide the db data in &lt;em>/home/anchore/clamav/db&lt;/em> inside the container (must be read-write for the anchore user) This can be used
to cache or share a db across multiple analyzers (e.g. using AWS EFS) or to support air-gapped deployments where the db cannot be automatically updated from deployment itself.&lt;/p>
&lt;h2 id="advanced-configuration">Advanced Configuration&lt;/h2>
&lt;p>The path for the db and db update configuration are also available as environment variables inside the analyzer containers. These should not need to be used in most cases, but
for air-gapped or other installation where the default configuration is not sufficient they are available for customization.&lt;/p>
&lt;table>
&lt;thead>
&lt;tr>
&lt;th>Name&lt;/th>
&lt;th>Description&lt;/th>
&lt;th>Default&lt;/th>
&lt;/tr>
&lt;/thead>
&lt;tbody>
&lt;tr>
&lt;td>ANCHORE_FRESHCLAM_CONFIG_FILE&lt;/td>
&lt;td>Location of freshclam.conf to use&lt;/td>
&lt;td>/home/anchore/clamav/freshclam.conf&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>ANCHORE_CLAMAV_DB_DIR&lt;/td>
&lt;td>Location of the db dir to read/write&lt;/td>
&lt;td>/home/anchore/clamav/db&lt;/td>
&lt;/tr>
&lt;/tbody>
&lt;/table>
&lt;p>For most cases, anchore uses the default values for the &lt;code>clamscan&lt;/code> and &lt;code>freshclam&lt;/code> invocations. The parameters that are already specified
can be found in the &lt;a href="https://github.com/anchore/anchore-engine/blob/master/anchore_engine/analyzers/malware.py">analyzer source&lt;/a>.
If you would like to override any of the default values of those commands or replace existing ones, you can add the following to the analyzer_config.yaml:&lt;/p>
&lt;div class="highlight">&lt;pre style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4">&lt;code class="language-yaml" data-lang="yaml">&lt;span style="color:#204a87;font-weight:bold">malware&lt;/span>&lt;span style="color:#000;font-weight:bold">:&lt;/span>&lt;span style="color:#f8f8f8;text-decoration:underline">
&lt;/span>&lt;span style="color:#f8f8f8;text-decoration:underline"> &lt;/span>&lt;span style="color:#204a87;font-weight:bold">clamav&lt;/span>&lt;span style="color:#000;font-weight:bold">:&lt;/span>&lt;span style="color:#f8f8f8;text-decoration:underline">
&lt;/span>&lt;span style="color:#f8f8f8;text-decoration:underline"> &lt;/span>&lt;span style="color:#204a87;font-weight:bold">clamscan_args&lt;/span>&lt;span style="color:#000;font-weight:bold">:&lt;/span>&lt;span style="color:#f8f8f8;text-decoration:underline">
&lt;/span>&lt;span style="color:#f8f8f8;text-decoration:underline"> &lt;/span>- &lt;span style="color:#000">max-filesize=1000m&lt;/span>&lt;span style="color:#f8f8f8;text-decoration:underline">
&lt;/span>&lt;span style="color:#f8f8f8;text-decoration:underline"> &lt;/span>- &lt;span style="color:#000">max-scansize=1000m&lt;/span>&lt;span style="color:#f8f8f8;text-decoration:underline">
&lt;/span>&lt;span style="color:#f8f8f8;text-decoration:underline"> &lt;/span>&lt;span style="color:#204a87;font-weight:bold">freshclam_args&lt;/span>&lt;span style="color:#000;font-weight:bold">:&lt;/span>&lt;span style="color:#f8f8f8;text-decoration:underline">
&lt;/span>&lt;span style="color:#f8f8f8;text-decoration:underline"> &lt;/span>- &lt;span style="color:#000">datadir=/tmp/different/datadir&lt;/span>&lt;span style="color:#f8f8f8;text-decoration:underline">
&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>Please note that the value above will be passed &lt;em>directly&lt;/em> to the corresponding commands, e.g.:&lt;/p>
&lt;div class="highlight">&lt;pre style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4">&lt;code class="language-shell" data-lang="shell">clamscan --suppress-ok-results --infected --recursive --allmatch --archive-verbose --tempdir&lt;span style="color:#ce5c00;font-weight:bold">={&lt;/span>tempdir&lt;span style="color:#ce5c00;font-weight:bold">}&lt;/span> --database&lt;span style="color:#ce5c00;font-weight:bold">={&lt;/span>database&lt;span style="color:#ce5c00;font-weight:bold">}&lt;/span> --max-filesize&lt;span style="color:#ce5c00;font-weight:bold">=&lt;/span>1000m --max-scansize&lt;span style="color:#ce5c00;font-weight:bold">=&lt;/span>1000m &amp;lt;path_to_tar&amp;gt;
&lt;/code>&lt;/pre>&lt;/div></description></item><item><title>Docs: Anchore Engine Release Notes - Version 1.1.0</title><link>/docs/releasenotes/110/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/docs/releasenotes/110/</guid><description>
&lt;h2 id="anchore-engine-110">Anchore Engine 1.1.0&lt;/h2>
&lt;p>API version - 0.1.20&lt;/p>
&lt;p>DB Schema version - 0.0.16&lt;/p>
&lt;h3 id="rocky-linux-support">Rocky Linux support&lt;/h3>
&lt;p>Anchore Enterprise can now scan Rocky Linux images for vulnerabilities.&lt;/p>
&lt;h3 id="fixes">Fixes&lt;/h3>
&lt;ul>
&lt;li>Images that had Go content and hints enabled were failing analysis. This has been fixed.&lt;/li>
&lt;li>Images reported via runtime inventory that also had port numbers in the registry host URL were failing to parse properly, which caused scan failures. This issue has been fixed.&lt;/li>
&lt;li>With the Grype provider, NVD and vendor CVSS scores were missing for records in non-NVD namespaces. This is now fixed.&lt;/li>
&lt;/ul>
&lt;h3 id="upgrading">Upgrading&lt;/h3>
&lt;ul>
&lt;li>&lt;a href="/docs/install/upgrade/">Upgrading Anchore Engine&lt;/a>&lt;/li>
&lt;/ul></description></item></channel></rss>